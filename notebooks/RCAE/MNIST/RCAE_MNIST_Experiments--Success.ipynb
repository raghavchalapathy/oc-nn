{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RCAE_MNIST_Experiments--Success.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"izSliiGYb5NE","colab_type":"code","outputId":"18008af1-3cc8-471a-a393-922170ee0b73","executionInfo":{"status":"ok","timestamp":1547274812064,"user_tz":-660,"elapsed":10941,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install fuel\n","!pip install picklable_itertools\n","\n","%matplotlib inline\n","%reload_ext autoreload\n","%autoreload 2\n","\n","\n","PROJECT_DIR = \"/content/drive/My Drive/2019/testing/oc-nn/\"\n","\n","\n","import sys,os\n","import numpy as np\n","sys.path.append(PROJECT_DIR)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: fuel in /usr/local/lib/python3.6/dist-packages (0.2.0)\n","Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from fuel) (3.38.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fuel) (1.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fuel) (1.14.6)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from fuel) (2.8.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fuel) (3.13)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fuel) (1.11.0)\n","Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from fuel) (17.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fuel) (2.18.4)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from fuel) (4.0.0)\n","Requirement already satisfied: picklable-itertools in /usr/local/lib/python3.6/dist-packages (from fuel) (0.1.1)\n","Requirement already satisfied: tables in /usr/local/lib/python3.6/dist-packages (from fuel) (3.4.4)\n","Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->fuel) (2.3.0)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (2.6)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (3.0.4)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (1.22)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (2018.11.29)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->fuel) (0.46)\n","Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.6/dist-packages (from tables->fuel) (2.6.9)\n","Requirement already satisfied: picklable_itertools in /usr/local/lib/python3.6/dist-packages (0.1.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from picklable_itertools) (1.11.0)\n"],"name":"stdout"}]},{"metadata":{"id":"eC1Lp2myJ_O7","colab_type":"text"},"cell_type":"markdown","source":["# **RCAE-MNIST 9_Vs_all** "]},{"metadata":{"id":"n7OiP7u8JuSL","colab_type":"code","outputId":"a7a31674-0893-4b9b-ca8f-1d7e7269b3f0","executionInfo":{"status":"ok","timestamp":1547277700284,"user_tz":-660,"elapsed":2883553,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":23057}},"cell_type":"code","source":["%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","# RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2019/testing/oc-nn//reports/figures/MNIST/RCAE/\n","INFO: The dataset is  mnist\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] Random Seed used is   42\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] Random Seed used is   42\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/150\n","5377/5377 [==============================] - 7s 1ms/step - loss: 5.0019 - val_loss: 4.9790\n","Epoch 2/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 4.9705 - val_loss: 4.9711\n","Epoch 3/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 4.9663 - val_loss: 4.9704\n","Epoch 4/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 4.9634 - val_loss: 4.9702\n","Epoch 5/150\n","5377/5377 [==============================] - 5s 872us/step - loss: 4.9619 - val_loss: 4.9633\n","Epoch 6/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 4.9609 - val_loss: 4.9622\n","Epoch 7/150\n","5377/5377 [==============================] - 5s 873us/step - loss: 4.9602 - val_loss: 4.9613\n","Epoch 8/150\n","5377/5377 [==============================] - 5s 872us/step - loss: 4.9595 - val_loss: 4.9601\n","Epoch 9/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 4.9592 - val_loss: 4.9601\n","Epoch 10/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 4.9587 - val_loss: 4.9597\n","Epoch 11/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 4.9585 - val_loss: 4.9591\n","Epoch 12/150\n","5377/5377 [==============================] - 5s 872us/step - loss: 4.9581 - val_loss: 4.9595\n","Epoch 13/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 4.9581 - val_loss: 4.9593\n","Epoch 14/150\n","5377/5377 [==============================] - 5s 872us/step - loss: 4.9577 - val_loss: 4.9585\n","Epoch 15/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 4.9575 - val_loss: 4.9588\n","Epoch 16/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9575 - val_loss: 4.9586\n","Epoch 17/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 4.9572 - val_loss: 4.9581\n","Epoch 18/150\n","5377/5377 [==============================] - 5s 872us/step - loss: 4.9570 - val_loss: 4.9582\n","Epoch 19/150\n","5377/5377 [==============================] - 5s 872us/step - loss: 4.9571 - val_loss: 4.9590\n","Epoch 20/150\n","5377/5377 [==============================] - 5s 863us/step - loss: 4.9569 - val_loss: 4.9579\n","Epoch 21/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9571 - val_loss: 4.9578\n","Epoch 22/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 4.9567 - val_loss: 4.9583\n","Epoch 23/150\n","5377/5377 [==============================] - 5s 872us/step - loss: 4.9569 - val_loss: 4.9580\n","Epoch 24/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9567 - val_loss: 4.9580\n","Epoch 25/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 4.9566 - val_loss: 4.9580\n","Epoch 26/150\n","5377/5377 [==============================] - 5s 871us/step - loss: 4.9564 - val_loss: 4.9583\n","Epoch 27/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 4.9565 - val_loss: 4.9580\n","Epoch 28/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 4.9564 - val_loss: 4.9586\n","Epoch 29/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 4.9563 - val_loss: 4.9577\n","Epoch 30/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 4.9562 - val_loss: 4.9581\n","Epoch 31/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 4.9563 - val_loss: 4.9578\n","Epoch 32/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 4.9562 - val_loss: 4.9577\n","Epoch 33/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 4.9560 - val_loss: 4.9580\n","Epoch 34/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 4.9560 - val_loss: 4.9578\n","Epoch 35/150\n","5377/5377 [==============================] - 5s 871us/step - loss: 4.9560 - val_loss: 4.9578\n","Epoch 36/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9560 - val_loss: 4.9576\n","Epoch 37/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9559 - val_loss: 4.9575\n","Epoch 38/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 4.9558 - val_loss: 4.9574\n","Epoch 39/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9558 - val_loss: 4.9579\n","Epoch 40/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 4.9558 - val_loss: 4.9590\n","Epoch 41/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 4.9558 - val_loss: 4.9578\n","Epoch 42/150\n","5377/5377 [==============================] - 5s 873us/step - loss: 4.9556 - val_loss: 4.9578\n","Epoch 43/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 4.9557 - val_loss: 4.9584\n","Epoch 44/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 4.9556 - val_loss: 4.9582\n","Epoch 45/150\n","5377/5377 [==============================] - 5s 873us/step - loss: 4.9556 - val_loss: 4.9619\n","Epoch 46/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 4.9555 - val_loss: 4.9577\n","Epoch 47/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 4.9556 - val_loss: 4.9576\n","Epoch 48/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 4.9555 - val_loss: 4.9578\n","Epoch 49/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 4.9555 - val_loss: 4.9580\n","Epoch 50/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 4.9554 - val_loss: 4.9579\n","Epoch 51/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9554 - val_loss: 4.9575\n","Epoch 52/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 4.9554 - val_loss: 4.9582\n","Epoch 53/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 4.9552 - val_loss: 4.9575\n","Epoch 54/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 4.9554 - val_loss: 4.9577\n","Epoch 55/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 4.9554 - val_loss: 4.9595\n","Epoch 56/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 4.9553 - val_loss: 4.9579\n","Epoch 57/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9552 - val_loss: 4.9579\n","Epoch 58/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 4.9552 - val_loss: 4.9575\n","Epoch 59/150\n","5377/5377 [==============================] - 5s 872us/step - loss: 4.9552 - val_loss: 4.9576\n","Epoch 60/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 61/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 62/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 4.9551 - val_loss: 4.9580\n","Epoch 63/150\n","5377/5377 [==============================] - 5s 872us/step - loss: 4.9552 - val_loss: 4.9576\n","Epoch 64/150\n","5377/5377 [==============================] - 5s 872us/step - loss: 4.9550 - val_loss: 4.9579\n","Epoch 65/150\n","5377/5377 [==============================] - 5s 870us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 66/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 67/150\n","5377/5377 [==============================] - 5s 872us/step - loss: 4.9550 - val_loss: 4.9586\n","Epoch 68/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 4.9549 - val_loss: 4.9593\n","Epoch 69/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 70/150\n","5377/5377 [==============================] - 5s 898us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 71/150\n","5377/5377 [==============================] - 5s 871us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 72/150\n","5377/5377 [==============================] - 5s 870us/step - loss: 4.9549 - val_loss: 4.9590\n","Epoch 73/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 4.9549 - val_loss: 4.9585\n","Epoch 74/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 75/150\n","5377/5377 [==============================] - 5s 871us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 76/150\n","5377/5377 [==============================] - 5s 869us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 77/150\n","5377/5377 [==============================] - 5s 864us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 78/150\n","5377/5377 [==============================] - 5s 868us/step - loss: 4.9549 - val_loss: 4.9587\n","Epoch 79/150\n","5377/5377 [==============================] - 5s 873us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 80/150\n","5377/5377 [==============================] - 5s 870us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 81/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 82/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 83/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 84/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 85/150\n","5377/5377 [==============================] - 5s 872us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 86/150\n","5377/5377 [==============================] - 5s 870us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 87/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9547 - val_loss: 4.9574\n","Epoch 88/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 89/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 90/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 91/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 92/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9547 - val_loss: 4.9582\n","Epoch 93/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 94/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 95/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 96/150\n","5377/5377 [==============================] - 5s 870us/step - loss: 4.9544 - val_loss: 4.9595\n","Epoch 97/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 98/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 99/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 4.9545 - val_loss: 4.9586\n","Epoch 100/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 101/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 102/150\n","5377/5377 [==============================] - 5s 873us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 103/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 104/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 105/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 106/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 107/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 108/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 109/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 110/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 111/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 112/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 113/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 114/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 115/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 116/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 117/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 118/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 119/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 120/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 121/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 122/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 123/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 124/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 125/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 126/150\n","5377/5377 [==============================] - 5s 871us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 127/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 128/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 129/150\n","5377/5377 [==============================] - 5s 888us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 130/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 131/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 132/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 133/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 134/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 135/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 136/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 137/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 138/150\n","5377/5377 [==============================] - 5s 869us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 139/150\n","5377/5377 [==============================] - 5s 873us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 140/150\n","5377/5377 [==============================] - 5s 871us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 141/150\n","5377/5377 [==============================] - 5s 869us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 142/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 143/150\n","5377/5377 [==============================] - 5s 873us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 144/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 145/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 146/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 147/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 148/150\n","5377/5377 [==============================] - 5s 871us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 149/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 150/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 4.9538 - val_loss: 4.9576\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4683826 0.0\n","The shape of N (5975, 784)\n","The minimum value of N  -1.0\n","The max value of N 1.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 28, 28, 1)\n","img shape: (280, 280, 1)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 28, 28, 1)\n","img shape: (280, 280, 1)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.9995416050698479\n","=======================\n","Saved model to disk....\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/150\n","5377/5377 [==============================] - 7s 1ms/step - loss: 14.3110 - val_loss: 14.3148\n","Epoch 2/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3109 - val_loss: 14.3148\n","Epoch 3/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3110 - val_loss: 14.3148\n","Epoch 4/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 14.3110 - val_loss: 14.3148\n","Epoch 5/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 14.3110 - val_loss: 14.3149\n","Epoch 6/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 14.3109 - val_loss: 14.3150\n","Epoch 7/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 14.3110 - val_loss: 14.3149\n","Epoch 8/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 14.3109 - val_loss: 14.3148\n","Epoch 9/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 14.3109 - val_loss: 14.3147\n","Epoch 10/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 14.3109 - val_loss: 14.3147\n","Epoch 11/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 14.3111 - val_loss: 14.3150\n","Epoch 12/150\n","5377/5377 [==============================] - 5s 888us/step - loss: 14.3109 - val_loss: 14.3150\n","Epoch 13/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 14.3110 - val_loss: 14.3149\n","Epoch 14/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 14.3109 - val_loss: 14.3148\n","Epoch 15/150\n","5377/5377 [==============================] - 5s 873us/step - loss: 14.3109 - val_loss: 14.3150\n","Epoch 16/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 14.3109 - val_loss: 14.3147\n","Epoch 17/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3109 - val_loss: 14.3147\n","Epoch 18/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3109 - val_loss: 14.3147\n","Epoch 19/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3110 - val_loss: 14.3148\n","Epoch 20/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 14.3109 - val_loss: 14.3146\n","Epoch 21/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 14.3109 - val_loss: 14.3150\n","Epoch 22/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 14.3109 - val_loss: 14.3147\n","Epoch 23/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 14.3108 - val_loss: 14.3148\n","Epoch 24/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 14.3108 - val_loss: 14.3149\n","Epoch 25/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3108 - val_loss: 14.3147\n","Epoch 26/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 14.3108 - val_loss: 14.3149\n","Epoch 27/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 14.3108 - val_loss: 14.3146\n","Epoch 28/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 14.3108 - val_loss: 14.3148\n","Epoch 29/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 14.3107 - val_loss: 14.3149\n","Epoch 30/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 14.3108 - val_loss: 14.3148\n","Epoch 31/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 14.3108 - val_loss: 14.3147\n","Epoch 32/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 14.3108 - val_loss: 14.3150\n","Epoch 33/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3108 - val_loss: 14.3146\n","Epoch 34/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 14.3108 - val_loss: 14.3147\n","Epoch 35/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 14.3108 - val_loss: 14.3146\n","Epoch 36/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3107 - val_loss: 14.3146\n","Epoch 37/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 14.3108 - val_loss: 14.3147\n","Epoch 38/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 14.3108 - val_loss: 14.3147\n","Epoch 39/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 14.3107 - val_loss: 14.3146\n","Epoch 40/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 14.3107 - val_loss: 14.3146\n","Epoch 41/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3109 - val_loss: 14.3145\n","Epoch 42/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 14.3107 - val_loss: 14.3146\n","Epoch 43/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3108 - val_loss: 14.3146\n","Epoch 44/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 14.3107 - val_loss: 14.3146\n","Epoch 45/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 14.3106 - val_loss: 14.3148\n","Epoch 46/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 14.3107 - val_loss: 14.3146\n","Epoch 47/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3108 - val_loss: 14.3145\n","Epoch 48/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3107 - val_loss: 14.3146\n","Epoch 49/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3107 - val_loss: 14.3146\n","Epoch 50/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 14.3106 - val_loss: 14.3146\n","Epoch 51/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 14.3106 - val_loss: 14.3146\n","Epoch 52/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 14.3107 - val_loss: 14.3148\n","Epoch 53/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3107 - val_loss: 14.3149\n","Epoch 54/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 14.3106 - val_loss: 14.3149\n","Epoch 55/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 14.3106 - val_loss: 14.3149\n","Epoch 56/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 14.3106 - val_loss: 14.3148\n","Epoch 57/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 14.3106 - val_loss: 14.3145\n","Epoch 58/150\n","5377/5377 [==============================] - 5s 873us/step - loss: 14.3108 - val_loss: 14.3148\n","Epoch 59/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3107 - val_loss: 14.3146\n","Epoch 60/150\n","5377/5377 [==============================] - 5s 870us/step - loss: 14.3107 - val_loss: 14.3147\n","Epoch 61/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 14.3106 - val_loss: 14.3147\n","Epoch 62/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 14.3106 - val_loss: 14.3150\n","Epoch 63/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 14.3107 - val_loss: 14.3147\n","Epoch 64/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 14.3106 - val_loss: 14.3146\n","Epoch 65/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3107 - val_loss: 14.3146\n","Epoch 66/150\n","5377/5377 [==============================] - 5s 866us/step - loss: 14.3106 - val_loss: 14.3146\n","Epoch 67/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3107 - val_loss: 14.3146\n","Epoch 68/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3106 - val_loss: 14.3146\n","Epoch 69/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 14.3106 - val_loss: 14.3148\n","Epoch 70/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 14.3106 - val_loss: 14.3146\n","Epoch 71/150\n","5377/5377 [==============================] - 5s 873us/step - loss: 14.3106 - val_loss: 14.3149\n","Epoch 72/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 14.3106 - val_loss: 14.3152\n","Epoch 73/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 14.3106 - val_loss: 14.3149\n","Epoch 74/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 14.3107 - val_loss: 14.3146\n","Epoch 75/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3106 - val_loss: 14.3148\n","Epoch 76/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 14.3105 - val_loss: 14.3148\n","Epoch 77/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3106 - val_loss: 14.3145\n","Epoch 78/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 14.3105 - val_loss: 14.3146\n","Epoch 79/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 14.3105 - val_loss: 14.3146\n","Epoch 80/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3106 - val_loss: 14.3148\n","Epoch 81/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 14.3106 - val_loss: 14.3146\n","Epoch 82/150\n","5377/5377 [==============================] - 5s 887us/step - loss: 14.3106 - val_loss: 14.3147\n","Epoch 83/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3106 - val_loss: 14.3146\n","Epoch 84/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 14.3105 - val_loss: 14.3147\n","Epoch 85/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3106 - val_loss: 14.3146\n","Epoch 86/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 14.3106 - val_loss: 14.3147\n","Epoch 87/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 14.3105 - val_loss: 14.3146\n","Epoch 88/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 14.3105 - val_loss: 14.3146\n","Epoch 89/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3105 - val_loss: 14.3146\n","Epoch 90/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3106 - val_loss: 14.3147\n","Epoch 91/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 14.3105 - val_loss: 14.3146\n","Epoch 92/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3105 - val_loss: 14.3146\n","Epoch 93/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 14.3105 - val_loss: 14.3146\n","Epoch 94/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3105 - val_loss: 14.3147\n","Epoch 95/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 14.3105 - val_loss: 14.3146\n","Epoch 96/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 14.3105 - val_loss: 14.3148\n","Epoch 97/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 14.3105 - val_loss: 14.3149\n","Epoch 98/150\n","5377/5377 [==============================] - 5s 873us/step - loss: 14.3105 - val_loss: 14.3148\n","Epoch 99/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 14.3105 - val_loss: 14.3147\n","Epoch 100/150\n","5377/5377 [==============================] - 5s 885us/step - loss: 14.3105 - val_loss: 14.3147\n","Epoch 101/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 14.3106 - val_loss: 14.3146\n","Epoch 102/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3105 - val_loss: 14.3148\n","Epoch 103/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 14.3105 - val_loss: 14.3147\n","Epoch 104/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 14.3105 - val_loss: 14.3147\n","Epoch 105/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 14.3105 - val_loss: 14.3146\n","Epoch 106/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 14.3104 - val_loss: 14.3147\n","Epoch 107/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3105 - val_loss: 14.3146\n","Epoch 108/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 14.3105 - val_loss: 14.3146\n","Epoch 109/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 14.3105 - val_loss: 14.3150\n","Epoch 110/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3105 - val_loss: 14.3149\n","Epoch 111/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3105 - val_loss: 14.3147\n","Epoch 112/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 14.3105 - val_loss: 14.3147\n","Epoch 113/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 14.3104 - val_loss: 14.3146\n","Epoch 114/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 14.3104 - val_loss: 14.3148\n","Epoch 115/150\n","5377/5377 [==============================] - 5s 887us/step - loss: 14.3105 - val_loss: 14.3147\n","Epoch 116/150\n","5377/5377 [==============================] - 5s 873us/step - loss: 14.3104 - val_loss: 14.3148\n","Epoch 117/150\n","5377/5377 [==============================] - 5s 885us/step - loss: 14.3105 - val_loss: 14.3147\n","Epoch 118/150\n","5377/5377 [==============================] - 5s 873us/step - loss: 14.3104 - val_loss: 14.3149\n","Epoch 119/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 14.3105 - val_loss: 14.3147\n","Epoch 120/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 14.3104 - val_loss: 14.3146\n","Epoch 121/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 14.3104 - val_loss: 14.3147\n","Epoch 122/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 14.3104 - val_loss: 14.3147\n","Epoch 123/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3104 - val_loss: 14.3149\n","Epoch 124/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 14.3104 - val_loss: 14.3146\n","Epoch 125/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 14.3104 - val_loss: 14.3146\n","Epoch 126/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3104 - val_loss: 14.3147\n","Epoch 127/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 14.3104 - val_loss: 14.3147\n","Epoch 128/150\n","5377/5377 [==============================] - 5s 873us/step - loss: 14.3105 - val_loss: 14.3147\n","Epoch 129/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 14.3104 - val_loss: 14.3146\n","Epoch 130/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3104 - val_loss: 14.3147\n","Epoch 131/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 14.3104 - val_loss: 14.3147\n","Epoch 132/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 14.3104 - val_loss: 14.3148\n","Epoch 133/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 14.3104 - val_loss: 14.3146\n","Epoch 134/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 14.3104 - val_loss: 14.3147\n","Epoch 135/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 14.3104 - val_loss: 14.3149\n","Epoch 136/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3105 - val_loss: 14.3147\n","Epoch 137/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 14.3104 - val_loss: 14.3147\n","Epoch 138/150\n","5377/5377 [==============================] - 5s 872us/step - loss: 14.3103 - val_loss: 14.3147\n","Epoch 139/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 14.3104 - val_loss: 14.3147\n","Epoch 140/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 14.3104 - val_loss: 14.3147\n","Epoch 141/150\n","5377/5377 [==============================] - 5s 885us/step - loss: 14.3104 - val_loss: 14.3145\n","Epoch 142/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 14.3104 - val_loss: 14.3146\n","Epoch 143/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 14.3103 - val_loss: 14.3145\n","Epoch 144/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 14.3104 - val_loss: 14.3146\n","Epoch 145/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 14.3103 - val_loss: 14.3147\n","Epoch 146/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 14.3105 - val_loss: 14.3147\n","Epoch 147/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 14.3103 - val_loss: 14.3146\n","Epoch 148/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 14.3104 - val_loss: 14.3145\n","Epoch 149/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 14.3103 - val_loss: 14.3146\n","Epoch 150/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 14.3103 - val_loss: 14.3144\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 46327 0.5\n","The shape of N (5975, 784)\n","The minimum value of N  -0.7470903396606445\n","The max value of N 0.7499860525131226\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 28, 28, 1)\n","img shape: (280, 280, 1)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 28, 28, 1)\n","img shape: (280, 280, 1)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.9994184114323696\n","=======================\n","Saved model to disk....\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/150\n","5377/5377 [==============================] - 8s 1ms/step - loss: 28.6173 - val_loss: 28.6214\n","Epoch 2/150\n","5377/5377 [==============================] - 5s 885us/step - loss: 28.6172 - val_loss: 28.6215\n","Epoch 3/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6172 - val_loss: 28.6217\n","Epoch 4/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 28.6172 - val_loss: 28.6214\n","Epoch 5/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 28.6173 - val_loss: 28.6215\n","Epoch 6/150\n","5377/5377 [==============================] - 5s 892us/step - loss: 28.6174 - val_loss: 28.6213\n","Epoch 7/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6173 - val_loss: 28.6214\n","Epoch 8/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 28.6173 - val_loss: 28.6215\n","Epoch 9/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 28.6172 - val_loss: 28.6217\n","Epoch 10/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 28.6173 - val_loss: 28.6214\n","Epoch 11/150\n","5377/5377 [==============================] - 5s 870us/step - loss: 28.6172 - val_loss: 28.6214\n","Epoch 12/150\n","5377/5377 [==============================] - 5s 885us/step - loss: 28.6172 - val_loss: 28.6214\n","Epoch 13/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 28.6172 - val_loss: 28.6217\n","Epoch 14/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6172 - val_loss: 28.6214\n","Epoch 15/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6173 - val_loss: 28.6215\n","Epoch 16/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 28.6173 - val_loss: 28.6215\n","Epoch 17/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 28.6172 - val_loss: 28.6214\n","Epoch 18/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 28.6173 - val_loss: 28.6215\n","Epoch 19/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6172 - val_loss: 28.6217\n","Epoch 20/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6172 - val_loss: 28.6215\n","Epoch 21/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 28.6172 - val_loss: 28.6215\n","Epoch 22/150\n","5377/5377 [==============================] - 5s 885us/step - loss: 28.6172 - val_loss: 28.6216\n","Epoch 23/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 28.6174 - val_loss: 28.6216\n","Epoch 24/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 28.6172 - val_loss: 28.6215\n","Epoch 25/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6172 - val_loss: 28.6217\n","Epoch 26/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 28.6172 - val_loss: 28.6215\n","Epoch 27/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6172 - val_loss: 28.6215\n","Epoch 28/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 28.6173 - val_loss: 28.6217\n","Epoch 29/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 28.6172 - val_loss: 28.6215\n","Epoch 30/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6173 - val_loss: 28.6214\n","Epoch 31/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 28.6172 - val_loss: 28.6217\n","Epoch 32/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 28.6172 - val_loss: 28.6216\n","Epoch 33/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 28.6172 - val_loss: 28.6216\n","Epoch 34/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 28.6172 - val_loss: 28.6216\n","Epoch 35/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6172 - val_loss: 28.6217\n","Epoch 36/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 28.6172 - val_loss: 28.6217\n","Epoch 37/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 28.6172 - val_loss: 28.6216\n","Epoch 38/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 28.6172 - val_loss: 28.6218\n","Epoch 39/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 28.6172 - val_loss: 28.6217\n","Epoch 40/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6172 - val_loss: 28.6217\n","Epoch 41/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6172 - val_loss: 28.6216\n","Epoch 42/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 43/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 28.6172 - val_loss: 28.6216\n","Epoch 44/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 28.6172 - val_loss: 28.6215\n","Epoch 45/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 28.6171 - val_loss: 28.6215\n","Epoch 46/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 47/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 28.6172 - val_loss: 28.6216\n","Epoch 48/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 28.6172 - val_loss: 28.6216\n","Epoch 49/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 50/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 28.6172 - val_loss: 28.6217\n","Epoch 51/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6172 - val_loss: 28.6215\n","Epoch 52/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 28.6172 - val_loss: 28.6217\n","Epoch 53/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6172 - val_loss: 28.6214\n","Epoch 54/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6172 - val_loss: 28.6216\n","Epoch 55/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6171 - val_loss: 28.6215\n","Epoch 56/150\n","5377/5377 [==============================] - 5s 885us/step - loss: 28.6172 - val_loss: 28.6216\n","Epoch 57/150\n","5377/5377 [==============================] - 5s 887us/step - loss: 28.6172 - val_loss: 28.6216\n","Epoch 58/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6172 - val_loss: 28.6216\n","Epoch 59/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 28.6171 - val_loss: 28.6214\n","Epoch 60/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 61/150\n","5377/5377 [==============================] - 5s 868us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 62/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 63/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 64/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 65/150\n","5377/5377 [==============================] - 5s 873us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 66/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 28.6172 - val_loss: 28.6217\n","Epoch 67/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 68/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 69/150\n","5377/5377 [==============================] - 5s 874us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 70/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 28.6172 - val_loss: 28.6217\n","Epoch 71/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6172 - val_loss: 28.6216\n","Epoch 72/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 73/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 28.6171 - val_loss: 28.6215\n","Epoch 74/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6171 - val_loss: 28.6218\n","Epoch 75/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 76/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6171 - val_loss: 28.6215\n","Epoch 77/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 78/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 28.6170 - val_loss: 28.6216\n","Epoch 79/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 80/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 81/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 82/150\n","5377/5377 [==============================] - 5s 887us/step - loss: 28.6171 - val_loss: 28.6215\n","Epoch 83/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 84/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 85/150\n","5377/5377 [==============================] - 5s 872us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 86/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 87/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 88/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 89/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6171 - val_loss: 28.6219\n","Epoch 90/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 91/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 92/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 93/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6170 - val_loss: 28.6217\n","Epoch 94/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 95/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 96/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 97/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6172 - val_loss: 28.6216\n","Epoch 98/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 99/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 100/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 28.6172 - val_loss: 28.6217\n","Epoch 101/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 102/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 28.6171 - val_loss: 28.6216\n","Epoch 103/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 28.6170 - val_loss: 28.6216\n","Epoch 104/150\n","5377/5377 [==============================] - 5s 872us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 105/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 106/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 28.6171 - val_loss: 28.6218\n","Epoch 107/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6170 - val_loss: 28.6217\n","Epoch 108/150\n","5377/5377 [==============================] - 5s 890us/step - loss: 28.6170 - val_loss: 28.6218\n","Epoch 109/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6170 - val_loss: 28.6219\n","Epoch 110/150\n","5377/5377 [==============================] - 5s 873us/step - loss: 28.6170 - val_loss: 28.6216\n","Epoch 111/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6170 - val_loss: 28.6217\n","Epoch 112/150\n","5377/5377 [==============================] - 5s 885us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 113/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 114/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6171 - val_loss: 28.6218\n","Epoch 115/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6170 - val_loss: 28.6218\n","Epoch 116/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 28.6170 - val_loss: 28.6217\n","Epoch 117/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 28.6170 - val_loss: 28.6218\n","Epoch 118/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 28.6170 - val_loss: 28.6219\n","Epoch 119/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6171 - val_loss: 28.6218\n","Epoch 120/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 28.6171 - val_loss: 28.6219\n","Epoch 121/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 28.6171 - val_loss: 28.6219\n","Epoch 122/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 123/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6171 - val_loss: 28.6218\n","Epoch 124/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6170 - val_loss: 28.6219\n","Epoch 125/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 28.6170 - val_loss: 28.6217\n","Epoch 126/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 28.6171 - val_loss: 28.6218\n","Epoch 127/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 28.6170 - val_loss: 28.6217\n","Epoch 128/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 129/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6170 - val_loss: 28.6218\n","Epoch 130/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 28.6170 - val_loss: 28.6217\n","Epoch 131/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6170 - val_loss: 28.6218\n","Epoch 132/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 28.6171 - val_loss: 28.6218\n","Epoch 133/150\n","5377/5377 [==============================] - 5s 888us/step - loss: 28.6170 - val_loss: 28.6218\n","Epoch 134/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 135/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6171 - val_loss: 28.6217\n","Epoch 136/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 28.6170 - val_loss: 28.6215\n","Epoch 137/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6170 - val_loss: 28.6218\n","Epoch 138/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 28.6170 - val_loss: 28.6218\n","Epoch 139/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6170 - val_loss: 28.6218\n","Epoch 140/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6170 - val_loss: 28.6217\n","Epoch 141/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 28.6170 - val_loss: 28.6219\n","Epoch 142/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6170 - val_loss: 28.6217\n","Epoch 143/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6170 - val_loss: 28.6217\n","Epoch 144/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 28.6170 - val_loss: 28.6216\n","Epoch 145/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 28.6170 - val_loss: 28.6218\n","Epoch 146/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 28.6170 - val_loss: 28.6217\n","Epoch 147/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 28.6170 - val_loss: 28.6217\n","Epoch 148/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 28.6170 - val_loss: 28.6217\n","Epoch 149/150\n","5377/5377 [==============================] - 5s 887us/step - loss: 28.6170 - val_loss: 28.6220\n","Epoch 150/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 28.6170 - val_loss: 28.6217\n","(lamda,Threshold) 1.0 0.5\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4419 1.0\n","The shape of N (5975, 784)\n","The minimum value of N  -0.4993782043457031\n","The max value of N 0.49998313188552856\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 28, 28, 1)\n","img shape: (280, 280, 1)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 28, 28, 1)\n","img shape: (280, 280, 1)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 1.0 0.9986505999243649\n","=======================\n","Saved model to disk....\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/150\n","5377/5377 [==============================] - 8s 1ms/step - loss: 32.1756 - val_loss: 32.1803\n","Epoch 2/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 32.1756 - val_loss: 32.1803\n","Epoch 3/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 32.1755 - val_loss: 32.1803\n","Epoch 4/150\n","5377/5377 [==============================] - 5s 871us/step - loss: 32.1755 - val_loss: 32.1804\n","Epoch 5/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 6/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1756 - val_loss: 32.1803\n","Epoch 7/150\n","5377/5377 [==============================] - 5s 885us/step - loss: 32.1755 - val_loss: 32.1804\n","Epoch 8/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 32.1756 - val_loss: 32.1805\n","Epoch 9/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 32.1755 - val_loss: 32.1803\n","Epoch 10/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 32.1755 - val_loss: 32.1805\n","Epoch 11/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1756 - val_loss: 32.1802\n","Epoch 12/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 32.1755 - val_loss: 32.1805\n","Epoch 13/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 14/150\n","5377/5377 [==============================] - 5s 889us/step - loss: 32.1755 - val_loss: 32.1803\n","Epoch 15/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1756 - val_loss: 32.1803\n","Epoch 16/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 17/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 32.1756 - val_loss: 32.1803\n","Epoch 18/150\n","5377/5377 [==============================] - 5s 885us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 19/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 32.1755 - val_loss: 32.1803\n","Epoch 20/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 32.1756 - val_loss: 32.1802\n","Epoch 21/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 22/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 32.1755 - val_loss: 32.1801\n","Epoch 23/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 24/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1756 - val_loss: 32.1802\n","Epoch 25/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1756 - val_loss: 32.1802\n","Epoch 26/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 32.1755 - val_loss: 32.1805\n","Epoch 27/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1756 - val_loss: 32.1802\n","Epoch 28/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1755 - val_loss: 32.1803\n","Epoch 29/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1755 - val_loss: 32.1801\n","Epoch 30/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 31/150\n","5377/5377 [==============================] - 5s 885us/step - loss: 32.1755 - val_loss: 32.1803\n","Epoch 32/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1755 - val_loss: 32.1804\n","Epoch 33/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1756 - val_loss: 32.1803\n","Epoch 34/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 32.1755 - val_loss: 32.1801\n","Epoch 35/150\n","5377/5377 [==============================] - 5s 887us/step - loss: 32.1755 - val_loss: 32.1803\n","Epoch 36/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 37/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 32.1756 - val_loss: 32.1803\n","Epoch 38/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 39/150\n","5377/5377 [==============================] - 5s 888us/step - loss: 32.1755 - val_loss: 32.1801\n","Epoch 40/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 32.1755 - val_loss: 32.1803\n","Epoch 41/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 42/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 43/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 32.1755 - val_loss: 32.1801\n","Epoch 44/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 45/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 46/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 47/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1755 - val_loss: 32.1803\n","Epoch 48/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 49/150\n","5377/5377 [==============================] - 5s 891us/step - loss: 32.1755 - val_loss: 32.1803\n","Epoch 50/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 32.1755 - val_loss: 32.1801\n","Epoch 51/150\n","5377/5377 [==============================] - 5s 891us/step - loss: 32.1755 - val_loss: 32.1801\n","Epoch 52/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 53/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 54/150\n","5377/5377 [==============================] - 5s 889us/step - loss: 32.1755 - val_loss: 32.1803\n","Epoch 55/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 56/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 57/150\n","5377/5377 [==============================] - 5s 891us/step - loss: 32.1755 - val_loss: 32.1803\n","Epoch 58/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 32.1755 - val_loss: 32.1801\n","Epoch 59/150\n","5377/5377 [==============================] - 5s 876us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 60/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 61/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 62/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 63/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 32.1754 - val_loss: 32.1803\n","Epoch 64/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 32.1755 - val_loss: 32.1803\n","Epoch 65/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 32.1755 - val_loss: 32.1803\n","Epoch 66/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 32.1756 - val_loss: 32.1803\n","Epoch 67/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 68/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 32.1754 - val_loss: 32.1803\n","Epoch 69/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 32.1755 - val_loss: 32.1803\n","Epoch 70/150\n","5377/5377 [==============================] - 5s 888us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 71/150\n","5377/5377 [==============================] - 5s 888us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 72/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 73/150\n","5377/5377 [==============================] - 5s 888us/step - loss: 32.1755 - val_loss: 32.1800\n","Epoch 74/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 75/150\n","5377/5377 [==============================] - 5s 888us/step - loss: 32.1755 - val_loss: 32.1801\n","Epoch 76/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 32.1755 - val_loss: 32.1800\n","Epoch 77/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 32.1755 - val_loss: 32.1800\n","Epoch 78/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1755 - val_loss: 32.1800\n","Epoch 79/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 32.1754 - val_loss: 32.1800\n","Epoch 80/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1755 - val_loss: 32.1800\n","Epoch 81/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 82/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 32.1754 - val_loss: 32.1800\n","Epoch 83/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 32.1755 - val_loss: 32.1800\n","Epoch 84/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 32.1754 - val_loss: 32.1803\n","Epoch 85/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1755 - val_loss: 32.1801\n","Epoch 86/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 87/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 32.1755 - val_loss: 32.1800\n","Epoch 88/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 32.1755 - val_loss: 32.1800\n","Epoch 89/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 90/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 91/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1755 - val_loss: 32.1801\n","Epoch 92/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 32.1755 - val_loss: 32.1800\n","Epoch 93/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 94/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 95/150\n","5377/5377 [==============================] - 5s 890us/step - loss: 32.1754 - val_loss: 32.1800\n","Epoch 96/150\n","5377/5377 [==============================] - 5s 889us/step - loss: 32.1754 - val_loss: 32.1800\n","Epoch 97/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 98/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 99/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 100/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 101/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 102/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 103/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 104/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 105/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1755 - val_loss: 32.1800\n","Epoch 106/150\n","5377/5377 [==============================] - 5s 885us/step - loss: 32.1755 - val_loss: 32.1801\n","Epoch 107/150\n","5377/5377 [==============================] - 5s 887us/step - loss: 32.1755 - val_loss: 32.1801\n","Epoch 108/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 109/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 110/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 111/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 32.1755 - val_loss: 32.1802\n","Epoch 112/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1755 - val_loss: 32.1801\n","Epoch 113/150\n","5377/5377 [==============================] - 5s 878us/step - loss: 32.1755 - val_loss: 32.1801\n","Epoch 114/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 32.1754 - val_loss: 32.1800\n","Epoch 115/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 116/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 32.1754 - val_loss: 32.1800\n","Epoch 117/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1755 - val_loss: 32.1800\n","Epoch 118/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 119/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 120/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 121/150\n","5377/5377 [==============================] - 5s 885us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 122/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 123/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 124/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 125/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 126/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 127/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 128/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1755 - val_loss: 32.1803\n","Epoch 129/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 130/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1754 - val_loss: 32.1803\n","Epoch 131/150\n","5377/5377 [==============================] - 5s 886us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 132/150\n","5377/5377 [==============================] - 5s 875us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 133/150\n","5377/5377 [==============================] - 5s 892us/step - loss: 32.1755 - val_loss: 32.1800\n","Epoch 134/150\n","5377/5377 [==============================] - 5s 885us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 135/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 32.1755 - val_loss: 32.1800\n","Epoch 136/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 137/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 138/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 139/150\n","5377/5377 [==============================] - 5s 887us/step - loss: 32.1755 - val_loss: 32.1803\n","Epoch 140/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 141/150\n","5377/5377 [==============================] - 5s 879us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 142/150\n","5377/5377 [==============================] - 5s 884us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 143/150\n","5377/5377 [==============================] - 5s 880us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 144/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 145/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 32.1754 - val_loss: 32.1801\n","Epoch 146/150\n","5377/5377 [==============================] - 5s 883us/step - loss: 32.1754 - val_loss: 32.1804\n","Epoch 147/150\n","5377/5377 [==============================] - 5s 882us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 148/150\n","5377/5377 [==============================] - 5s 877us/step - loss: 32.1754 - val_loss: 32.1803\n","Epoch 149/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1754 - val_loss: 32.1802\n","Epoch 150/150\n","5377/5377 [==============================] - 5s 881us/step - loss: 32.1754 - val_loss: 32.1802\n","(lamda,Threshold) 100.0 50.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 0 100.0\n","The shape of N (5975, 784)\n","The minimum value of N  0.0\n","The max value of N 0.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 28, 28, 1)\n","img shape: (280, 280, 1)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 28, 28, 1)\n","img shape: (280, 280, 1)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 100.0 0.9981836100892724\n","=======================\n","Saved model to disk....\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.9981836100892724]\n","AUROC ===== 0.9981836100892724 +/- 0.0\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX0AAAEVCAYAAAAM3jVmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH3BSoTpjECl/AAAAAElFTkSuQmCC\n","text/plain":["<matplotlib.figure.Figure at 0x7f06125568d0>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"ce4jNl9iXmhX","colab_type":"text"},"cell_type":"markdown","source":["# **RCAE-MNIST 7_Vs_all** "]},{"metadata":{"id":"d7MS-COEWoIq","colab_type":"code","outputId":"a65c2e8c-2bf4-4f75-cdd3-cf0af62f8fb2","executionInfo":{"status":"ok","timestamp":1541203602792,"user_tz":-660,"elapsed":4430711,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":102065}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5275, 28, 28, 1)\n","Train Label Shape:  (5275,)\n","Validation Data Shape:  (1054, 28, 28, 1)\n","Validation Label Shape:  (1054,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (62, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (62, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5679 samples, validate on 632 samples\n","Epoch 1/250\n","5679/5679 [==============================] - 6s 1ms/step - loss: 5.0820 - val_loss: 5.2313\n","Epoch 2/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9953 - val_loss: 5.0083\n","Epoch 3/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9772 - val_loss: 4.9816\n","Epoch 4/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9697 - val_loss: 4.9766\n","Epoch 5/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9654 - val_loss: 4.9722\n","Epoch 6/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9630 - val_loss: 4.9698\n","Epoch 7/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9616 - val_loss: 4.9714\n","Epoch 8/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9609 - val_loss: 4.9690\n","Epoch 9/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9594 - val_loss: 4.9653\n","Epoch 10/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9592 - val_loss: 4.9648\n","Epoch 11/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9583 - val_loss: 4.9646\n","Epoch 12/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9577 - val_loss: 4.9630\n","Epoch 13/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9572 - val_loss: 4.9626\n","Epoch 14/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9569 - val_loss: 4.9620\n","Epoch 15/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9569 - val_loss: 4.9618\n","Epoch 16/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9566 - val_loss: 4.9615\n","Epoch 17/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9562 - val_loss: 4.9612\n","Epoch 18/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9560 - val_loss: 4.9642\n","Epoch 19/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9558 - val_loss: 4.9611\n","Epoch 20/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9556 - val_loss: 4.9599\n","Epoch 21/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9560 - val_loss: 4.9626\n","Epoch 22/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9558 - val_loss: 4.9616\n","Epoch 23/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9553 - val_loss: 4.9609\n","Epoch 24/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9550 - val_loss: 4.9606\n","Epoch 25/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9551 - val_loss: 4.9595\n","Epoch 26/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9550 - val_loss: 4.9596\n","Epoch 27/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9547 - val_loss: 4.9590\n","Epoch 28/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9547 - val_loss: 4.9585\n","Epoch 29/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9601\n","Epoch 30/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9586\n","Epoch 31/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 32/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9588\n","Epoch 33/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 34/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 35/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 36/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9584\n","Epoch 37/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 38/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 39/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 40/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 41/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 42/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 43/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 44/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 45/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 46/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 47/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 48/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 49/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 50/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 51/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 52/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 53/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 54/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 55/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 56/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 57/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 58/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 59/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 60/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 61/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 62/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 63/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 64/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 65/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 66/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 67/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 68/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 69/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 70/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 71/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 72/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 73/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 74/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 75/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 76/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 77/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 78/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 79/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 80/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 81/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 82/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 83/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 84/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 85/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 86/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 87/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 88/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 89/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 90/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 91/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 92/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 93/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9783\n","Epoch 94/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9617\n","Epoch 95/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 96/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 97/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 98/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 99/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 100/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 101/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 102/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 103/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 104/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 105/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 106/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 107/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 108/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 109/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 110/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 111/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 112/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 113/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 114/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 115/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 116/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 117/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 118/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 119/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 120/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 121/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 122/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 123/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 124/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 125/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 126/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 127/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 128/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 129/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 130/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 131/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 132/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9551\n","Epoch 133/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 134/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 135/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 136/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 137/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 138/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 139/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 140/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 141/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 142/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 143/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 144/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 145/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 146/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 147/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9551\n","Epoch 148/250\n","5679/5679 [==============================] - 2s 286us/step - loss: 4.9518 - val_loss: 4.9552\n","Epoch 149/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9518 - val_loss: 4.9551\n","Epoch 150/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 151/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9551\n","Epoch 152/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9517 - val_loss: 4.9550\n","Epoch 153/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9552\n","Epoch 154/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9518 - val_loss: 4.9553\n","Epoch 155/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 156/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 157/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9552\n","Epoch 158/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 159/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9551\n","Epoch 160/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9551\n","Epoch 161/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 162/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9549\n","Epoch 163/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9516 - val_loss: 4.9549\n","Epoch 164/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 165/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9549\n","Epoch 166/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9550\n","Epoch 167/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9551\n","Epoch 168/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9517 - val_loss: 4.9550\n","Epoch 169/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 170/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 171/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9550\n","Epoch 172/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9549\n","Epoch 173/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9551\n","Epoch 174/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9550\n","Epoch 175/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9552\n","Epoch 176/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9516 - val_loss: 4.9551\n","Epoch 177/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9549\n","Epoch 178/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 179/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9550\n","Epoch 180/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9516 - val_loss: 4.9549\n","Epoch 181/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 182/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9551\n","Epoch 183/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 184/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9549\n","Epoch 185/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9550\n","Epoch 186/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 187/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9552\n","Epoch 188/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 189/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9516 - val_loss: 4.9549\n","Epoch 190/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 191/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 192/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9516 - val_loss: 4.9551\n","Epoch 193/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 194/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9517 - val_loss: 4.9550\n","Epoch 195/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9551\n","Epoch 196/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 197/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 198/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 199/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9551\n","Epoch 200/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 201/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9516 - val_loss: 4.9549\n","Epoch 202/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 203/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9515 - val_loss: 4.9549\n","Epoch 204/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9516 - val_loss: 4.9551\n","Epoch 205/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9515 - val_loss: 4.9549\n","Epoch 206/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9515 - val_loss: 4.9551\n","Epoch 207/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9551\n","Epoch 208/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9515 - val_loss: 4.9549\n","Epoch 209/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9515 - val_loss: 4.9549\n","Epoch 210/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9515 - val_loss: 4.9551\n","Epoch 211/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9515 - val_loss: 4.9568\n","Epoch 212/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 213/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9549\n","Epoch 214/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 215/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9515 - val_loss: 4.9551\n","Epoch 216/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9515 - val_loss: 4.9550\n","Epoch 217/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9515 - val_loss: 4.9549\n","Epoch 218/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9515 - val_loss: 4.9549\n","Epoch 219/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9549\n","Epoch 220/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9515 - val_loss: 4.9550\n","Epoch 221/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9515 - val_loss: 4.9552\n","Epoch 222/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9548\n","Epoch 223/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9515 - val_loss: 4.9549\n","Epoch 224/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9551\n","Epoch 225/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9515 - val_loss: 4.9551\n","Epoch 226/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9515 - val_loss: 4.9549\n","Epoch 227/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9515 - val_loss: 4.9549\n","Epoch 228/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9515 - val_loss: 4.9550\n","Epoch 229/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9514 - val_loss: 4.9549\n","Epoch 230/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9515 - val_loss: 4.9551\n","Epoch 231/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9552\n","Epoch 232/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9514 - val_loss: 4.9550\n","Epoch 233/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9514 - val_loss: 4.9550\n","Epoch 234/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9515 - val_loss: 4.9550\n","Epoch 235/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9554\n","Epoch 236/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9515 - val_loss: 4.9549\n","Epoch 237/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9514 - val_loss: 4.9549\n","Epoch 238/250\n","5679/5679 [==============================] - 2s 286us/step - loss: 4.9514 - val_loss: 4.9550\n","Epoch 239/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9514 - val_loss: 4.9550\n","Epoch 240/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9515 - val_loss: 4.9550\n","Epoch 241/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9514 - val_loss: 4.9550\n","Epoch 242/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9514 - val_loss: 4.9549\n","Epoch 243/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9514 - val_loss: 4.9549\n","Epoch 244/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9514 - val_loss: 4.9549\n","Epoch 245/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9514 - val_loss: 4.9549\n","Epoch 246/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9514 - val_loss: 4.9549\n","Epoch 247/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9514 - val_loss: 4.9549\n","Epoch 248/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9514 - val_loss: 4.9553\n","Epoch 249/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9514 - val_loss: 4.9552\n","Epoch 250/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9514 - val_loss: 4.9549\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6311, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 26804 0.5\n","The shape of N (6311, 784)\n","The minimum value of N  -0.7485941052436829\n","The max value of N 0.74997478723526\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9938545005910624\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5275, 28, 28, 1)\n","Train Label Shape:  (5275,)\n","Validation Data Shape:  (1054, 28, 28, 1)\n","Validation Label Shape:  (1054,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (62, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (62, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5679 samples, validate on 632 samples\n","Epoch 1/250\n","5679/5679 [==============================] - 4s 710us/step - loss: 5.0750 - val_loss: 5.1403\n","Epoch 2/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9918 - val_loss: 4.9998\n","Epoch 3/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9757 - val_loss: 4.9777\n","Epoch 4/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9689 - val_loss: 4.9736\n","Epoch 5/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9649 - val_loss: 4.9703\n","Epoch 6/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9626 - val_loss: 4.9685\n","Epoch 7/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9607 - val_loss: 4.9651\n","Epoch 8/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9594 - val_loss: 4.9648\n","Epoch 9/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9588 - val_loss: 4.9650\n","Epoch 10/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9578 - val_loss: 4.9632\n","Epoch 11/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9576 - val_loss: 4.9641\n","Epoch 12/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9570 - val_loss: 4.9634\n","Epoch 13/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9565 - val_loss: 4.9621\n","Epoch 14/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9563 - val_loss: 4.9609\n","Epoch 15/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9559 - val_loss: 4.9616\n","Epoch 16/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9558 - val_loss: 4.9626\n","Epoch 17/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9554 - val_loss: 4.9611\n","Epoch 18/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9553 - val_loss: 4.9602\n","Epoch 19/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9551 - val_loss: 4.9607\n","Epoch 20/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9547 - val_loss: 4.9595\n","Epoch 21/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9546 - val_loss: 4.9590\n","Epoch 22/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9546 - val_loss: 4.9593\n","Epoch 23/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 24/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9590\n","Epoch 25/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9598\n","Epoch 26/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 27/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 28/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 29/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9593\n","Epoch 30/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9588\n","Epoch 31/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 32/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 33/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 34/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 35/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 36/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 37/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 38/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 39/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 40/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 41/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 42/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 43/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 44/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 45/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 46/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 47/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 48/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 49/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 50/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 51/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 52/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 53/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 54/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 55/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 56/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 57/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 58/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 59/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 60/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 61/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 62/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 63/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 64/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 65/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 66/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 67/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 68/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 69/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 70/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9549\n","Epoch 71/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 72/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 73/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9550\n","Epoch 74/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9549\n","Epoch 75/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9548\n","Epoch 76/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9520 - val_loss: 4.9548\n","Epoch 77/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 78/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 79/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9549\n","Epoch 80/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 81/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 82/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9548\n","Epoch 83/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 84/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9549\n","Epoch 85/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 86/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9547\n","Epoch 87/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 88/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9551\n","Epoch 89/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9547\n","Epoch 90/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9518 - val_loss: 4.9547\n","Epoch 91/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9548\n","Epoch 92/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 93/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9548\n","Epoch 94/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9549\n","Epoch 95/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9547\n","Epoch 96/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9549\n","Epoch 97/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 98/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9548\n","Epoch 99/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9549\n","Epoch 100/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9548\n","Epoch 101/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9547\n","Epoch 102/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9517 - val_loss: 4.9549\n","Epoch 103/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9547\n","Epoch 104/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 105/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9546\n","Epoch 106/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 107/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9517 - val_loss: 4.9547\n","Epoch 108/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9549\n","Epoch 109/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9516 - val_loss: 4.9548\n","Epoch 110/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9515 - val_loss: 4.9544\n","Epoch 111/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9546\n","Epoch 112/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9547\n","Epoch 113/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9516 - val_loss: 4.9546\n","Epoch 114/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9516 - val_loss: 4.9548\n","Epoch 115/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9546\n","Epoch 116/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9515 - val_loss: 4.9547\n","Epoch 117/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9516 - val_loss: 4.9547\n","Epoch 118/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9516 - val_loss: 4.9547\n","Epoch 119/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9517 - val_loss: 4.9549\n","Epoch 120/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9515 - val_loss: 4.9547\n","Epoch 121/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9547\n","Epoch 122/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9547\n","Epoch 123/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9548\n","Epoch 124/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9516 - val_loss: 4.9549\n","Epoch 125/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9549\n","Epoch 126/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9515 - val_loss: 4.9545\n","Epoch 127/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9546\n","Epoch 128/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9547\n","Epoch 129/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9547\n","Epoch 130/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9516 - val_loss: 4.9555\n","Epoch 131/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9545\n","Epoch 132/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9515 - val_loss: 4.9548\n","Epoch 133/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9515 - val_loss: 4.9547\n","Epoch 134/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9514 - val_loss: 4.9545\n","Epoch 135/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9515 - val_loss: 4.9546\n","Epoch 136/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9515 - val_loss: 4.9545\n","Epoch 137/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9515 - val_loss: 4.9548\n","Epoch 138/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9514 - val_loss: 4.9546\n","Epoch 139/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9515 - val_loss: 4.9544\n","Epoch 140/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9515 - val_loss: 4.9545\n","Epoch 141/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9514 - val_loss: 4.9544\n","Epoch 142/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9513 - val_loss: 4.9545\n","Epoch 143/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9513 - val_loss: 4.9544\n","Epoch 144/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9514 - val_loss: 4.9545\n","Epoch 145/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9514 - val_loss: 4.9545\n","Epoch 146/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9513 - val_loss: 4.9544\n","Epoch 147/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9546\n","Epoch 148/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9514 - val_loss: 4.9545\n","Epoch 149/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9547\n","Epoch 150/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9514 - val_loss: 4.9547\n","Epoch 151/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9514 - val_loss: 4.9546\n","Epoch 152/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9513 - val_loss: 4.9578\n","Epoch 153/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9514 - val_loss: 4.9546\n","Epoch 154/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9513 - val_loss: 4.9545\n","Epoch 155/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9514 - val_loss: 4.9548\n","Epoch 156/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9514 - val_loss: 4.9548\n","Epoch 157/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9514 - val_loss: 4.9547\n","Epoch 158/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9514 - val_loss: 4.9545\n","Epoch 159/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9544\n","Epoch 160/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9513 - val_loss: 4.9547\n","Epoch 161/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9513 - val_loss: 4.9545\n","Epoch 162/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9513 - val_loss: 4.9544\n","Epoch 163/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9513 - val_loss: 4.9546\n","Epoch 164/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9512 - val_loss: 4.9544\n","Epoch 165/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9513 - val_loss: 4.9545\n","Epoch 166/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9513 - val_loss: 4.9544\n","Epoch 167/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9513 - val_loss: 4.9546\n","Epoch 168/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9513 - val_loss: 4.9545\n","Epoch 169/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9513 - val_loss: 4.9545\n","Epoch 170/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9513 - val_loss: 4.9546\n","Epoch 171/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9513 - val_loss: 4.9544\n","Epoch 172/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9547\n","Epoch 173/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9513 - val_loss: 4.9546\n","Epoch 174/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9513 - val_loss: 4.9546\n","Epoch 175/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9512 - val_loss: 4.9547\n","Epoch 176/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9545\n","Epoch 177/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9546\n","Epoch 178/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9546\n","Epoch 179/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9513 - val_loss: 4.9553\n","Epoch 180/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9559\n","Epoch 181/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9546\n","Epoch 182/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9512 - val_loss: 4.9549\n","Epoch 183/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9512 - val_loss: 4.9544\n","Epoch 184/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9512 - val_loss: 4.9544\n","Epoch 185/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9512 - val_loss: 4.9547\n","Epoch 186/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9512 - val_loss: 4.9544\n","Epoch 187/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9511 - val_loss: 4.9546\n","Epoch 188/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9511 - val_loss: 4.9547\n","Epoch 189/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9512 - val_loss: 4.9545\n","Epoch 190/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9513 - val_loss: 4.9546\n","Epoch 191/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9512 - val_loss: 4.9545\n","Epoch 192/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9511 - val_loss: 4.9548\n","Epoch 193/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9512 - val_loss: 4.9549\n","Epoch 194/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9512 - val_loss: 4.9548\n","Epoch 195/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9513 - val_loss: 4.9545\n","Epoch 196/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9512 - val_loss: 4.9545\n","Epoch 197/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9512 - val_loss: 4.9548\n","Epoch 198/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9511 - val_loss: 4.9545\n","Epoch 199/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9511 - val_loss: 4.9547\n","Epoch 200/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9511 - val_loss: 4.9549\n","Epoch 201/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9512 - val_loss: 4.9546\n","Epoch 202/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9512 - val_loss: 4.9547\n","Epoch 203/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9512 - val_loss: 4.9547\n","Epoch 204/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9511 - val_loss: 4.9547\n","Epoch 205/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9512 - val_loss: 4.9547\n","Epoch 206/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9512 - val_loss: 4.9547\n","Epoch 207/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9512 - val_loss: 4.9545\n","Epoch 208/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9512 - val_loss: 4.9546\n","Epoch 209/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9512 - val_loss: 4.9546\n","Epoch 210/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9511 - val_loss: 4.9591\n","Epoch 211/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9511 - val_loss: 4.9551\n","Epoch 212/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9511 - val_loss: 4.9545\n","Epoch 213/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9511 - val_loss: 4.9550\n","Epoch 214/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9511 - val_loss: 4.9547\n","Epoch 215/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9511 - val_loss: 4.9546\n","Epoch 216/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9511 - val_loss: 4.9545\n","Epoch 217/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9511 - val_loss: 4.9545\n","Epoch 218/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9511 - val_loss: 4.9548\n","Epoch 219/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9511 - val_loss: 4.9549\n","Epoch 220/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9510 - val_loss: 4.9545\n","Epoch 221/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9510 - val_loss: 4.9547\n","Epoch 222/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9512 - val_loss: 4.9548\n","Epoch 223/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9511 - val_loss: 4.9545\n","Epoch 224/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9511 - val_loss: 4.9547\n","Epoch 225/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9511 - val_loss: 4.9545\n","Epoch 226/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9510 - val_loss: 4.9546\n","Epoch 227/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9511 - val_loss: 4.9547\n","Epoch 228/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9511 - val_loss: 4.9546\n","Epoch 229/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9510 - val_loss: 4.9547\n","Epoch 230/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9511 - val_loss: 4.9546\n","Epoch 231/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9511 - val_loss: 4.9545\n","Epoch 232/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9510 - val_loss: 4.9546\n","Epoch 233/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9510 - val_loss: 4.9547\n","Epoch 234/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9511 - val_loss: 4.9547\n","Epoch 235/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9510 - val_loss: 4.9546\n","Epoch 236/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9510 - val_loss: 4.9547\n","Epoch 237/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9510 - val_loss: 4.9546\n","Epoch 238/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9510 - val_loss: 4.9546\n","Epoch 239/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9510 - val_loss: 4.9546\n","Epoch 240/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9510 - val_loss: 4.9547\n","Epoch 241/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9510 - val_loss: 4.9547\n","Epoch 242/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9511 - val_loss: 4.9547\n","Epoch 243/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9511 - val_loss: 4.9548\n","Epoch 244/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9510 - val_loss: 4.9546\n","Epoch 245/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9511 - val_loss: 4.9547\n","Epoch 246/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9511 - val_loss: 4.9549\n","Epoch 247/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9511 - val_loss: 4.9553\n","Epoch 248/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9511 - val_loss: 4.9549\n","Epoch 249/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9510 - val_loss: 4.9547\n","Epoch 250/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9510 - val_loss: 4.9546\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6311, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 24727 0.5\n","The shape of N (6311, 784)\n","The minimum value of N  -0.7454293966293335\n","The max value of N 0.7499917149543762\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9872986129393606\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5275, 28, 28, 1)\n","Train Label Shape:  (5275,)\n","Validation Data Shape:  (1054, 28, 28, 1)\n","Validation Label Shape:  (1054,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (62, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (62, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5679 samples, validate on 632 samples\n","Epoch 1/250\n","5679/5679 [==============================] - 5s 803us/step - loss: 5.0646 - val_loss: 5.1508\n","Epoch 2/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9918 - val_loss: 5.0011\n","Epoch 3/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9769 - val_loss: 4.9833\n","Epoch 4/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9702 - val_loss: 4.9736\n","Epoch 5/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9659 - val_loss: 4.9716\n","Epoch 6/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9635 - val_loss: 4.9689\n","Epoch 7/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9622 - val_loss: 4.9662\n","Epoch 8/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9605 - val_loss: 4.9660\n","Epoch 9/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9604 - val_loss: 4.9687\n","Epoch 10/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9592 - val_loss: 4.9634\n","Epoch 11/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9586 - val_loss: 4.9629\n","Epoch 12/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9578 - val_loss: 4.9617\n","Epoch 13/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9582 - val_loss: 4.9650\n","Epoch 14/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9573 - val_loss: 4.9619\n","Epoch 15/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9570 - val_loss: 4.9613\n","Epoch 16/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9569 - val_loss: 4.9609\n","Epoch 17/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9564 - val_loss: 4.9610\n","Epoch 18/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9562 - val_loss: 4.9600\n","Epoch 19/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9555 - val_loss: 4.9598\n","Epoch 20/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9556 - val_loss: 4.9599\n","Epoch 21/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9556 - val_loss: 4.9601\n","Epoch 22/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9552 - val_loss: 4.9594\n","Epoch 23/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9551 - val_loss: 4.9593\n","Epoch 24/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9549 - val_loss: 4.9587\n","Epoch 25/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9553 - val_loss: 4.9594\n","Epoch 26/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9593\n","Epoch 27/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9548 - val_loss: 4.9599\n","Epoch 28/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9614\n","Epoch 29/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9549 - val_loss: 4.9704\n","Epoch 30/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9549 - val_loss: 4.9604\n","Epoch 31/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9545 - val_loss: 4.9590\n","Epoch 32/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9585\n","Epoch 33/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 34/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 35/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 36/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 37/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 38/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 39/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 40/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 41/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9585\n","Epoch 42/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 43/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 44/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 45/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 46/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 47/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 48/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 49/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 50/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 51/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 52/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 53/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 54/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 55/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 56/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 57/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 58/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9727\n","Epoch 59/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9598\n","Epoch 60/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 61/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 62/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 63/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 64/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 65/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 66/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 67/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 68/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 69/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 70/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 71/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 72/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 73/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 74/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 75/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 76/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 77/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 78/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 79/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 80/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 81/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 82/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 83/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9802\n","Epoch 84/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9592\n","Epoch 85/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9591\n","Epoch 86/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 87/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 88/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 89/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 90/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 91/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 92/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 93/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 94/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9583\n","Epoch 95/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 96/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 97/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 98/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 99/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 100/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 101/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 102/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 103/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 104/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 105/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 106/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 107/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 108/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 109/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 110/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 111/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 112/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 113/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 114/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 115/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 116/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 117/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 118/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 119/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 120/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 121/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 122/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 123/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 124/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 125/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 126/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9580\n","Epoch 127/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 128/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 129/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 130/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 131/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 132/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 133/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 134/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 135/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9584\n","Epoch 136/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 137/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 138/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 139/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 140/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 141/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 142/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 143/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 144/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 145/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 146/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 147/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 148/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 149/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 150/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 151/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 152/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 153/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 154/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 155/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 156/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 157/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 158/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9558\n","Epoch 159/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 160/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 161/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 162/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 163/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 164/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 165/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 166/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 167/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 168/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 169/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 170/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 171/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 172/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 173/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 174/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 175/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9566\n","Epoch 176/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 177/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 178/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 179/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 180/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 181/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9558\n","Epoch 182/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9557\n","Epoch 183/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 184/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 185/250\n","5679/5679 [==============================] - 2s 286us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 186/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9519 - val_loss: 4.9557\n","Epoch 187/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 188/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9556\n","Epoch 189/250\n","5679/5679 [==============================] - 2s 285us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 190/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9556\n","Epoch 191/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 192/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 193/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 194/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 195/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 196/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 197/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 198/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 199/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 200/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 201/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 202/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 203/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 204/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9557\n","Epoch 205/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 206/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 207/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 208/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 209/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 210/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9518 - val_loss: 4.9556\n","Epoch 211/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9556\n","Epoch 212/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9556\n","Epoch 213/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9556\n","Epoch 214/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 215/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9562\n","Epoch 216/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9556\n","Epoch 217/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 218/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 219/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 220/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9556\n","Epoch 221/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 222/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 223/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9556\n","Epoch 224/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9556\n","Epoch 225/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 226/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 227/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 228/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9556\n","Epoch 229/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9561\n","Epoch 230/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 231/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9556\n","Epoch 232/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 233/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 234/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 235/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 236/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 237/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 238/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 239/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 240/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9556\n","Epoch 241/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 242/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 243/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9556\n","Epoch 244/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9558\n","Epoch 245/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 246/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9556\n","Epoch 247/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 248/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9556\n","Epoch 249/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9559\n","Epoch 250/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9558\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6311, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 33537 0.5\n","The shape of N (6311, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7499586939811707\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9901351958248803\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5275, 28, 28, 1)\n","Train Label Shape:  (5275,)\n","Validation Data Shape:  (1054, 28, 28, 1)\n","Validation Label Shape:  (1054,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (62, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (62, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5679 samples, validate on 632 samples\n","Epoch 1/250\n","5679/5679 [==============================] - 5s 929us/step - loss: 5.0641 - val_loss: 5.1130\n","Epoch 2/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9910 - val_loss: 5.0145\n","Epoch 3/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9763 - val_loss: 4.9854\n","Epoch 4/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9708 - val_loss: 4.9797\n","Epoch 5/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9666 - val_loss: 4.9752\n","Epoch 6/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9643 - val_loss: 4.9681\n","Epoch 7/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9626 - val_loss: 4.9686\n","Epoch 8/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9614 - val_loss: 4.9661\n","Epoch 9/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9604 - val_loss: 4.9768\n","Epoch 10/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9609 - val_loss: 4.9845\n","Epoch 11/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9591 - val_loss: 4.9690\n","Epoch 12/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9582 - val_loss: 4.9684\n","Epoch 13/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9582 - val_loss: 4.9725\n","Epoch 14/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9586 - val_loss: 4.9644\n","Epoch 15/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9575 - val_loss: 4.9621\n","Epoch 16/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9571 - val_loss: 4.9608\n","Epoch 17/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9570 - val_loss: 4.9630\n","Epoch 18/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9564 - val_loss: 4.9607\n","Epoch 19/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9560 - val_loss: 4.9596\n","Epoch 20/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9557 - val_loss: 4.9851\n","Epoch 21/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9564 - val_loss: 4.9670\n","Epoch 22/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9564 - val_loss: 4.9609\n","Epoch 23/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9560 - val_loss: 4.9613\n","Epoch 24/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9556 - val_loss: 4.9587\n","Epoch 25/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9554 - val_loss: 4.9589\n","Epoch 26/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9572 - val_loss: 4.9633\n","Epoch 27/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9555 - val_loss: 4.9589\n","Epoch 28/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9551 - val_loss: 4.9590\n","Epoch 29/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9555 - val_loss: 4.9584\n","Epoch 30/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9551 - val_loss: 4.9585\n","Epoch 31/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9595\n","Epoch 32/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9593\n","Epoch 33/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 34/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 35/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 36/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 37/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9545 - val_loss: 4.9653\n","Epoch 38/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9602\n","Epoch 39/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9588\n","Epoch 40/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 41/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 42/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 43/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 44/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 45/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 46/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 47/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 48/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 49/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 50/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 51/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 52/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 53/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 54/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 55/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 56/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 57/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 58/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 59/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 60/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 61/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 62/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 63/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 64/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9707\n","Epoch 65/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9585\n","Epoch 66/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 67/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9710\n","Epoch 68/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9646\n","Epoch 69/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 70/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9653\n","Epoch 71/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9590\n","Epoch 72/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 73/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 74/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 75/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 76/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 77/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 78/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 79/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 80/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 81/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 82/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 83/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 84/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 85/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9602\n","Epoch 86/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 87/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 88/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 89/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 90/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 91/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 92/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 93/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 94/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 95/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 96/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 97/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 98/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 99/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 100/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 101/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 102/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 103/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 104/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 105/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 106/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9679\n","Epoch 107/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9632\n","Epoch 108/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 109/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 110/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 111/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 112/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 113/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 114/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 115/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 116/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 117/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 118/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 119/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 120/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 121/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 122/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 123/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 124/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 125/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 126/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9634\n","Epoch 127/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 128/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 129/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 130/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9552\n","Epoch 131/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 132/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 133/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9551\n","Epoch 134/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 135/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 136/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 137/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9589\n","Epoch 138/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 139/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 140/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 141/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 142/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 143/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 144/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 145/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9558\n","Epoch 146/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9571\n","Epoch 147/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 148/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 149/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 150/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 151/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 152/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 153/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 154/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 155/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 156/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 157/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 158/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 159/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 160/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 161/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 162/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 163/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 164/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 165/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9519 - val_loss: 4.9576\n","Epoch 166/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9557\n","Epoch 167/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 168/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 169/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9553\n","Epoch 170/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9551\n","Epoch 171/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 172/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9559\n","Epoch 173/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 174/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 175/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9551\n","Epoch 176/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 177/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9518 - val_loss: 4.9552\n","Epoch 178/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 179/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 180/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 181/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 182/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 183/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 184/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9614\n","Epoch 185/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9559\n","Epoch 186/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 187/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9639\n","Epoch 188/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 189/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 190/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 191/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9558\n","Epoch 192/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 193/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9556\n","Epoch 194/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 195/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 196/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9549\n","Epoch 197/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9548\n","Epoch 198/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9518 - val_loss: 4.9548\n","Epoch 199/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 200/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9549\n","Epoch 201/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9517 - val_loss: 4.9548\n","Epoch 202/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9517 - val_loss: 4.9549\n","Epoch 203/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9517 - val_loss: 4.9550\n","Epoch 204/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 205/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9548\n","Epoch 206/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9518 - val_loss: 4.9549\n","Epoch 207/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9518 - val_loss: 4.9549\n","Epoch 208/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9551\n","Epoch 209/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9553\n","Epoch 210/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9551\n","Epoch 211/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9518 - val_loss: 4.9551\n","Epoch 212/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9517 - val_loss: 4.9550\n","Epoch 213/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9517 - val_loss: 4.9551\n","Epoch 214/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9551\n","Epoch 215/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 216/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 217/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 218/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9516 - val_loss: 4.9551\n","Epoch 219/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 220/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9551\n","Epoch 221/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9551\n","Epoch 222/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 223/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9552\n","Epoch 224/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9515 - val_loss: 4.9551\n","Epoch 225/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9515 - val_loss: 4.9550\n","Epoch 226/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 227/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9516 - val_loss: 4.9551\n","Epoch 228/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9551\n","Epoch 229/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9604\n","Epoch 230/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 231/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 232/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 233/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 234/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9551\n","Epoch 235/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9516 - val_loss: 4.9552\n","Epoch 236/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9577\n","Epoch 237/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9556\n","Epoch 238/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9585\n","Epoch 239/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9571\n","Epoch 240/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 241/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9569\n","Epoch 242/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 243/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 244/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9552\n","Epoch 245/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 246/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 247/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 248/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9551\n","Epoch 249/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 250/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9553\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6311, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 34491 0.5\n","The shape of N (6311, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9787837021665402\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5275, 28, 28, 1)\n","Train Label Shape:  (5275,)\n","Validation Data Shape:  (1054, 28, 28, 1)\n","Validation Label Shape:  (1054,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (62, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (62, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5679 samples, validate on 632 samples\n","Epoch 1/250\n","5679/5679 [==============================] - 6s 1ms/step - loss: 5.0727 - val_loss: 5.2262\n","Epoch 2/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9919 - val_loss: 5.0082\n","Epoch 3/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9760 - val_loss: 4.9814\n","Epoch 4/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9688 - val_loss: 4.9737\n","Epoch 5/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9646 - val_loss: 4.9697\n","Epoch 6/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9619 - val_loss: 4.9672\n","Epoch 7/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9604 - val_loss: 4.9646\n","Epoch 8/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9594 - val_loss: 4.9644\n","Epoch 9/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9588 - val_loss: 4.9633\n","Epoch 10/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9581 - val_loss: 4.9632\n","Epoch 11/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9575 - val_loss: 4.9633\n","Epoch 12/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9568 - val_loss: 4.9630\n","Epoch 13/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9568 - val_loss: 4.9628\n","Epoch 14/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9562 - val_loss: 4.9625\n","Epoch 15/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9559 - val_loss: 4.9615\n","Epoch 16/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9555 - val_loss: 4.9620\n","Epoch 17/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9552 - val_loss: 4.9611\n","Epoch 18/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9552 - val_loss: 4.9594\n","Epoch 19/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9550 - val_loss: 4.9592\n","Epoch 20/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9554 - val_loss: 4.9600\n","Epoch 21/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9548 - val_loss: 4.9634\n","Epoch 22/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9548 - val_loss: 4.9682\n","Epoch 23/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9546 - val_loss: 4.9610\n","Epoch 24/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9604\n","Epoch 25/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9604\n","Epoch 26/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9601\n","Epoch 27/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9587\n","Epoch 28/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9583\n","Epoch 29/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 30/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9592\n","Epoch 31/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 32/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 33/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 34/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9622\n","Epoch 35/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9591\n","Epoch 36/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 37/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 38/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 39/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 40/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 41/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 42/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9570\n","Epoch 43/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 44/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 45/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 46/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9567\n","Epoch 47/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9567\n","Epoch 48/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9570\n","Epoch 49/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 50/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 51/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 52/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 53/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9568\n","Epoch 54/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9560\n","Epoch 55/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9559\n","Epoch 56/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9560\n","Epoch 57/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 58/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9557\n","Epoch 59/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9643\n","Epoch 60/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9587\n","Epoch 61/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 62/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 63/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9556\n","Epoch 64/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 65/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9561\n","Epoch 66/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 67/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9556\n","Epoch 68/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9556\n","Epoch 69/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 70/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9556\n","Epoch 71/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9552\n","Epoch 72/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9551\n","Epoch 73/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9515 - val_loss: 4.9550\n","Epoch 74/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9515 - val_loss: 4.9551\n","Epoch 75/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9514 - val_loss: 4.9561\n","Epoch 76/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9548\n","Epoch 77/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9514 - val_loss: 4.9548\n","Epoch 78/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9514 - val_loss: 4.9548\n","Epoch 79/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9513 - val_loss: 4.9547\n","Epoch 80/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9547\n","Epoch 81/250\n","5679/5679 [==============================] - 2s 286us/step - loss: 4.9513 - val_loss: 4.9551\n","Epoch 82/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9513 - val_loss: 4.9550\n","Epoch 83/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9553\n","Epoch 84/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9513 - val_loss: 4.9549\n","Epoch 85/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9512 - val_loss: 4.9551\n","Epoch 86/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9513 - val_loss: 4.9550\n","Epoch 87/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9513 - val_loss: 4.9547\n","Epoch 88/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9511 - val_loss: 4.9545\n","Epoch 89/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9511 - val_loss: 4.9549\n","Epoch 90/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9512 - val_loss: 4.9547\n","Epoch 91/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9512 - val_loss: 4.9547\n","Epoch 92/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9511 - val_loss: 4.9547\n","Epoch 93/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9511 - val_loss: 4.9548\n","Epoch 94/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9511 - val_loss: 4.9546\n","Epoch 95/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9510 - val_loss: 4.9545\n","Epoch 96/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9510 - val_loss: 4.9546\n","Epoch 97/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9510 - val_loss: 4.9546\n","Epoch 98/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9510 - val_loss: 4.9549\n","Epoch 99/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9510 - val_loss: 4.9546\n","Epoch 100/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9510 - val_loss: 4.9548\n","Epoch 101/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9510 - val_loss: 4.9545\n","Epoch 102/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9510 - val_loss: 4.9547\n","Epoch 103/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9509 - val_loss: 4.9547\n","Epoch 104/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9511 - val_loss: 4.9546\n","Epoch 105/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9510 - val_loss: 4.9550\n","Epoch 106/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9510 - val_loss: 4.9547\n","Epoch 107/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9509 - val_loss: 4.9545\n","Epoch 108/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9509 - val_loss: 4.9547\n","Epoch 109/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9509 - val_loss: 4.9545\n","Epoch 110/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9509 - val_loss: 4.9545\n","Epoch 111/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9508 - val_loss: 4.9545\n","Epoch 112/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9510 - val_loss: 4.9548\n","Epoch 113/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9509 - val_loss: 4.9548\n","Epoch 114/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9508 - val_loss: 4.9546\n","Epoch 115/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9508 - val_loss: 4.9545\n","Epoch 116/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9509 - val_loss: 4.9550\n","Epoch 117/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9509 - val_loss: 4.9547\n","Epoch 118/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9509 - val_loss: 4.9546\n","Epoch 119/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9508 - val_loss: 4.9544\n","Epoch 120/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9509 - val_loss: 4.9543\n","Epoch 121/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9508 - val_loss: 4.9545\n","Epoch 122/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9508 - val_loss: 4.9547\n","Epoch 123/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9508 - val_loss: 4.9544\n","Epoch 124/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9508 - val_loss: 4.9547\n","Epoch 125/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9509 - val_loss: 4.9546\n","Epoch 126/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9507 - val_loss: 4.9544\n","Epoch 127/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9508 - val_loss: 4.9548\n","Epoch 128/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9508 - val_loss: 4.9544\n","Epoch 129/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9508 - val_loss: 4.9543\n","Epoch 130/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9508 - val_loss: 4.9543\n","Epoch 131/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9507 - val_loss: 4.9543\n","Epoch 132/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9507 - val_loss: 4.9548\n","Epoch 133/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9507 - val_loss: 4.9543\n","Epoch 134/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9507 - val_loss: 4.9548\n","Epoch 135/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9509 - val_loss: 4.9544\n","Epoch 136/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9507 - val_loss: 4.9546\n","Epoch 137/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9508 - val_loss: 4.9549\n","Epoch 138/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9508 - val_loss: 4.9544\n","Epoch 139/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9508 - val_loss: 4.9544\n","Epoch 140/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9508 - val_loss: 4.9545\n","Epoch 141/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9507 - val_loss: 4.9547\n","Epoch 142/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9507 - val_loss: 4.9546\n","Epoch 143/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9507 - val_loss: 4.9546\n","Epoch 144/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9507 - val_loss: 4.9546\n","Epoch 145/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9507 - val_loss: 4.9545\n","Epoch 146/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9507 - val_loss: 4.9545\n","Epoch 147/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9507 - val_loss: 4.9543\n","Epoch 148/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9506 - val_loss: 4.9544\n","Epoch 149/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9507 - val_loss: 4.9545\n","Epoch 150/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9506 - val_loss: 4.9545\n","Epoch 151/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9507 - val_loss: 4.9545\n","Epoch 152/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9507 - val_loss: 4.9545\n","Epoch 153/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9506 - val_loss: 4.9543\n","Epoch 154/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9506 - val_loss: 4.9543\n","Epoch 155/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9506 - val_loss: 4.9547\n","Epoch 156/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9506 - val_loss: 4.9555\n","Epoch 157/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9506 - val_loss: 4.9546\n","Epoch 158/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9507 - val_loss: 4.9545\n","Epoch 159/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9506 - val_loss: 4.9546\n","Epoch 160/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9506 - val_loss: 4.9549\n","Epoch 161/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9506 - val_loss: 4.9545\n","Epoch 162/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9505 - val_loss: 4.9544\n","Epoch 163/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9507 - val_loss: 4.9544\n","Epoch 164/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9506 - val_loss: 4.9543\n","Epoch 165/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9506 - val_loss: 4.9544\n","Epoch 166/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9507 - val_loss: 4.9546\n","Epoch 167/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9505 - val_loss: 4.9544\n","Epoch 168/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9505 - val_loss: 4.9545\n","Epoch 169/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9505 - val_loss: 4.9543\n","Epoch 170/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9506 - val_loss: 4.9545\n","Epoch 171/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9506 - val_loss: 4.9545\n","Epoch 172/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9505 - val_loss: 4.9543\n","Epoch 173/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9505 - val_loss: 4.9547\n","Epoch 174/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9506 - val_loss: 4.9544\n","Epoch 175/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9505 - val_loss: 4.9544\n","Epoch 176/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9505 - val_loss: 4.9543\n","Epoch 177/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9505 - val_loss: 4.9547\n","Epoch 178/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9505 - val_loss: 4.9543\n","Epoch 179/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9506 - val_loss: 4.9544\n","Epoch 180/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9505 - val_loss: 4.9545\n","Epoch 181/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9505 - val_loss: 4.9545\n","Epoch 182/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9505 - val_loss: 4.9544\n","Epoch 183/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9504 - val_loss: 4.9545\n","Epoch 184/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9505 - val_loss: 4.9547\n","Epoch 185/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9506 - val_loss: 4.9544\n","Epoch 186/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9505 - val_loss: 4.9544\n","Epoch 187/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9505 - val_loss: 4.9546\n","Epoch 188/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9505 - val_loss: 4.9543\n","Epoch 189/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9505 - val_loss: 4.9544\n","Epoch 190/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9505 - val_loss: 4.9544\n","Epoch 191/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9505 - val_loss: 4.9545\n","Epoch 192/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9504 - val_loss: 4.9543\n","Epoch 193/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9504 - val_loss: 4.9544\n","Epoch 194/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9504 - val_loss: 4.9543\n","Epoch 195/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9504 - val_loss: 4.9545\n","Epoch 196/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9505 - val_loss: 4.9543\n","Epoch 197/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9504 - val_loss: 4.9551\n","Epoch 198/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9505 - val_loss: 4.9548\n","Epoch 199/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9504 - val_loss: 4.9547\n","Epoch 200/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9503 - val_loss: 4.9549\n","Epoch 201/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9504 - val_loss: 4.9544\n","Epoch 202/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9504 - val_loss: 4.9546\n","Epoch 203/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9504 - val_loss: 4.9544\n","Epoch 204/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9504 - val_loss: 4.9543\n","Epoch 205/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9504 - val_loss: 4.9543\n","Epoch 206/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9503 - val_loss: 4.9544\n","Epoch 207/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9504 - val_loss: 4.9544\n","Epoch 208/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9503 - val_loss: 4.9543\n","Epoch 209/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9504 - val_loss: 4.9543\n","Epoch 210/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9504 - val_loss: 4.9544\n","Epoch 211/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9505 - val_loss: 4.9548\n","Epoch 212/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9504 - val_loss: 4.9545\n","Epoch 213/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9504 - val_loss: 4.9550\n","Epoch 214/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9504 - val_loss: 4.9545\n","Epoch 215/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9504 - val_loss: 4.9543\n","Epoch 216/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9503 - val_loss: 4.9544\n","Epoch 217/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9503 - val_loss: 4.9544\n","Epoch 218/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9503 - val_loss: 4.9544\n","Epoch 219/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9503 - val_loss: 4.9543\n","Epoch 220/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9503 - val_loss: 4.9542\n","Epoch 221/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9504 - val_loss: 4.9543\n","Epoch 222/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9503 - val_loss: 4.9544\n","Epoch 223/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9504 - val_loss: 4.9545\n","Epoch 224/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9504 - val_loss: 4.9544\n","Epoch 225/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9504 - val_loss: 4.9544\n","Epoch 226/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9503 - val_loss: 4.9542\n","Epoch 227/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9503 - val_loss: 4.9544\n","Epoch 228/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9503 - val_loss: 4.9543\n","Epoch 229/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9503 - val_loss: 4.9543\n","Epoch 230/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9503 - val_loss: 4.9544\n","Epoch 231/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9503 - val_loss: 4.9546\n","Epoch 232/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9503 - val_loss: 4.9543\n","Epoch 233/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9503 - val_loss: 4.9543\n","Epoch 234/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9503 - val_loss: 4.9546\n","Epoch 235/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9504 - val_loss: 4.9544\n","Epoch 236/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9503 - val_loss: 4.9547\n","Epoch 237/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9503 - val_loss: 4.9543\n","Epoch 238/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9503 - val_loss: 4.9543\n","Epoch 239/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9503 - val_loss: 4.9543\n","Epoch 240/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9503 - val_loss: 4.9544\n","Epoch 241/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9502 - val_loss: 4.9544\n","Epoch 242/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9503 - val_loss: 4.9544\n","Epoch 243/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9503 - val_loss: 4.9545\n","Epoch 244/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9504 - val_loss: 4.9545\n","Epoch 245/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9503 - val_loss: 4.9599\n","Epoch 246/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9503 - val_loss: 4.9544\n","Epoch 247/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9502 - val_loss: 4.9547\n","Epoch 248/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9503 - val_loss: 4.9543\n","Epoch 249/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9502 - val_loss: 4.9543\n","Epoch 250/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9502 - val_loss: 4.9545\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6311, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 29956 0.5\n","The shape of N (6311, 784)\n","The minimum value of N  -0.7479179501533508\n","The max value of N 0.7499659061431885\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9950856653193543\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5275, 28, 28, 1)\n","Train Label Shape:  (5275,)\n","Validation Data Shape:  (1054, 28, 28, 1)\n","Validation Label Shape:  (1054,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (62, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (62, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5679 samples, validate on 632 samples\n","Epoch 1/250\n","5679/5679 [==============================] - 6s 1ms/step - loss: 5.0761 - val_loss: 5.2781\n","Epoch 2/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9943 - val_loss: 5.0185\n","Epoch 3/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9783 - val_loss: 4.9866\n","Epoch 4/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9712 - val_loss: 4.9773\n","Epoch 5/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9663 - val_loss: 4.9840\n","Epoch 6/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9636 - val_loss: 4.9734\n","Epoch 7/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9622 - val_loss: 4.9715\n","Epoch 8/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9609 - val_loss: 4.9694\n","Epoch 9/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9597 - val_loss: 4.9655\n","Epoch 10/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9592 - val_loss: 4.9653\n","Epoch 11/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9587 - val_loss: 4.9655\n","Epoch 12/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9584 - val_loss: 4.9634\n","Epoch 13/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9575 - val_loss: 4.9632\n","Epoch 14/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9574 - val_loss: 4.9630\n","Epoch 15/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9571 - val_loss: 4.9650\n","Epoch 16/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9570 - val_loss: 4.9624\n","Epoch 17/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9570 - val_loss: 4.9636\n","Epoch 18/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9566 - val_loss: 4.9625\n","Epoch 19/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9568 - val_loss: 4.9655\n","Epoch 20/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9559 - val_loss: 4.9623\n","Epoch 21/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9559 - val_loss: 4.9621\n","Epoch 22/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9556 - val_loss: 4.9627\n","Epoch 23/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9555 - val_loss: 4.9606\n","Epoch 24/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9607\n","Epoch 25/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9553 - val_loss: 4.9604\n","Epoch 26/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9550 - val_loss: 4.9593\n","Epoch 27/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9554 - val_loss: 4.9659\n","Epoch 28/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9571 - val_loss: 4.9819\n","Epoch 29/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9557 - val_loss: 4.9677\n","Epoch 30/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9551 - val_loss: 4.9611\n","Epoch 31/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9550 - val_loss: 4.9605\n","Epoch 32/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9548 - val_loss: 4.9595\n","Epoch 33/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9588\n","Epoch 34/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9546 - val_loss: 4.9587\n","Epoch 35/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 36/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 37/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 38/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 39/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 40/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 41/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 42/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 43/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 44/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 45/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 46/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 47/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 48/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 49/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 50/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 51/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 52/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 53/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 54/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 55/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 56/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 57/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 58/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 59/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 60/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 61/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 62/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 63/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 64/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 65/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 66/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 67/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 68/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 69/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 70/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 71/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 72/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 73/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 74/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 75/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 76/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 77/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 78/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 79/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 80/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 81/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 82/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 83/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 84/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 85/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 86/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 87/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 88/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 89/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 90/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 91/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 92/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 93/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 94/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 95/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 96/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 97/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 98/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 99/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 100/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 101/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 102/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 103/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 104/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 105/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 106/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 107/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 108/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 109/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 110/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 111/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 112/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 113/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 114/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 115/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 116/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 117/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 118/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 119/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 120/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 121/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 122/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 123/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 124/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 125/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 126/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 127/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 128/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 129/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 130/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 131/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 132/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9542 - val_loss: 4.9647\n","Epoch 133/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9591\n","Epoch 134/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 135/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 136/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 137/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 138/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 139/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 140/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 141/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 142/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 143/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 144/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 145/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 146/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 147/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 148/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 149/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 150/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 151/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 152/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 153/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 154/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 155/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 156/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 157/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 158/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 159/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 160/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 161/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 162/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 163/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 164/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 165/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 166/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 167/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 168/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 169/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 170/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 171/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 172/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 173/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 174/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 175/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 176/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 177/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 178/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 179/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 180/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 181/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 182/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 183/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 184/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 185/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 186/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 187/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 188/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 189/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 190/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 191/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 192/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 193/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 194/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 195/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 196/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 197/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 198/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 199/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 200/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 201/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 202/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 203/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 204/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 205/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 206/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 207/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 208/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 209/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 210/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 211/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 212/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 213/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9553\n","Epoch 214/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 215/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 216/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 217/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 218/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 219/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 220/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 221/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 222/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 223/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 224/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 225/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9518 - val_loss: 4.9559\n","Epoch 226/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9552\n","Epoch 227/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 228/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9569\n","Epoch 229/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 230/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 231/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 232/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 233/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 234/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 235/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9551\n","Epoch 236/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9566\n","Epoch 237/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 238/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 239/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 240/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 241/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9569\n","Epoch 242/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 243/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 244/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 245/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9551\n","Epoch 246/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9518 - val_loss: 4.9553\n","Epoch 247/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9518 - val_loss: 4.9552\n","Epoch 248/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 249/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9518 - val_loss: 4.9552\n","Epoch 250/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9518 - val_loss: 4.9551\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6311, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 28307 0.5\n","The shape of N (6311, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9896938348845492\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5275, 28, 28, 1)\n","Train Label Shape:  (5275,)\n","Validation Data Shape:  (1054, 28, 28, 1)\n","Validation Label Shape:  (1054,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (62, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (62, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5679 samples, validate on 632 samples\n","Epoch 1/250\n","5679/5679 [==============================] - 7s 1ms/step - loss: 5.0635 - val_loss: 5.2004\n","Epoch 2/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9903 - val_loss: 5.0172\n","Epoch 3/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9761 - val_loss: 4.9861\n","Epoch 4/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9695 - val_loss: 4.9829\n","Epoch 5/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9662 - val_loss: 4.9755\n","Epoch 6/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9637 - val_loss: 4.9704\n","Epoch 7/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9633 - val_loss: 4.9666\n","Epoch 8/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9609 - val_loss: 4.9656\n","Epoch 9/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9597 - val_loss: 4.9629\n","Epoch 10/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9594 - val_loss: 4.9625\n","Epoch 11/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9594 - val_loss: 4.9770\n","Epoch 12/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9597 - val_loss: 4.9697\n","Epoch 13/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9586 - val_loss: 4.9631\n","Epoch 14/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9579 - val_loss: 4.9617\n","Epoch 15/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9574 - val_loss: 4.9605\n","Epoch 16/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9568 - val_loss: 4.9597\n","Epoch 17/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9568 - val_loss: 4.9602\n","Epoch 18/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9562 - val_loss: 4.9593\n","Epoch 19/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9563 - val_loss: 4.9591\n","Epoch 20/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9568 - val_loss: 4.9632\n","Epoch 21/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9562 - val_loss: 4.9610\n","Epoch 22/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9557 - val_loss: 4.9586\n","Epoch 23/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9554 - val_loss: 4.9621\n","Epoch 24/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9552 - val_loss: 4.9598\n","Epoch 25/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9555 - val_loss: 4.9602\n","Epoch 26/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9553 - val_loss: 4.9591\n","Epoch 27/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9586\n","Epoch 28/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9583\n","Epoch 29/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 30/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9578 - val_loss: 4.9755\n","Epoch 31/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9590 - val_loss: 4.9651\n","Epoch 32/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9560 - val_loss: 4.9612\n","Epoch 33/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9554 - val_loss: 4.9587\n","Epoch 34/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9550 - val_loss: 4.9588\n","Epoch 35/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9597\n","Epoch 36/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9588\n","Epoch 37/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 38/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 39/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 40/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 41/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 42/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 43/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 44/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 45/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 46/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9593\n","Epoch 47/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9585\n","Epoch 48/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 49/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 50/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 51/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9689\n","Epoch 52/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9583\n","Epoch 53/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 54/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 55/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 56/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 57/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 58/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 59/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9558\n","Epoch 60/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 61/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 62/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9554\n","Epoch 63/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9554\n","Epoch 64/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9553\n","Epoch 65/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 66/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 67/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 68/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 69/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 70/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9552\n","Epoch 71/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9552\n","Epoch 72/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 73/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9552\n","Epoch 74/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9551\n","Epoch 75/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 76/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 77/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9551\n","Epoch 78/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9551\n","Epoch 79/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9551\n","Epoch 80/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 81/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9550\n","Epoch 82/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9550\n","Epoch 83/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9552\n","Epoch 84/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9550\n","Epoch 85/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9551\n","Epoch 86/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9549\n","Epoch 87/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9548\n","Epoch 88/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9551\n","Epoch 89/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9548\n","Epoch 90/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9550\n","Epoch 91/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9548\n","Epoch 92/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9549\n","Epoch 93/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9549\n","Epoch 94/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9550\n","Epoch 95/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 96/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9608\n","Epoch 97/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 98/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 99/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 100/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 101/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9549\n","Epoch 102/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9547\n","Epoch 103/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9547\n","Epoch 104/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9547\n","Epoch 105/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9550\n","Epoch 106/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9547\n","Epoch 107/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9548\n","Epoch 108/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9546\n","Epoch 109/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9548\n","Epoch 110/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9546\n","Epoch 111/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9546\n","Epoch 112/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9547\n","Epoch 113/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9548\n","Epoch 114/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9548\n","Epoch 115/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9546\n","Epoch 116/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9547\n","Epoch 117/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9545\n","Epoch 118/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9752\n","Epoch 119/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9597\n","Epoch 120/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 121/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 122/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 123/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9550\n","Epoch 124/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9547\n","Epoch 125/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9547\n","Epoch 126/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9545\n","Epoch 127/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9519 - val_loss: 4.9545\n","Epoch 128/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 129/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9520 - val_loss: 4.9547\n","Epoch 130/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9548\n","Epoch 131/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9549\n","Epoch 132/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9546\n","Epoch 133/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9519 - val_loss: 4.9547\n","Epoch 134/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9519 - val_loss: 4.9546\n","Epoch 135/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9546\n","Epoch 136/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9518 - val_loss: 4.9544\n","Epoch 137/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 138/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 139/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9518 - val_loss: 4.9549\n","Epoch 140/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9518 - val_loss: 4.9548\n","Epoch 141/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 142/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9544\n","Epoch 143/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9551\n","Epoch 144/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9519 - val_loss: 4.9547\n","Epoch 145/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 146/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9545\n","Epoch 147/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9544\n","Epoch 148/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 149/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9545\n","Epoch 150/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9547\n","Epoch 151/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9517 - val_loss: 4.9545\n","Epoch 152/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 153/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9544\n","Epoch 154/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9546\n","Epoch 155/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9547\n","Epoch 156/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 157/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9544\n","Epoch 158/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9544\n","Epoch 159/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9546\n","Epoch 160/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9546\n","Epoch 161/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9546\n","Epoch 162/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9549\n","Epoch 163/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9517 - val_loss: 4.9548\n","Epoch 164/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9544\n","Epoch 165/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9544\n","Epoch 166/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9517 - val_loss: 4.9545\n","Epoch 167/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 168/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9515 - val_loss: 4.9544\n","Epoch 169/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9515 - val_loss: 4.9545\n","Epoch 170/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9545\n","Epoch 171/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 172/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9557\n","Epoch 173/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9517 - val_loss: 4.9548\n","Epoch 174/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 175/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 176/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 177/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9546\n","Epoch 178/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9543\n","Epoch 179/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9544\n","Epoch 180/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9543\n","Epoch 181/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9517 - val_loss: 4.9544\n","Epoch 182/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 183/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9544\n","Epoch 184/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9516 - val_loss: 4.9546\n","Epoch 185/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9543\n","Epoch 186/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 187/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 188/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9517 - val_loss: 4.9544\n","Epoch 189/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 190/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9515 - val_loss: 4.9545\n","Epoch 191/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9544\n","Epoch 192/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9544\n","Epoch 193/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 194/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9515 - val_loss: 4.9547\n","Epoch 195/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9514 - val_loss: 4.9545\n","Epoch 196/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9514 - val_loss: 4.9543\n","Epoch 197/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9515 - val_loss: 4.9543\n","Epoch 198/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 199/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9515 - val_loss: 4.9543\n","Epoch 200/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9514 - val_loss: 4.9544\n","Epoch 201/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9515 - val_loss: 4.9546\n","Epoch 202/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9515 - val_loss: 4.9543\n","Epoch 203/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9515 - val_loss: 4.9545\n","Epoch 204/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9549\n","Epoch 205/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9515 - val_loss: 4.9545\n","Epoch 206/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9515 - val_loss: 4.9619\n","Epoch 207/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9551\n","Epoch 208/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9549\n","Epoch 209/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 210/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9514 - val_loss: 4.9545\n","Epoch 211/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9514 - val_loss: 4.9544\n","Epoch 212/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9514 - val_loss: 4.9543\n","Epoch 213/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9514 - val_loss: 4.9544\n","Epoch 214/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9546\n","Epoch 215/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9544\n","Epoch 216/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9514 - val_loss: 4.9544\n","Epoch 217/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9514 - val_loss: 4.9543\n","Epoch 218/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9514 - val_loss: 4.9543\n","Epoch 219/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9514 - val_loss: 4.9543\n","Epoch 220/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9514 - val_loss: 4.9543\n","Epoch 221/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9513 - val_loss: 4.9543\n","Epoch 222/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9513 - val_loss: 4.9545\n","Epoch 223/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9513 - val_loss: 4.9543\n","Epoch 224/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9514 - val_loss: 4.9546\n","Epoch 225/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9515 - val_loss: 4.9544\n","Epoch 226/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9513 - val_loss: 4.9544\n","Epoch 227/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9514 - val_loss: 4.9543\n","Epoch 228/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9514 - val_loss: 4.9544\n","Epoch 229/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9514 - val_loss: 4.9545\n","Epoch 230/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9514 - val_loss: 4.9544\n","Epoch 231/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9513 - val_loss: 4.9543\n","Epoch 232/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9513 - val_loss: 4.9544\n","Epoch 233/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9513 - val_loss: 4.9543\n","Epoch 234/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9513 - val_loss: 4.9545\n","Epoch 235/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9513 - val_loss: 4.9544\n","Epoch 236/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9515 - val_loss: 4.9545\n","Epoch 237/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9514 - val_loss: 4.9544\n","Epoch 238/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9513 - val_loss: 4.9544\n","Epoch 239/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9513 - val_loss: 4.9544\n","Epoch 240/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9513 - val_loss: 4.9544\n","Epoch 241/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9512 - val_loss: 4.9543\n","Epoch 242/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9513 - val_loss: 4.9544\n","Epoch 243/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9513 - val_loss: 4.9543\n","Epoch 244/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9512 - val_loss: 4.9544\n","Epoch 245/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9513 - val_loss: 4.9543\n","Epoch 246/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9513 - val_loss: 4.9544\n","Epoch 247/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9513 - val_loss: 4.9544\n","Epoch 248/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9512 - val_loss: 4.9543\n","Epoch 249/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9513 - val_loss: 4.9544\n","Epoch 250/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9513 - val_loss: 4.9543\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6311, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 29767 0.5\n","The shape of N (6311, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9936660833475291\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5275, 28, 28, 1)\n","Train Label Shape:  (5275,)\n","Validation Data Shape:  (1054, 28, 28, 1)\n","Validation Label Shape:  (1054,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (62, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (62, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5679 samples, validate on 632 samples\n","Epoch 1/250\n","5679/5679 [==============================] - 8s 1ms/step - loss: 5.0707 - val_loss: 5.1853\n","Epoch 2/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9904 - val_loss: 5.0060\n","Epoch 3/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9754 - val_loss: 4.9789\n","Epoch 4/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9689 - val_loss: 4.9736\n","Epoch 5/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9650 - val_loss: 4.9702\n","Epoch 6/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9626 - val_loss: 4.9693\n","Epoch 7/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9608 - val_loss: 4.9666\n","Epoch 8/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9597 - val_loss: 4.9705\n","Epoch 9/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9589 - val_loss: 4.9658\n","Epoch 10/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9583 - val_loss: 4.9643\n","Epoch 11/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9575 - val_loss: 4.9625\n","Epoch 12/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9571 - val_loss: 4.9628\n","Epoch 13/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9569 - val_loss: 4.9636\n","Epoch 14/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9567 - val_loss: 4.9623\n","Epoch 15/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9563 - val_loss: 4.9618\n","Epoch 16/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9560 - val_loss: 4.9634\n","Epoch 17/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9559 - val_loss: 4.9622\n","Epoch 18/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9557 - val_loss: 4.9606\n","Epoch 19/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9554 - val_loss: 4.9617\n","Epoch 20/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9552 - val_loss: 4.9601\n","Epoch 21/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9551 - val_loss: 4.9598\n","Epoch 22/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9607\n","Epoch 23/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9548 - val_loss: 4.9606\n","Epoch 24/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9594\n","Epoch 25/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9602\n","Epoch 26/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9546 - val_loss: 4.9603\n","Epoch 27/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9544 - val_loss: 4.9593\n","Epoch 28/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9601\n","Epoch 29/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9595\n","Epoch 30/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9589\n","Epoch 31/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9592\n","Epoch 32/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 33/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 34/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 35/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 36/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 37/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 38/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9584\n","Epoch 39/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 40/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 41/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 42/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 43/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 44/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 45/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 46/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9599\n","Epoch 47/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 48/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 49/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 50/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 51/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 52/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 53/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 54/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 55/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 56/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 57/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 58/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 59/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 60/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 61/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 62/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 63/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 64/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 65/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 66/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 67/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 68/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 69/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 70/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9521 - val_loss: 4.9559\n","Epoch 71/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9520 - val_loss: 4.9559\n","Epoch 72/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 73/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 74/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9521 - val_loss: 4.9560\n","Epoch 75/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9558\n","Epoch 76/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9560\n","Epoch 77/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 78/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 79/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9558\n","Epoch 80/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9520 - val_loss: 4.9559\n","Epoch 81/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9558\n","Epoch 82/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9557\n","Epoch 83/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9559\n","Epoch 84/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9557\n","Epoch 85/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9559\n","Epoch 86/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9560\n","Epoch 87/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 88/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9558\n","Epoch 89/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9558\n","Epoch 90/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9557\n","Epoch 91/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9557\n","Epoch 92/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 93/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9635\n","Epoch 94/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9607\n","Epoch 95/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 96/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9568\n","Epoch 97/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9566\n","Epoch 98/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9520 - val_loss: 4.9562\n","Epoch 99/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9558\n","Epoch 100/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 101/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 102/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9557\n","Epoch 103/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9557\n","Epoch 104/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 105/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9557\n","Epoch 106/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 107/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 108/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 109/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 110/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 111/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 112/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 113/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 114/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 115/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 116/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 117/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9516 - val_loss: 4.9554\n","Epoch 118/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 119/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 120/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 121/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 122/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9556\n","Epoch 123/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9554\n","Epoch 124/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 125/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 126/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 127/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 128/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9555\n","Epoch 129/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 130/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 131/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9553\n","Epoch 132/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9514 - val_loss: 4.9553\n","Epoch 133/250\n","5679/5679 [==============================] - 2s 288us/step - loss: 4.9515 - val_loss: 4.9554\n","Epoch 134/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 135/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9515 - val_loss: 4.9554\n","Epoch 136/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9515 - val_loss: 4.9554\n","Epoch 137/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9515 - val_loss: 4.9553\n","Epoch 138/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9515 - val_loss: 4.9552\n","Epoch 139/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9515 - val_loss: 4.9557\n","Epoch 140/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9515 - val_loss: 4.9557\n","Epoch 141/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9515 - val_loss: 4.9552\n","Epoch 142/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9515 - val_loss: 4.9553\n","Epoch 143/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9553\n","Epoch 144/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9514 - val_loss: 4.9553\n","Epoch 145/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9515 - val_loss: 4.9554\n","Epoch 146/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9515 - val_loss: 4.9554\n","Epoch 147/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9514 - val_loss: 4.9553\n","Epoch 148/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9514 - val_loss: 4.9552\n","Epoch 149/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9514 - val_loss: 4.9553\n","Epoch 150/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 151/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9515 - val_loss: 4.9553\n","Epoch 152/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9514 - val_loss: 4.9555\n","Epoch 153/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9514 - val_loss: 4.9553\n","Epoch 154/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9514 - val_loss: 4.9553\n","Epoch 155/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9514 - val_loss: 4.9552\n","Epoch 156/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9514 - val_loss: 4.9557\n","Epoch 157/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9514 - val_loss: 4.9553\n","Epoch 158/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9514 - val_loss: 4.9552\n","Epoch 159/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9553\n","Epoch 160/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9515 - val_loss: 4.9553\n","Epoch 161/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9514 - val_loss: 4.9552\n","Epoch 162/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9513 - val_loss: 4.9552\n","Epoch 163/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9513 - val_loss: 4.9552\n","Epoch 164/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9514 - val_loss: 4.9552\n","Epoch 165/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9514 - val_loss: 4.9554\n","Epoch 166/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9514 - val_loss: 4.9554\n","Epoch 167/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9514 - val_loss: 4.9553\n","Epoch 168/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9513 - val_loss: 4.9552\n","Epoch 169/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9553\n","Epoch 170/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9513 - val_loss: 4.9554\n","Epoch 171/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9513 - val_loss: 4.9552\n","Epoch 172/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9513 - val_loss: 4.9552\n","Epoch 173/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9513 - val_loss: 4.9552\n","Epoch 174/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9513 - val_loss: 4.9552\n","Epoch 175/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9513 - val_loss: 4.9553\n","Epoch 176/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9571\n","Epoch 177/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9513 - val_loss: 4.9553\n","Epoch 178/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9513 - val_loss: 4.9552\n","Epoch 179/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9552\n","Epoch 180/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9513 - val_loss: 4.9554\n","Epoch 181/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9512 - val_loss: 4.9552\n","Epoch 182/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9513 - val_loss: 4.9553\n","Epoch 183/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9513 - val_loss: 4.9555\n","Epoch 184/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9513 - val_loss: 4.9552\n","Epoch 185/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9513 - val_loss: 4.9552\n","Epoch 186/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9512 - val_loss: 4.9583\n","Epoch 187/250\n","5679/5679 [==============================] - 2s 287us/step - loss: 4.9514 - val_loss: 4.9556\n","Epoch 188/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9514 - val_loss: 4.9554\n","Epoch 189/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9512 - val_loss: 4.9552\n","Epoch 190/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9552\n","Epoch 191/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9512 - val_loss: 4.9554\n","Epoch 192/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9513 - val_loss: 4.9552\n","Epoch 193/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9513 - val_loss: 4.9554\n","Epoch 194/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9552\n","Epoch 195/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9512 - val_loss: 4.9552\n","Epoch 196/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9512 - val_loss: 4.9553\n","Epoch 197/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9512 - val_loss: 4.9553\n","Epoch 198/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9513 - val_loss: 4.9552\n","Epoch 199/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9513 - val_loss: 4.9554\n","Epoch 200/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9511 - val_loss: 4.9552\n","Epoch 201/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9512 - val_loss: 4.9553\n","Epoch 202/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9512 - val_loss: 4.9551\n","Epoch 203/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9512 - val_loss: 4.9556\n","Epoch 204/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9512 - val_loss: 4.9552\n","Epoch 205/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9512 - val_loss: 4.9554\n","Epoch 206/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9512 - val_loss: 4.9553\n","Epoch 207/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9512 - val_loss: 4.9555\n","Epoch 208/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9511 - val_loss: 4.9553\n","Epoch 209/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9511 - val_loss: 4.9553\n","Epoch 210/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9512 - val_loss: 4.9552\n","Epoch 211/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9511 - val_loss: 4.9552\n","Epoch 212/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9512 - val_loss: 4.9552\n","Epoch 213/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9512 - val_loss: 4.9553\n","Epoch 214/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9512 - val_loss: 4.9554\n","Epoch 215/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9512 - val_loss: 4.9553\n","Epoch 216/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9512 - val_loss: 4.9553\n","Epoch 217/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9511 - val_loss: 4.9555\n","Epoch 218/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9512 - val_loss: 4.9553\n","Epoch 219/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9511 - val_loss: 4.9553\n","Epoch 220/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9512 - val_loss: 4.9554\n","Epoch 221/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9511 - val_loss: 4.9553\n","Epoch 222/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9512 - val_loss: 4.9554\n","Epoch 223/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9513 - val_loss: 4.9557\n","Epoch 224/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9512 - val_loss: 4.9553\n","Epoch 225/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9511 - val_loss: 4.9555\n","Epoch 226/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9512 - val_loss: 4.9553\n","Epoch 227/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9512 - val_loss: 4.9554\n","Epoch 228/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9511 - val_loss: 4.9554\n","Epoch 229/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9512 - val_loss: 4.9556\n","Epoch 230/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9512 - val_loss: 4.9554\n","Epoch 231/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9511 - val_loss: 4.9557\n","Epoch 232/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9511 - val_loss: 4.9553\n","Epoch 233/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9511 - val_loss: 4.9554\n","Epoch 234/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9511 - val_loss: 4.9555\n","Epoch 235/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9511 - val_loss: 4.9554\n","Epoch 236/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9511 - val_loss: 4.9555\n","Epoch 237/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9511 - val_loss: 4.9552\n","Epoch 238/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9512 - val_loss: 4.9557\n","Epoch 239/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9512 - val_loss: 4.9552\n","Epoch 240/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9511 - val_loss: 4.9552\n","Epoch 241/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9511 - val_loss: 4.9557\n","Epoch 242/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9511 - val_loss: 4.9553\n","Epoch 243/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9511 - val_loss: 4.9553\n","Epoch 244/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9511 - val_loss: 4.9553\n","Epoch 245/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9511 - val_loss: 4.9553\n","Epoch 246/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9511 - val_loss: 4.9552\n","Epoch 247/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9511 - val_loss: 4.9552\n","Epoch 248/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9511 - val_loss: 4.9553\n","Epoch 249/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9511 - val_loss: 4.9553\n","Epoch 250/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9511 - val_loss: 4.9554\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6311, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 28705 0.5\n","The shape of N (6311, 784)\n","The minimum value of N  -0.7441396713256836\n","The max value of N 0.7499656081199646\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9977983574146058\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5275, 28, 28, 1)\n","Train Label Shape:  (5275,)\n","Validation Data Shape:  (1054, 28, 28, 1)\n","Validation Label Shape:  (1054,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (62, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (62, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5679 samples, validate on 632 samples\n","Epoch 1/250\n","5679/5679 [==============================] - 9s 2ms/step - loss: 5.0762 - val_loss: 5.2954\n","Epoch 2/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9953 - val_loss: 5.0093\n","Epoch 3/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9782 - val_loss: 4.9875\n","Epoch 4/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9706 - val_loss: 4.9825\n","Epoch 5/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9674 - val_loss: 4.9777\n","Epoch 6/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9643 - val_loss: 4.9773\n","Epoch 7/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9625 - val_loss: 4.9791\n","Epoch 8/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9623 - val_loss: 4.9752\n","Epoch 9/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9611 - val_loss: 4.9719\n","Epoch 10/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9601 - val_loss: 4.9711\n","Epoch 11/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9594 - val_loss: 4.9688\n","Epoch 12/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9589 - val_loss: 4.9672\n","Epoch 13/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9583 - val_loss: 4.9671\n","Epoch 14/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9594 - val_loss: 4.9723\n","Epoch 15/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9582 - val_loss: 4.9703\n","Epoch 16/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9585 - val_loss: 4.9683\n","Epoch 17/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9576 - val_loss: 4.9697\n","Epoch 18/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9571 - val_loss: 4.9673\n","Epoch 19/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9567 - val_loss: 4.9643\n","Epoch 20/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9565 - val_loss: 4.9631\n","Epoch 21/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9571 - val_loss: 4.9735\n","Epoch 22/250\n","5679/5679 [==============================] - 2s 307us/step - loss: 4.9566 - val_loss: 4.9700\n","Epoch 23/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9560 - val_loss: 4.9660\n","Epoch 24/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9562 - val_loss: 4.9693\n","Epoch 25/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9559 - val_loss: 4.9665\n","Epoch 26/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9558 - val_loss: 4.9627\n","Epoch 27/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9556 - val_loss: 4.9621\n","Epoch 28/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9555 - val_loss: 4.9618\n","Epoch 29/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9553 - val_loss: 4.9610\n","Epoch 30/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9551 - val_loss: 4.9605\n","Epoch 31/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9551 - val_loss: 4.9610\n","Epoch 32/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9550 - val_loss: 4.9594\n","Epoch 33/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9550 - val_loss: 4.9600\n","Epoch 34/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9549 - val_loss: 4.9591\n","Epoch 35/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9603\n","Epoch 36/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9592\n","Epoch 37/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9587\n","Epoch 38/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9584\n","Epoch 39/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 40/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 41/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 42/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 43/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 44/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 45/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 46/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 47/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 48/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 49/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 50/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 51/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 52/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 53/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9593\n","Epoch 54/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9555 - val_loss: 4.9692\n","Epoch 55/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9623\n","Epoch 56/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9610\n","Epoch 57/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 58/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 59/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 60/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 61/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9544 - val_loss: 4.9590\n","Epoch 62/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 63/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 64/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 65/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 66/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 67/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9610\n","Epoch 68/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 69/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 70/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 71/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 72/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 73/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 74/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 75/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 76/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 77/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 78/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 79/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 80/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 81/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 82/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 83/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 84/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 85/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 86/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 87/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 88/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 89/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 90/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 91/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 92/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 93/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 94/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 95/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 96/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 97/250\n","5679/5679 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 98/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 99/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 100/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 101/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 102/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 103/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 104/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 105/250\n","5679/5679 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 106/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 107/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 108/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 109/250\n","5679/5679 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 110/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 111/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 112/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 113/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 114/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 115/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 116/250\n","5679/5679 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 117/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 118/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 119/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 120/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 121/250\n","5679/5679 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 122/250\n","5679/5679 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 123/250\n","5679/5679 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 124/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 125/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 126/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 127/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 128/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 129/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 130/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 131/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 132/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 133/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 134/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 135/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 136/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 137/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 138/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 139/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 140/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 141/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 142/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 143/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 144/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 145/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 146/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 147/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 148/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 149/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 150/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 151/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 152/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 153/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 154/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 155/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 156/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 157/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 158/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 159/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 160/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 161/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 162/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 163/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 164/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 165/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 166/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 167/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 168/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 169/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 170/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9566\n","Epoch 171/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 172/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 173/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 174/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 175/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 176/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 177/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 178/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9566\n","Epoch 179/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 180/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 181/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 182/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 183/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 184/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 185/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 186/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 187/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 188/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9566\n","Epoch 189/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 190/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 191/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 192/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 193/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 194/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 195/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 196/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9566\n","Epoch 197/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 198/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9626\n","Epoch 199/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9567\n","Epoch 200/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 201/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9566\n","Epoch 202/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 203/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 204/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 205/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9567\n","Epoch 206/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 207/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 208/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9567\n","Epoch 209/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 210/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9592\n","Epoch 211/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9570\n","Epoch 212/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9571\n","Epoch 213/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 214/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 215/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9524 - val_loss: 4.9566\n","Epoch 216/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 217/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9522 - val_loss: 4.9570\n","Epoch 218/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9527 - val_loss: 4.9616\n","Epoch 219/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 220/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 221/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9524 - val_loss: 4.9569\n","Epoch 222/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 223/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9523 - val_loss: 4.9567\n","Epoch 224/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 225/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 226/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 227/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 228/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 229/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 230/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 231/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 232/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 233/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 234/250\n","5679/5679 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 235/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 236/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 237/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 238/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 239/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 240/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 241/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 242/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9573\n","Epoch 243/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 244/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 245/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 246/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 247/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 248/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9521 - val_loss: 4.9566\n","Epoch 249/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 250/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9566\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6311, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 33503 0.5\n","The shape of N (6311, 784)\n","The minimum value of N  -0.7486565113067627\n","The max value of N 0.7499974370002747\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9969879051616002\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  7\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [ True False False ... False False False]\n","[INFO] : The idx_outlier is:  [False  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5275, 28, 28, 1)\n","Train Label Shape:  (5275,)\n","Validation Data Shape:  (1054, 28, 28, 1)\n","Validation Label Shape:  (1054,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (62, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6311, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6249, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (62, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5679 samples, validate on 632 samples\n","Epoch 1/250\n","5679/5679 [==============================] - 10s 2ms/step - loss: 5.0665 - val_loss: 5.2306\n","Epoch 2/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9925 - val_loss: 5.0148\n","Epoch 3/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9762 - val_loss: 4.9828\n","Epoch 4/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9694 - val_loss: 4.9791\n","Epoch 5/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9655 - val_loss: 4.9730\n","Epoch 6/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9637 - val_loss: 4.9764\n","Epoch 7/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9629 - val_loss: 4.9695\n","Epoch 8/250\n","5679/5679 [==============================] - 2s 297us/step - loss: 4.9614 - val_loss: 4.9701\n","Epoch 9/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9600 - val_loss: 4.9695\n","Epoch 10/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9600 - val_loss: 4.9712\n","Epoch 11/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9587 - val_loss: 4.9712\n","Epoch 12/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9595 - val_loss: 4.9785\n","Epoch 13/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9582 - val_loss: 4.9694\n","Epoch 14/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9577 - val_loss: 4.9668\n","Epoch 15/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9576 - val_loss: 4.9636\n","Epoch 16/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9571 - val_loss: 4.9638\n","Epoch 17/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9570 - val_loss: 4.9635\n","Epoch 18/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9568 - val_loss: 4.9615\n","Epoch 19/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9573 - val_loss: 4.9688\n","Epoch 20/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9562 - val_loss: 4.9635\n","Epoch 21/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9557 - val_loss: 4.9614\n","Epoch 22/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9559 - val_loss: 4.9602\n","Epoch 23/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9555 - val_loss: 4.9591\n","Epoch 24/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9595\n","Epoch 25/250\n","5679/5679 [==============================] - 2s 308us/step - loss: 4.9550 - val_loss: 4.9594\n","Epoch 26/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9554 - val_loss: 4.9613\n","Epoch 27/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9554 - val_loss: 4.9599\n","Epoch 28/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9548 - val_loss: 4.9592\n","Epoch 29/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9588\n","Epoch 30/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9584\n","Epoch 31/250\n","5679/5679 [==============================] - 2s 308us/step - loss: 4.9546 - val_loss: 4.9585\n","Epoch 32/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9593\n","Epoch 33/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9607\n","Epoch 34/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 35/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 36/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 37/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 38/250\n","5679/5679 [==============================] - 2s 307us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 39/250\n","5679/5679 [==============================] - 2s 313us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 40/250\n","5679/5679 [==============================] - 2s 312us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 41/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 42/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 43/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 44/250\n","5679/5679 [==============================] - 2s 310us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 45/250\n","5679/5679 [==============================] - 2s 311us/step - loss: 4.9538 - val_loss: 4.9606\n","Epoch 46/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9590\n","Epoch 47/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 48/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 49/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 50/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 51/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 52/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 53/250\n","5679/5679 [==============================] - 2s 318us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 54/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9744\n","Epoch 55/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9610\n","Epoch 56/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 57/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 58/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 59/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 60/250\n","5679/5679 [==============================] - 2s 310us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 61/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 62/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 63/250\n","5679/5679 [==============================] - 2s 319us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 64/250\n","5679/5679 [==============================] - 2s 317us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 65/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 66/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 67/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9554 - val_loss: 4.9803\n","Epoch 68/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9542 - val_loss: 4.9638\n","Epoch 69/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9594\n","Epoch 70/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9585\n","Epoch 71/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 72/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 73/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 74/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 75/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 76/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 77/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 78/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 79/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 80/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 81/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 82/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 83/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 84/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 85/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 86/250\n","5679/5679 [==============================] - 2s 334us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 87/250\n","5679/5679 [==============================] - 2s 323us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 88/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 89/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 90/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 91/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 92/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 93/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 94/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 95/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 96/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 97/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 98/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 99/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 100/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 101/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 102/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9821\n","Epoch 103/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9550 - val_loss: 4.9771\n","Epoch 104/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9639\n","Epoch 105/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9611\n","Epoch 106/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9595\n","Epoch 107/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 108/250\n","5679/5679 [==============================] - 2s 307us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 109/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 110/250\n","5679/5679 [==============================] - 2s 307us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 111/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 112/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 113/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9649\n","Epoch 114/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 115/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 116/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 117/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 118/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 119/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 120/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 121/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 122/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 123/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 124/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 125/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 126/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 127/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 128/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 129/250\n","5679/5679 [==============================] - 2s 307us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 130/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 131/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 132/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 133/250\n","5679/5679 [==============================] - 2s 339us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 134/250\n","5679/5679 [==============================] - 2s 332us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 135/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 136/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 137/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 138/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9521 - val_loss: 4.9557\n","Epoch 139/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 140/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 141/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 142/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 143/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 144/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 145/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 146/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 147/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 148/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 149/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 150/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 151/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 152/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 153/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 154/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 155/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 156/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 157/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 158/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 159/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 160/250\n","5679/5679 [==============================] - 2s 308us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 161/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9519 - val_loss: 4.9556\n","Epoch 162/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 163/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 164/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 165/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 166/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 167/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 168/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 169/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9518 - val_loss: 4.9557\n","Epoch 170/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9518 - val_loss: 4.9553\n","Epoch 171/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 172/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 173/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 174/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 175/250\n","5679/5679 [==============================] - 2s 299us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 176/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9518 - val_loss: 4.9558\n","Epoch 177/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 178/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9518 - val_loss: 4.9557\n","Epoch 179/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 180/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9517 - val_loss: 4.9556\n","Epoch 181/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 182/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9519 - val_loss: 4.9556\n","Epoch 183/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 184/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 185/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 186/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 187/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 188/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9516 - val_loss: 4.9555\n","Epoch 189/250\n","5679/5679 [==============================] - 2s 298us/step - loss: 4.9517 - val_loss: 4.9556\n","Epoch 190/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 191/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 192/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 193/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 194/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9516 - val_loss: 4.9554\n","Epoch 195/250\n","5679/5679 [==============================] - 2s 307us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 196/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 197/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 198/250\n","5679/5679 [==============================] - 2s 307us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 199/250\n","5679/5679 [==============================] - 2s 309us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 200/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9517 - val_loss: 4.9556\n","Epoch 201/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 202/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9516 - val_loss: 4.9554\n","Epoch 203/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 204/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 205/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9555\n","Epoch 206/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 207/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9554\n","Epoch 208/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9515 - val_loss: 4.9554\n","Epoch 209/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9516 - val_loss: 4.9556\n","Epoch 210/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 211/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9516 - val_loss: 4.9555\n","Epoch 212/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9516 - val_loss: 4.9554\n","Epoch 213/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 214/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9554\n","Epoch 215/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9516 - val_loss: 4.9554\n","Epoch 216/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9516 - val_loss: 4.9555\n","Epoch 217/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 218/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9554\n","Epoch 219/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9555\n","Epoch 220/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 221/250\n","5679/5679 [==============================] - 2s 307us/step - loss: 4.9516 - val_loss: 4.9555\n","Epoch 222/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 223/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9518 - val_loss: 4.9559\n","Epoch 224/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9555\n","Epoch 225/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 226/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9516 - val_loss: 4.9555\n","Epoch 227/250\n","5679/5679 [==============================] - 2s 308us/step - loss: 4.9516 - val_loss: 4.9558\n","Epoch 228/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9516 - val_loss: 4.9557\n","Epoch 229/250\n","5679/5679 [==============================] - 2s 300us/step - loss: 4.9516 - val_loss: 4.9554\n","Epoch 230/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9515 - val_loss: 4.9553\n","Epoch 231/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9515 - val_loss: 4.9553\n","Epoch 232/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9515 - val_loss: 4.9553\n","Epoch 233/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9516 - val_loss: 4.9554\n","Epoch 234/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9516 - val_loss: 4.9554\n","Epoch 235/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9514 - val_loss: 4.9553\n","Epoch 236/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9516 - val_loss: 4.9555\n","Epoch 237/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9516 - val_loss: 4.9555\n","Epoch 238/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9515 - val_loss: 4.9554\n","Epoch 239/250\n","5679/5679 [==============================] - 2s 303us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 240/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9515 - val_loss: 4.9555\n","Epoch 241/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9517 - val_loss: 4.9556\n","Epoch 242/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9515 - val_loss: 4.9557\n","Epoch 243/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9555\n","Epoch 244/250\n","5679/5679 [==============================] - 2s 305us/step - loss: 4.9515 - val_loss: 4.9554\n","Epoch 245/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9515 - val_loss: 4.9553\n","Epoch 246/250\n","5679/5679 [==============================] - 2s 301us/step - loss: 4.9514 - val_loss: 4.9553\n","Epoch 247/250\n","5679/5679 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 248/250\n","5679/5679 [==============================] - 2s 306us/step - loss: 4.9515 - val_loss: 4.9553\n","Epoch 249/250\n","5679/5679 [==============================] - 2s 304us/step - loss: 4.9515 - val_loss: 4.9553\n","Epoch 250/250\n","5679/5679 [==============================] - 2s 307us/step - loss: 4.9515 - val_loss: 4.9554\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6311, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 29634 0.5\n","The shape of N (6311, 784)\n","The minimum value of N  -0.7432418465614319\n","The max value of N 0.7499580979347229\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9953153794929769\n","=======================\n","Saved model to disk....\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.9938545005910624, 0.9872986129393606, 0.9901351958248803, 0.9787837021665402, 0.9950856653193543, 0.9896938348845492, 0.9936660833475291, 0.9977983574146058, 0.9969879051616002, 0.9953153794929769]\n","AUROC ===== 0.991861923714246 +/- 0.005390798036268514\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niDIJCkIIIcokKAghhCj7/wE/6H7XBWQkqgAAAABJRU5ErkJggg==\n","text/plain":["<matplotlib.figure.Figure at 0x7f751be8b1d0>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"muKsh23999sd","colab_type":"text"},"cell_type":"markdown","source":["# **RCAE-MNIST 8_Vs_all**"]},{"metadata":{"id":"5TM8nMK6-MXU","colab_type":"code","outputId":"7ffdb2bc-7386-4bb8-e86d-95be61c81868","executionInfo":{"status":"ok","timestamp":1540720626090,"user_tz":-660,"elapsed":7886937,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":106386}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/500\n","5317/5317 [==============================] - 5s 1ms/step - loss: 5.0639 - val_loss: 5.1559\n","Epoch 2/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9996 - val_loss: 5.0105\n","Epoch 3/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9880 - val_loss: 4.9940\n","Epoch 4/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9808 - val_loss: 4.9849\n","Epoch 5/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9752 - val_loss: 4.9860\n","Epoch 6/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9719 - val_loss: 4.9783\n","Epoch 7/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9701 - val_loss: 4.9850\n","Epoch 8/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9703 - val_loss: 4.9783\n","Epoch 9/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9668 - val_loss: 4.9741\n","Epoch 10/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9644 - val_loss: 4.9709\n","Epoch 11/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9632 - val_loss: 4.9700\n","Epoch 12/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9640 - val_loss: 4.9846\n","Epoch 13/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9628 - val_loss: 4.9672\n","Epoch 14/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9615 - val_loss: 4.9673\n","Epoch 15/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9614 - val_loss: 4.9674\n","Epoch 16/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9606 - val_loss: 4.9673\n","Epoch 17/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9606 - val_loss: 4.9821\n","Epoch 18/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9604 - val_loss: 4.9687\n","Epoch 19/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9596 - val_loss: 4.9653\n","Epoch 20/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9594 - val_loss: 4.9666\n","Epoch 21/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9588 - val_loss: 4.9703\n","Epoch 22/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9596 - val_loss: 4.9739\n","Epoch 23/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9598 - val_loss: 4.9668\n","Epoch 24/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9589 - val_loss: 4.9641\n","Epoch 25/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9583 - val_loss: 4.9639\n","Epoch 26/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9578 - val_loss: 4.9633\n","Epoch 27/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9579 - val_loss: 4.9635\n","Epoch 28/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9584 - val_loss: 4.9644\n","Epoch 29/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9579 - val_loss: 4.9630\n","Epoch 30/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9578 - val_loss: 4.9630\n","Epoch 31/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9573 - val_loss: 4.9616\n","Epoch 32/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9573 - val_loss: 4.9613\n","Epoch 33/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9574 - val_loss: 4.9610\n","Epoch 34/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9570 - val_loss: 4.9640\n","Epoch 35/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9570 - val_loss: 4.9605\n","Epoch 36/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9566 - val_loss: 4.9602\n","Epoch 37/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9565 - val_loss: 4.9642\n","Epoch 38/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9571 - val_loss: 4.9624\n","Epoch 39/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9565 - val_loss: 4.9605\n","Epoch 40/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9564 - val_loss: 4.9596\n","Epoch 41/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9574 - val_loss: 4.9803\n","Epoch 42/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9591 - val_loss: 4.9637\n","Epoch 43/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9570 - val_loss: 4.9616\n","Epoch 44/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9564 - val_loss: 4.9660\n","Epoch 45/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9562 - val_loss: 4.9601\n","Epoch 46/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9560 - val_loss: 4.9594\n","Epoch 47/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9564 - val_loss: 4.9597\n","Epoch 48/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9562 - val_loss: 4.9590\n","Epoch 49/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9559 - val_loss: 4.9586\n","Epoch 50/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9556 - val_loss: 4.9584\n","Epoch 51/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9556 - val_loss: 4.9586\n","Epoch 52/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9556 - val_loss: 4.9587\n","Epoch 53/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9557 - val_loss: 4.9588\n","Epoch 54/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9554 - val_loss: 4.9611\n","Epoch 55/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9555 - val_loss: 4.9588\n","Epoch 56/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9560 - val_loss: 4.9628\n","Epoch 57/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9561 - val_loss: 4.9589\n","Epoch 58/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9556 - val_loss: 4.9586\n","Epoch 59/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9554 - val_loss: 4.9583\n","Epoch 60/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9554 - val_loss: 4.9613\n","Epoch 61/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9554 - val_loss: 4.9587\n","Epoch 62/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9553 - val_loss: 4.9583\n","Epoch 63/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 64/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 65/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 66/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9552 - val_loss: 4.9588\n","Epoch 67/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9552 - val_loss: 4.9583\n","Epoch 68/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 69/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 70/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 71/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 72/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 73/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9551 - val_loss: 4.9623\n","Epoch 74/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9565 - val_loss: 4.9602\n","Epoch 75/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9554 - val_loss: 4.9582\n","Epoch 76/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 77/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 78/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9548 - val_loss: 4.9584\n","Epoch 79/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 80/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 81/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 82/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 83/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 84/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 85/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 86/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 87/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 88/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9702\n","Epoch 89/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9552 - val_loss: 4.9640\n","Epoch 90/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9552 - val_loss: 4.9653\n","Epoch 91/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9557 - val_loss: 4.9601\n","Epoch 92/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9549 - val_loss: 4.9584\n","Epoch 93/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 94/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 95/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 96/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 97/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 98/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 99/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 100/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 101/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 102/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 103/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 104/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 105/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 106/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 107/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 108/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 109/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 110/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 111/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 112/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 113/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 114/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 115/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 116/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 117/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 118/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 119/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 120/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 121/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 122/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9584\n","Epoch 123/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 124/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 125/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 126/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 127/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 128/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 129/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 130/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 131/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 132/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 133/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9647\n","Epoch 134/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 135/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 136/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 137/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 138/500\n","5317/5317 [==============================] - 1s 278us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 139/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 140/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 141/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 142/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 143/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 144/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 145/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 146/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 147/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 148/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 149/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 150/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 151/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 152/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 153/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 154/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 155/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9613\n","Epoch 156/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9599\n","Epoch 157/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 158/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 5.0776\n","Epoch 159/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9549 - val_loss: 4.9970\n","Epoch 160/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9541 - val_loss: 4.9657\n","Epoch 161/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9606\n","Epoch 162/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9538 - val_loss: 4.9593\n","Epoch 163/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 164/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9536 - val_loss: 4.9584\n","Epoch 165/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 166/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 167/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 168/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9588\n","Epoch 169/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 170/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 171/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 172/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 173/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 174/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 175/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 176/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 177/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 178/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 179/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 5.1064\n","Epoch 180/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9557 - val_loss: 4.9967\n","Epoch 181/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9547 - val_loss: 4.9589\n","Epoch 182/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 183/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 184/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 185/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 186/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 187/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 188/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 189/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 190/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 191/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 192/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 193/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9537 - val_loss: 4.9584\n","Epoch 194/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 195/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 196/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 197/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 198/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 199/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 200/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 201/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 202/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 203/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 204/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 205/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 206/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 207/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 208/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 209/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 210/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 211/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 212/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 213/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 214/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 215/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 216/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 217/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 218/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 219/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 220/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 221/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 222/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 223/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 224/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 225/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 226/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 227/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 228/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 229/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 230/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 231/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 232/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 233/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 234/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 235/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 236/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 237/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 238/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 239/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 240/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 241/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 242/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 243/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 244/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 245/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 246/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 247/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 248/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 249/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 250/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 251/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 252/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 253/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 254/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 255/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 256/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 257/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 258/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 259/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 260/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 261/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 262/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9535 - val_loss: 4.9610\n","Epoch 263/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 264/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 265/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 266/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 267/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 268/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 269/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 270/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 271/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 272/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 273/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 274/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 275/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 276/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 277/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 278/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 279/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 280/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 281/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 282/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 283/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 284/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 285/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 286/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 287/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 288/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 289/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 290/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 291/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 292/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 293/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 294/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 295/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 296/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 297/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 298/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 299/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 300/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 301/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 302/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 303/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 304/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 305/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 306/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 307/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 308/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 309/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 310/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 311/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 312/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 313/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 314/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 315/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 316/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 317/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 318/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 319/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 320/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 321/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 322/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 323/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 324/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 325/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 326/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 327/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 328/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 329/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 330/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 331/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 332/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 333/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 334/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 335/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9577\n","Epoch 336/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 337/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 338/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 339/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 340/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 341/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 342/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 343/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 344/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 345/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 346/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 347/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 348/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 349/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 350/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 351/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 352/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 353/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 354/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 355/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 356/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 357/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 358/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9577\n","Epoch 359/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 360/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 361/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 362/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 363/500\n","5317/5317 [==============================] - 1s 278us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 364/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9524 - val_loss: 4.9573\n","Epoch 365/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 366/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 367/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9525 - val_loss: 4.9577\n","Epoch 368/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 369/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 370/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 371/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 372/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 373/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9577\n","Epoch 374/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 375/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 376/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 377/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 378/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 379/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 380/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9524 - val_loss: 4.9573\n","Epoch 381/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 382/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9524 - val_loss: 4.9573\n","Epoch 383/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9524 - val_loss: 4.9573\n","Epoch 384/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9573\n","Epoch 385/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 386/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 387/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 388/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 389/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 390/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 391/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 392/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 393/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 394/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 395/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 396/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 397/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9524 - val_loss: 4.9573\n","Epoch 398/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 399/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 400/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 401/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 402/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 403/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9523 - val_loss: 4.9577\n","Epoch 404/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 405/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 406/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9573\n","Epoch 407/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 408/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 409/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 410/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 411/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 412/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 413/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 414/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 415/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 416/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9523 - val_loss: 4.9573\n","Epoch 417/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 418/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 419/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9523 - val_loss: 4.9573\n","Epoch 420/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 421/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9577\n","Epoch 422/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9524 - val_loss: 4.9577\n","Epoch 423/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 424/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 425/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 426/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9523 - val_loss: 4.9573\n","Epoch 427/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 428/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9577\n","Epoch 429/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9523 - val_loss: 4.9577\n","Epoch 430/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 431/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 432/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 433/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 434/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 435/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 436/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 437/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 438/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 439/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9576\n","Epoch 440/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9523 - val_loss: 4.9576\n","Epoch 441/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 442/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 443/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9574\n","Epoch 444/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 445/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 446/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9576\n","Epoch 447/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 448/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 449/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 450/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 451/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 452/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9574\n","Epoch 453/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 454/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9574\n","Epoch 455/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 456/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 457/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 458/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9577\n","Epoch 459/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9577\n","Epoch 460/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9522 - val_loss: 4.9574\n","Epoch 461/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 462/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 463/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9521 - val_loss: 4.9574\n","Epoch 464/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 465/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 466/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 467/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 468/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9578\n","Epoch 469/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9522 - val_loss: 4.9577\n","Epoch 470/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9523 - val_loss: 4.9577\n","Epoch 471/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9576\n","Epoch 472/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 473/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9521 - val_loss: 4.9576\n","Epoch 474/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9522 - val_loss: 4.9574\n","Epoch 475/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 476/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 477/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 478/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9578\n","Epoch 479/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 480/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9521 - val_loss: 4.9577\n","Epoch 481/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 482/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9522 - val_loss: 4.9577\n","Epoch 483/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 484/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 485/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 486/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9522 - val_loss: 4.9576\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 488/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 489/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9521 - val_loss: 4.9575\n","Epoch 490/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9576\n","Epoch 491/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9521 - val_loss: 4.9575\n","Epoch 492/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9576\n","Epoch 493/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9521 - val_loss: 4.9575\n","Epoch 494/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9521 - val_loss: 4.9576\n","Epoch 495/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9577\n","Epoch 496/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9577\n","Epoch 497/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9522 - val_loss: 4.9578\n","Epoch 498/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9522 - val_loss: 4.9578\n","Epoch 499/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9522 - val_loss: 4.9580\n","Epoch 500/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9581\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 49005 0.5\n","The shape of N (5908, 784)\n","The minimum value of N  -0.7499748468399048\n","The max value of N 0.7468274235725403\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9788712054229296\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/500\n","5317/5317 [==============================] - 4s 664us/step - loss: 5.0642 - val_loss: 5.2151\n","Epoch 2/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 5.0014 - val_loss: 5.0237\n","Epoch 3/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9893 - val_loss: 4.9971\n","Epoch 4/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9818 - val_loss: 4.9862\n","Epoch 5/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9767 - val_loss: 4.9808\n","Epoch 6/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9725 - val_loss: 4.9765\n","Epoch 7/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9697 - val_loss: 4.9832\n","Epoch 8/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9689 - val_loss: 5.0185\n","Epoch 9/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9685 - val_loss: 4.9813\n","Epoch 10/500\n","5317/5317 [==============================] - 1s 278us/step - loss: 4.9659 - val_loss: 4.9733\n","Epoch 11/500\n","5317/5317 [==============================] - 1s 276us/step - loss: 4.9645 - val_loss: 4.9697\n","Epoch 12/500\n","5317/5317 [==============================] - 1s 276us/step - loss: 4.9644 - val_loss: 4.9726\n","Epoch 13/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9628 - val_loss: 4.9703\n","Epoch 14/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9622 - val_loss: 4.9692\n","Epoch 15/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9630 - val_loss: 4.9745\n","Epoch 16/500\n","5317/5317 [==============================] - 1s 277us/step - loss: 4.9622 - val_loss: 4.9704\n","Epoch 17/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9611 - val_loss: 4.9688\n","Epoch 18/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9605 - val_loss: 4.9674\n","Epoch 19/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9599 - val_loss: 4.9667\n","Epoch 20/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9594 - val_loss: 4.9658\n","Epoch 21/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9597 - val_loss: 4.9664\n","Epoch 22/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9590 - val_loss: 4.9663\n","Epoch 23/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9588 - val_loss: 4.9631\n","Epoch 24/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9587 - val_loss: 4.9642\n","Epoch 25/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9586 - val_loss: 4.9640\n","Epoch 26/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9581 - val_loss: 4.9643\n","Epoch 27/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9591 - val_loss: 4.9702\n","Epoch 28/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9587 - val_loss: 4.9638\n","Epoch 29/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9580 - val_loss: 4.9636\n","Epoch 30/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9576 - val_loss: 4.9623\n","Epoch 31/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9573 - val_loss: 4.9619\n","Epoch 32/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9573 - val_loss: 4.9617\n","Epoch 33/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9571 - val_loss: 4.9608\n","Epoch 34/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9570 - val_loss: 4.9609\n","Epoch 35/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9570 - val_loss: 4.9626\n","Epoch 36/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9568 - val_loss: 4.9603\n","Epoch 37/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9572 - val_loss: 4.9601\n","Epoch 38/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9569 - val_loss: 4.9610\n","Epoch 39/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9566 - val_loss: 4.9614\n","Epoch 40/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9564 - val_loss: 4.9596\n","Epoch 41/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9566 - val_loss: 4.9606\n","Epoch 42/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9565 - val_loss: 4.9597\n","Epoch 43/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9614 - val_loss: 4.9871\n","Epoch 44/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9586 - val_loss: 4.9637\n","Epoch 45/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9585 - val_loss: 4.9679\n","Epoch 46/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9574 - val_loss: 4.9628\n","Epoch 47/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9573 - val_loss: 4.9653\n","Epoch 48/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9612 - val_loss: 5.0339\n","Epoch 49/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9581 - val_loss: 4.9687\n","Epoch 50/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9572 - val_loss: 4.9622\n","Epoch 51/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9569 - val_loss: 4.9603\n","Epoch 52/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9566 - val_loss: 4.9593\n","Epoch 53/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9565 - val_loss: 4.9747\n","Epoch 54/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9566 - val_loss: 4.9631\n","Epoch 55/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9561 - val_loss: 4.9589\n","Epoch 56/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9562 - val_loss: 4.9584\n","Epoch 57/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9560 - val_loss: 4.9583\n","Epoch 58/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9560 - val_loss: 4.9583\n","Epoch 59/500\n","5317/5317 [==============================] - 1s 277us/step - loss: 4.9560 - val_loss: 4.9585\n","Epoch 60/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9563 - val_loss: 4.9614\n","Epoch 61/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9560 - val_loss: 4.9748\n","Epoch 62/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9561 - val_loss: 4.9598\n","Epoch 63/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9560 - val_loss: 4.9587\n","Epoch 64/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9557 - val_loss: 4.9582\n","Epoch 65/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9556 - val_loss: 4.9579\n","Epoch 66/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 67/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9557 - val_loss: 4.9620\n","Epoch 68/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9555 - val_loss: 4.9586\n","Epoch 69/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9555 - val_loss: 4.9580\n","Epoch 70/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9553 - val_loss: 4.9580\n","Epoch 71/500\n","5317/5317 [==============================] - 1s 278us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 72/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9551 - val_loss: 4.9576\n","Epoch 73/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 74/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9552 - val_loss: 4.9576\n","Epoch 75/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 76/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9552 - val_loss: 4.9583\n","Epoch 77/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 78/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9550 - val_loss: 4.9582\n","Epoch 79/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9551 - val_loss: 4.9592\n","Epoch 80/500\n","5317/5317 [==============================] - 1s 277us/step - loss: 4.9582 - val_loss: 4.9871\n","Epoch 81/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9573 - val_loss: 4.9670\n","Epoch 82/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9565 - val_loss: 4.9659\n","Epoch 83/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9570 - val_loss: 4.9778\n","Epoch 84/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9563 - val_loss: 4.9635\n","Epoch 85/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9562 - val_loss: 4.9628\n","Epoch 86/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9560 - val_loss: 4.9598\n","Epoch 87/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9557 - val_loss: 4.9587\n","Epoch 88/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9555 - val_loss: 4.9585\n","Epoch 89/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 90/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 91/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 92/500\n","5317/5317 [==============================] - 1s 278us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 93/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9552 - val_loss: 4.9579\n","Epoch 94/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9553 - val_loss: 4.9579\n","Epoch 95/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 96/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 97/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 98/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9549 - val_loss: 4.9577\n","Epoch 99/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 100/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9549 - val_loss: 4.9577\n","Epoch 101/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 102/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 103/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 104/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 105/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 106/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9548 - val_loss: 4.9586\n","Epoch 107/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9547 - val_loss: 4.9589\n","Epoch 108/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 109/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 110/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 111/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9546 - val_loss: 4.9596\n","Epoch 112/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 113/500\n","5317/5317 [==============================] - 1s 276us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 114/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 115/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 116/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 117/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 118/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 119/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 120/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 121/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9554 - val_loss: 5.0149\n","Epoch 122/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9556 - val_loss: 4.9772\n","Epoch 123/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9552 - val_loss: 4.9672\n","Epoch 124/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9550 - val_loss: 4.9635\n","Epoch 125/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9550 - val_loss: 4.9600\n","Epoch 126/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9551 - val_loss: 4.9587\n","Epoch 127/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9547 - val_loss: 4.9584\n","Epoch 128/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 129/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 130/500\n","5317/5317 [==============================] - 1s 278us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 131/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9552 - val_loss: 4.9594\n","Epoch 132/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 133/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 134/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 135/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 136/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 137/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 138/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 139/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 140/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 141/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 142/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 143/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 144/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 145/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9589\n","Epoch 146/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9541 - val_loss: 4.9593\n","Epoch 147/500\n","5317/5317 [==============================] - 1s 278us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 148/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 149/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 150/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 151/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 152/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 153/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 154/500\n","5317/5317 [==============================] - 1s 278us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 155/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 156/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 157/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 158/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9540 - val_loss: 4.9585\n","Epoch 159/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9539 - val_loss: 4.9585\n","Epoch 160/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9539 - val_loss: 4.9587\n","Epoch 161/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9539 - val_loss: 4.9584\n","Epoch 162/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 163/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9575 - val_loss: 4.9936\n","Epoch 164/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9565 - val_loss: 4.9698\n","Epoch 165/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9552 - val_loss: 4.9659\n","Epoch 166/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9552 - val_loss: 4.9628\n","Epoch 167/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9548 - val_loss: 4.9599\n","Epoch 168/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9545 - val_loss: 4.9584\n","Epoch 169/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 170/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 171/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 172/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 173/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 174/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 175/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 176/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 177/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 178/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 179/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 180/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 181/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 182/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 183/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 184/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 185/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 186/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 187/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 188/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 189/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 190/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 191/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 192/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 193/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 194/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 195/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 196/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 197/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 198/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 199/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 200/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 201/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 202/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9584\n","Epoch 203/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9540 - val_loss: 4.9651\n","Epoch 204/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9541 - val_loss: 4.9601\n","Epoch 205/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9539 - val_loss: 4.9592\n","Epoch 206/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9588\n","Epoch 207/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 208/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 209/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 210/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 211/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 212/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 213/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 214/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 215/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 216/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 217/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 218/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 219/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 220/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 221/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 222/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 223/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 224/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 225/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 226/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 227/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 228/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 229/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 230/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 231/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 232/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 233/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 234/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 235/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 236/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 237/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 238/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9535 - val_loss: 4.9583\n","Epoch 239/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 240/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 241/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 242/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 243/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 244/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 245/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 246/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 247/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 248/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 249/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 250/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 251/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 252/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 253/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 254/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 255/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 256/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 257/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 258/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 259/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 260/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 261/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 262/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 263/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 264/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 265/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 266/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 267/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 268/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 269/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 270/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 271/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 272/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 273/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 274/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 275/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 276/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 277/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 278/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 279/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 280/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 281/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 282/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 283/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 284/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 285/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 286/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 287/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 288/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 289/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 290/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 291/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 292/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 293/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 294/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 295/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 296/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 297/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 298/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 299/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 300/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 301/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 302/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 303/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 304/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 305/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 306/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 307/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 308/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 309/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9583\n","Epoch 310/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 311/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 312/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 313/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 314/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 315/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 316/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 317/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 318/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 319/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 320/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 321/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 322/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 323/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 324/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 325/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 326/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 327/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 328/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 329/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 330/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 331/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 332/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9583\n","Epoch 333/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 334/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 335/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 336/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 337/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 338/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 339/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 340/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 341/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 342/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 343/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 344/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 345/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 346/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9584\n","Epoch 347/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 348/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9582\n","Epoch 349/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9582\n","Epoch 350/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 351/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 352/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 353/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 354/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 355/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9590\n","Epoch 356/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 357/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 358/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 359/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9582\n","Epoch 360/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 361/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9583\n","Epoch 362/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 363/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 364/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 365/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 366/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 367/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 368/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 369/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 370/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 371/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 372/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 373/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 374/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 375/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 376/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 377/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 378/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 379/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 380/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 381/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 382/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 383/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 384/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 385/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 386/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 387/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 388/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 389/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 390/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 391/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 392/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 393/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 394/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 395/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9528 - val_loss: 4.9583\n","Epoch 396/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 397/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 398/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 399/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9583\n","Epoch 400/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9584\n","Epoch 401/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 402/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 403/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 404/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 405/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 406/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 407/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 408/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 409/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 410/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 411/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 412/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 413/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 414/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 415/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 416/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 417/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 418/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 419/500\n","5317/5317 [==============================] - 1s 278us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 420/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 421/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 422/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 423/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 424/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 425/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 426/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 427/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 428/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 429/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 430/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 431/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 432/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9579\n","Epoch 433/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 434/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 435/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 436/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 437/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 438/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9527 - val_loss: 4.9583\n","Epoch 439/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 440/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 441/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 442/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 443/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 444/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 445/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 446/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9579\n","Epoch 447/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9589\n","Epoch 448/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 449/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 450/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 451/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 452/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 453/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 454/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 455/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 456/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 457/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 458/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 459/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 460/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 461/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 462/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 463/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 464/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 465/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 466/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 467/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 468/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 469/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 470/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 471/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 472/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 473/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 474/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 475/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 476/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9584\n","Epoch 477/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 478/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 479/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9579\n","Epoch 480/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 481/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 482/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 483/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9597\n","Epoch 484/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9583\n","Epoch 485/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 486/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 487/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 488/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 489/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 490/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 491/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 492/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 493/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 494/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9580\n","Epoch 495/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9582\n","Epoch 496/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9525 - val_loss: 4.9583\n","Epoch 497/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 498/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9586\n","Epoch 499/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9584\n","Epoch 500/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9524 - val_loss: 4.9581\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 42186 0.5\n","The shape of N (5908, 784)\n","The minimum value of N  -0.7499995231628418\n","The max value of N 0.7487714290618896\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9887592101385204\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/500\n","5317/5317 [==============================] - 4s 819us/step - loss: 5.0604 - val_loss: 5.2565\n","Epoch 2/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 5.0008 - val_loss: 5.0406\n","Epoch 3/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9888 - val_loss: 4.9950\n","Epoch 4/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9817 - val_loss: 4.9890\n","Epoch 5/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9754 - val_loss: 4.9798\n","Epoch 6/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9715 - val_loss: 4.9774\n","Epoch 7/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9692 - val_loss: 4.9747\n","Epoch 8/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9674 - val_loss: 4.9719\n","Epoch 9/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9665 - val_loss: 4.9720\n","Epoch 10/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9654 - val_loss: 4.9735\n","Epoch 11/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9641 - val_loss: 4.9693\n","Epoch 12/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9644 - val_loss: 4.9863\n","Epoch 13/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9648 - val_loss: 4.9761\n","Epoch 14/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9629 - val_loss: 4.9723\n","Epoch 15/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9624 - val_loss: 4.9717\n","Epoch 16/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9621 - val_loss: 4.9701\n","Epoch 17/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9609 - val_loss: 4.9682\n","Epoch 18/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9605 - val_loss: 4.9648\n","Epoch 19/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9623 - val_loss: 5.0086\n","Epoch 20/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9613 - val_loss: 4.9845\n","Epoch 21/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9620 - val_loss: 4.9835\n","Epoch 22/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9608 - val_loss: 4.9738\n","Epoch 23/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9594 - val_loss: 4.9759\n","Epoch 24/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9591 - val_loss: 4.9709\n","Epoch 25/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9610 - val_loss: 4.9782\n","Epoch 26/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9598 - val_loss: 4.9704\n","Epoch 27/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9590 - val_loss: 4.9645\n","Epoch 28/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9585 - val_loss: 4.9619\n","Epoch 29/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9581 - val_loss: 4.9615\n","Epoch 30/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9579 - val_loss: 4.9613\n","Epoch 31/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9581 - val_loss: 4.9610\n","Epoch 32/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9576 - val_loss: 4.9609\n","Epoch 33/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9576 - val_loss: 4.9612\n","Epoch 34/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9575 - val_loss: 4.9608\n","Epoch 35/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9573 - val_loss: 4.9605\n","Epoch 36/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9571 - val_loss: 4.9597\n","Epoch 37/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9572 - val_loss: 4.9598\n","Epoch 38/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9568 - val_loss: 4.9596\n","Epoch 39/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9572 - val_loss: 4.9847\n","Epoch 40/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9586 - val_loss: 4.9702\n","Epoch 41/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9575 - val_loss: 4.9620\n","Epoch 42/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9571 - val_loss: 4.9624\n","Epoch 43/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9571 - val_loss: 4.9608\n","Epoch 44/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9569 - val_loss: 4.9595\n","Epoch 45/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9566 - val_loss: 4.9592\n","Epoch 46/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9565 - val_loss: 4.9595\n","Epoch 47/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9567 - val_loss: 4.9592\n","Epoch 48/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9567 - val_loss: 4.9594\n","Epoch 49/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9562 - val_loss: 4.9591\n","Epoch 50/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9563 - val_loss: 4.9592\n","Epoch 51/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9564 - val_loss: 4.9588\n","Epoch 52/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9561 - val_loss: 4.9584\n","Epoch 53/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9559 - val_loss: 4.9584\n","Epoch 54/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9560 - val_loss: 4.9583\n","Epoch 55/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9559 - val_loss: 4.9584\n","Epoch 56/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9559 - val_loss: 4.9590\n","Epoch 57/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9558 - val_loss: 4.9583\n","Epoch 58/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9558 - val_loss: 4.9583\n","Epoch 59/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9559 - val_loss: 4.9604\n","Epoch 60/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9563 - val_loss: 4.9599\n","Epoch 61/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9559 - val_loss: 4.9587\n","Epoch 62/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9556 - val_loss: 4.9584\n","Epoch 63/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9556 - val_loss: 4.9582\n","Epoch 64/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9556 - val_loss: 4.9583\n","Epoch 65/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 66/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9558 - val_loss: 4.9583\n","Epoch 67/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9555 - val_loss: 4.9581\n","Epoch 68/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9553 - val_loss: 4.9579\n","Epoch 69/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 70/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9552 - val_loss: 4.9579\n","Epoch 71/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9552 - val_loss: 4.9579\n","Epoch 72/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 73/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 74/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9551 - val_loss: 4.9584\n","Epoch 75/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 76/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 77/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9550 - val_loss: 4.9579\n","Epoch 78/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 79/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9551 - val_loss: 4.9580\n","Epoch 80/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 81/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 82/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 83/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9549 - val_loss: 4.9577\n","Epoch 84/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 85/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 86/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 87/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 88/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 89/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 90/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 91/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 92/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 93/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 94/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 95/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9550 - val_loss: 4.9586\n","Epoch 96/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 97/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 98/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 99/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 100/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 101/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 102/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 103/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 104/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 105/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 106/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9583\n","Epoch 107/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 108/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 109/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 110/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 111/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 112/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 113/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 114/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 115/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 116/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 117/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 118/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 119/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 120/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 121/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 122/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 123/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 124/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 125/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9544 - val_loss: 4.9581\n","Epoch 126/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 127/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 128/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 129/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 130/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9548 - val_loss: 4.9786\n","Epoch 131/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9577 - val_loss: 4.9828\n","Epoch 132/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9564 - val_loss: 4.9651\n","Epoch 133/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9554 - val_loss: 4.9617\n","Epoch 134/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9551 - val_loss: 4.9605\n","Epoch 135/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9587\n","Epoch 136/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9547 - val_loss: 4.9588\n","Epoch 137/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9551 - val_loss: 4.9624\n","Epoch 138/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 139/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9547 - val_loss: 4.9583\n","Epoch 140/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 141/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 142/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 143/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 144/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 145/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 146/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 147/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 148/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 149/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 150/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 151/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 152/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 153/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 154/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 155/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 156/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 157/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 158/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 159/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 160/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 161/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 162/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 163/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 164/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 165/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 166/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 167/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 168/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 169/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 170/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 171/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 172/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 173/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 174/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 175/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 176/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 177/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 178/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 179/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 180/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 181/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 182/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 183/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 184/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 185/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 186/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 187/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 188/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 189/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 190/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 191/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 192/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 193/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 194/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 195/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 196/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 197/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 198/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 199/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 200/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9591\n","Epoch 201/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9543 - val_loss: 4.9592\n","Epoch 202/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 203/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 204/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 205/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9542 - val_loss: 4.9624\n","Epoch 206/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9541 - val_loss: 4.9586\n","Epoch 207/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 208/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 209/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 210/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 211/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 212/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 213/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 214/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 215/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 216/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 217/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 218/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 219/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 220/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 221/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 222/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 223/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 224/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 225/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 226/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 227/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 228/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 229/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 230/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 231/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 232/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 233/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 234/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 235/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 236/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 237/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 238/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 239/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 240/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 241/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 242/500\n","5317/5317 [==============================] - 1s 278us/step - loss: 4.9535 - val_loss: 4.9595\n","Epoch 243/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 244/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 245/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 246/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 247/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9584\n","Epoch 248/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 249/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 250/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 251/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 252/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 253/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 254/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 255/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 256/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 257/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 258/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 259/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 260/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 261/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 262/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 263/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 264/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 265/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 266/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 267/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 268/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 269/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 270/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 271/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 272/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 273/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 274/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 275/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 276/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 277/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 278/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 279/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 280/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 281/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 282/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 283/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 284/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 285/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 286/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 287/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 288/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 289/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 290/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 291/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 292/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 293/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 294/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 295/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 296/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 297/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 298/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 299/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 300/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 301/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 302/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 303/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 304/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 305/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9595\n","Epoch 306/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9585\n","Epoch 307/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 308/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 309/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 310/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 311/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 312/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 313/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 314/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 315/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 316/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9586\n","Epoch 317/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 318/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 319/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 320/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9585\n","Epoch 321/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9585\n","Epoch 322/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 323/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 324/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 325/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 326/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 327/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 328/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 329/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 330/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 331/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 332/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 333/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 334/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 335/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 336/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 337/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 338/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 339/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 340/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 341/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 342/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 343/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 344/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 345/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 346/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 347/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 348/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 349/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 350/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 351/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 352/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 353/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 354/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 355/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 356/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 357/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 358/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 359/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 360/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 361/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 362/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 363/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 364/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 365/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 366/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 367/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 368/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 369/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 370/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 371/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 372/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 373/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 374/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 375/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 376/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 377/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 378/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 379/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 380/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 381/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 382/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 383/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 384/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9585\n","Epoch 385/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 386/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 387/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9584\n","Epoch 388/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 389/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9583\n","Epoch 390/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 391/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9588\n","Epoch 392/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 393/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 394/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9590\n","Epoch 395/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9582\n","Epoch 396/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 397/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 398/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 399/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9583\n","Epoch 400/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9583\n","Epoch 401/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 402/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 403/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 404/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 405/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 406/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 407/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 408/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 409/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 410/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 411/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 412/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 413/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 414/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9589\n","Epoch 415/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 416/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 417/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 418/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 419/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 420/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 421/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 422/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 423/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 424/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 425/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 426/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 427/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 428/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 429/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 430/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 431/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 432/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 433/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 434/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 435/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 436/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 437/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 438/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 439/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 440/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 441/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 442/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 443/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 444/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 445/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 446/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 447/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 448/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 449/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 450/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 451/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 452/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 453/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 454/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 455/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 456/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 457/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 458/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 459/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 460/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 461/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 462/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 463/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 464/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 465/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9587\n","Epoch 466/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9583\n","Epoch 467/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 468/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 469/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 470/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 471/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 472/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 473/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 474/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 475/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 476/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 477/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 478/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 479/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 480/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 481/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 482/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 483/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 484/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9584\n","Epoch 485/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9583\n","Epoch 486/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 487/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 488/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9583\n","Epoch 489/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 490/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 491/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9526 - val_loss: 4.9583\n","Epoch 492/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 493/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9583\n","Epoch 494/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 495/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9526 - val_loss: 4.9583\n","Epoch 496/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9583\n","Epoch 497/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 498/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9584\n","Epoch 499/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9584\n","Epoch 500/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9582\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 33234 0.5\n","The shape of N (5908, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9865016209843795\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/500\n","5317/5317 [==============================] - 5s 947us/step - loss: 5.0734 - val_loss: 5.1330\n","Epoch 2/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 5.0019 - val_loss: 5.0126\n","Epoch 3/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9880 - val_loss: 4.9912\n","Epoch 4/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9799 - val_loss: 4.9829\n","Epoch 5/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9751 - val_loss: 4.9817\n","Epoch 6/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9712 - val_loss: 4.9810\n","Epoch 7/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9686 - val_loss: 4.9726\n","Epoch 8/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9663 - val_loss: 4.9700\n","Epoch 9/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9651 - val_loss: 5.0053\n","Epoch 10/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9661 - val_loss: 4.9965\n","Epoch 11/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9638 - val_loss: 4.9735\n","Epoch 12/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9631 - val_loss: 4.9698\n","Epoch 13/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9627 - val_loss: 5.0224\n","Epoch 14/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9622 - val_loss: 4.9892\n","Epoch 15/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9611 - val_loss: 4.9752\n","Epoch 16/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9602 - val_loss: 4.9689\n","Epoch 17/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9620 - val_loss: 5.0134\n","Epoch 18/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9611 - val_loss: 4.9879\n","Epoch 19/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9600 - val_loss: 4.9738\n","Epoch 20/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9597 - val_loss: 4.9663\n","Epoch 21/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9591 - val_loss: 4.9662\n","Epoch 22/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9587 - val_loss: 4.9647\n","Epoch 23/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9584 - val_loss: 4.9632\n","Epoch 24/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9587 - val_loss: 4.9635\n","Epoch 25/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9580 - val_loss: 4.9612\n","Epoch 26/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9579 - val_loss: 4.9624\n","Epoch 27/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9577 - val_loss: 4.9623\n","Epoch 28/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9574 - val_loss: 4.9610\n","Epoch 29/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9574 - val_loss: 4.9607\n","Epoch 30/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9574 - val_loss: 4.9622\n","Epoch 31/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9573 - val_loss: 4.9609\n","Epoch 32/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9570 - val_loss: 4.9614\n","Epoch 33/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9572 - val_loss: 4.9604\n","Epoch 34/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9571 - val_loss: 4.9629\n","Epoch 35/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9568 - val_loss: 4.9611\n","Epoch 36/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9566 - val_loss: 4.9603\n","Epoch 37/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9567 - val_loss: 4.9601\n","Epoch 38/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9564 - val_loss: 4.9607\n","Epoch 39/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9563 - val_loss: 4.9596\n","Epoch 40/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9563 - val_loss: 4.9592\n","Epoch 41/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9631 - val_loss: 4.9862\n","Epoch 42/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9590 - val_loss: 4.9728\n","Epoch 43/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9575 - val_loss: 4.9644\n","Epoch 44/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9573 - val_loss: 4.9617\n","Epoch 45/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9568 - val_loss: 4.9598\n","Epoch 46/500\n","5317/5317 [==============================] - 1s 279us/step - loss: 4.9569 - val_loss: 4.9603\n","Epoch 47/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9564 - val_loss: 4.9594\n","Epoch 48/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9565 - val_loss: 4.9603\n","Epoch 49/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9564 - val_loss: 4.9633\n","Epoch 50/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9562 - val_loss: 4.9591\n","Epoch 51/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9561 - val_loss: 4.9587\n","Epoch 52/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9559 - val_loss: 4.9590\n","Epoch 53/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9559 - val_loss: 4.9583\n","Epoch 54/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9558 - val_loss: 4.9585\n","Epoch 55/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9559 - val_loss: 4.9585\n","Epoch 56/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9557 - val_loss: 4.9582\n","Epoch 57/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 58/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9555 - val_loss: 4.9586\n","Epoch 59/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9555 - val_loss: 4.9578\n","Epoch 60/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9557 - val_loss: 4.9588\n","Epoch 61/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 62/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 63/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9555 - val_loss: 4.9600\n","Epoch 64/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9553 - val_loss: 4.9583\n","Epoch 65/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9553 - val_loss: 4.9580\n","Epoch 66/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 67/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 68/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 69/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 70/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9559 - val_loss: 4.9733\n","Epoch 71/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9579 - val_loss: 4.9890\n","Epoch 72/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9564 - val_loss: 4.9667\n","Epoch 73/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9558 - val_loss: 4.9622\n","Epoch 74/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9555 - val_loss: 4.9601\n","Epoch 75/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9593\n","Epoch 76/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9553 - val_loss: 4.9584\n","Epoch 77/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9550 - val_loss: 4.9583\n","Epoch 78/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 79/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 80/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 81/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 82/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 83/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9547 - val_loss: 4.9575\n","Epoch 84/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9599\n","Epoch 85/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 86/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 87/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 88/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 89/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9548 - val_loss: 4.9584\n","Epoch 90/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 91/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 92/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 93/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 94/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 95/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 96/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 97/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 98/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 99/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 100/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 101/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 102/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 103/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 104/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 105/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 106/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 107/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 108/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 109/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9584\n","Epoch 110/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 111/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 112/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 113/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 114/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 115/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 116/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 117/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 118/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 119/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 120/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 121/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 122/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 123/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 124/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 125/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 126/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 127/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 128/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 129/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 130/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 131/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 132/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 133/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 134/500\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 135/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 136/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 137/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 138/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 139/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 140/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 141/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 142/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 143/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 144/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 145/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 146/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 147/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 148/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 149/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 150/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 151/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 152/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 153/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 154/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 155/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 156/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 157/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 158/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 159/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 160/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 161/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 162/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 163/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 164/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 165/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 166/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 167/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 168/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 169/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 170/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 171/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 172/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 173/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 174/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 175/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 176/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 177/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 178/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 179/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 180/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 181/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 182/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 183/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 184/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 185/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 186/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 187/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 188/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 189/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 190/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 191/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 192/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 193/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 194/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 195/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 196/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 197/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 198/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 199/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 200/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 201/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 202/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 203/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 204/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 205/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 206/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 207/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 208/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 209/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 210/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 211/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 212/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9534 - val_loss: 4.9630\n","Epoch 213/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 214/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 215/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 216/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 217/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 218/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 219/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 220/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 221/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 222/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 223/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 224/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 225/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 226/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 227/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 228/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 229/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 230/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 231/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 232/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 233/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 234/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 235/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 236/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 237/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 238/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 239/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 240/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 241/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 242/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 243/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 244/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 245/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 246/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 247/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 248/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 249/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 250/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 251/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 252/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 253/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 254/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 255/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 256/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 257/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 258/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 259/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 260/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 261/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 262/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 263/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 264/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 265/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 266/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 267/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 268/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 269/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 270/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 271/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 272/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 273/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 274/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 275/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 276/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 277/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 278/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 279/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 280/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 281/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 282/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 283/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 284/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 285/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 286/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 287/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 288/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 289/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 290/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 291/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 292/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 293/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 294/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 295/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 296/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 297/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 298/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 299/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 300/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 301/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 302/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 303/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 304/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 305/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 306/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 307/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 308/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 309/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 310/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 311/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 312/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 313/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 314/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 315/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 316/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 317/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9528 - val_loss: 4.9583\n","Epoch 318/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 319/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 320/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 321/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9583\n","Epoch 322/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9584\n","Epoch 323/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9583\n","Epoch 324/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9583\n","Epoch 325/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 326/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 327/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 328/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 329/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 330/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 331/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9584\n","Epoch 332/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 333/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 334/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 335/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 336/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 337/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 338/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9584\n","Epoch 339/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 340/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 341/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 342/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 343/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 344/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 345/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 346/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 347/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 348/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 349/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 350/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9584\n","Epoch 351/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 352/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 353/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 354/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 355/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 356/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 357/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 358/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 359/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 360/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 361/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9527 - val_loss: 4.9584\n","Epoch 362/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 363/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 364/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 365/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 366/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 367/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 368/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9583\n","Epoch 369/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9584\n","Epoch 370/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9583\n","Epoch 371/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9583\n","Epoch 372/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9583\n","Epoch 373/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 374/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9583\n","Epoch 375/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9587\n","Epoch 376/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 377/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 378/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 379/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 380/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 381/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 382/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 383/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 384/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9583\n","Epoch 385/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 386/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 387/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9583\n","Epoch 388/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 389/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 390/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 391/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 392/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 393/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 394/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 395/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 396/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 397/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 398/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 399/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 400/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 401/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9583\n","Epoch 402/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9583\n","Epoch 403/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 404/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 405/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9585\n","Epoch 406/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9586\n","Epoch 407/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9586\n","Epoch 408/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 409/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 410/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9585\n","Epoch 411/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9586\n","Epoch 412/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9581\n","Epoch 413/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 414/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 415/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9584\n","Epoch 416/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9585\n","Epoch 417/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 418/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9586\n","Epoch 419/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 420/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9581\n","Epoch 421/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9583\n","Epoch 422/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9593\n","Epoch 423/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9589\n","Epoch 424/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 425/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9583\n","Epoch 426/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 427/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9583\n","Epoch 428/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9585\n","Epoch 429/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 430/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 431/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9582\n","Epoch 432/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9524 - val_loss: 4.9582\n","Epoch 433/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9584\n","Epoch 434/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9589\n","Epoch 435/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9584\n","Epoch 436/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9583\n","Epoch 437/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9524 - val_loss: 4.9588\n","Epoch 438/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9582\n","Epoch 439/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9586\n","Epoch 440/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9582\n","Epoch 441/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 442/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9525 - val_loss: 4.9584\n","Epoch 443/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 444/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 445/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9524 - val_loss: 4.9582\n","Epoch 446/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9590\n","Epoch 447/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9597\n","Epoch 448/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9586\n","Epoch 449/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9583\n","Epoch 450/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9524 - val_loss: 4.9584\n","Epoch 451/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 452/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9583\n","Epoch 453/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 454/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 455/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 456/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9585\n","Epoch 457/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 458/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9585\n","Epoch 459/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9525 - val_loss: 4.9584\n","Epoch 460/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 461/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 462/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9591\n","Epoch 463/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9524 - val_loss: 4.9586\n","Epoch 464/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9584\n","Epoch 465/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9584\n","Epoch 466/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 467/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 468/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9523 - val_loss: 4.9583\n","Epoch 469/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9587\n","Epoch 470/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9585\n","Epoch 471/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9585\n","Epoch 472/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9585\n","Epoch 473/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 474/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 475/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9584\n","Epoch 476/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 477/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9596\n","Epoch 478/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9585\n","Epoch 479/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9524 - val_loss: 4.9587\n","Epoch 480/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9585\n","Epoch 481/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9586\n","Epoch 482/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9524 - val_loss: 4.9585\n","Epoch 483/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9523 - val_loss: 4.9586\n","Epoch 484/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9587\n","Epoch 485/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9587\n","Epoch 486/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9586\n","Epoch 487/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9524 - val_loss: 4.9587\n","Epoch 488/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9524 - val_loss: 4.9588\n","Epoch 489/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9588\n","Epoch 490/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9587\n","Epoch 491/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9587\n","Epoch 492/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9588\n","Epoch 493/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9585\n","Epoch 494/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9523 - val_loss: 4.9585\n","Epoch 495/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9589\n","Epoch 496/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9588\n","Epoch 497/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9588\n","Epoch 498/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9587\n","Epoch 499/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9587\n","Epoch 500/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9523 - val_loss: 4.9587\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 36740 0.5\n","The shape of N (5908, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9914942528735632\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/500\n","5317/5317 [==============================] - 5s 1ms/step - loss: 5.0598 - val_loss: 5.1857\n","Epoch 2/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 5.0007 - val_loss: 5.0254\n","Epoch 3/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9889 - val_loss: 4.9887\n","Epoch 4/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9813 - val_loss: 4.9823\n","Epoch 5/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9757 - val_loss: 4.9799\n","Epoch 6/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9723 - val_loss: 4.9786\n","Epoch 7/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9695 - val_loss: 4.9757\n","Epoch 8/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9675 - val_loss: 4.9720\n","Epoch 9/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9655 - val_loss: 4.9734\n","Epoch 10/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9671 - val_loss: 5.0203\n","Epoch 11/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9658 - val_loss: 4.9813\n","Epoch 12/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9642 - val_loss: 5.0004\n","Epoch 13/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9630 - val_loss: 4.9767\n","Epoch 14/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9620 - val_loss: 4.9699\n","Epoch 15/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9617 - val_loss: 4.9702\n","Epoch 16/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9614 - val_loss: 4.9689\n","Epoch 17/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9608 - val_loss: 4.9672\n","Epoch 18/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9605 - val_loss: 4.9673\n","Epoch 19/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9599 - val_loss: 4.9674\n","Epoch 20/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9599 - val_loss: 4.9676\n","Epoch 21/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9599 - val_loss: 4.9693\n","Epoch 22/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9596 - val_loss: 4.9645\n","Epoch 23/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9589 - val_loss: 4.9629\n","Epoch 24/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9595 - val_loss: 4.9733\n","Epoch 25/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9620 - val_loss: 4.9694\n","Epoch 26/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9593 - val_loss: 4.9647\n","Epoch 27/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9584 - val_loss: 4.9619\n","Epoch 28/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9583 - val_loss: 4.9621\n","Epoch 29/500\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9581 - val_loss: 4.9612\n","Epoch 30/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9581 - val_loss: 4.9811\n","Epoch 31/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9602 - val_loss: 5.0016\n","Epoch 32/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9587 - val_loss: 4.9879\n","Epoch 33/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9579 - val_loss: 4.9689\n","Epoch 34/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9574 - val_loss: 4.9650\n","Epoch 35/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9573 - val_loss: 4.9617\n","Epoch 36/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9573 - val_loss: 4.9619\n","Epoch 37/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9573 - val_loss: 4.9606\n","Epoch 38/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9568 - val_loss: 4.9603\n","Epoch 39/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9566 - val_loss: 4.9601\n","Epoch 40/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9564 - val_loss: 4.9596\n","Epoch 41/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9564 - val_loss: 4.9602\n","Epoch 42/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9565 - val_loss: 4.9590\n","Epoch 43/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9561 - val_loss: 4.9585\n","Epoch 44/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9560 - val_loss: 4.9582\n","Epoch 45/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9562 - val_loss: 4.9622\n","Epoch 46/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9613 - val_loss: 4.9831\n","Epoch 47/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9586 - val_loss: 4.9673\n","Epoch 48/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9571 - val_loss: 4.9623\n","Epoch 49/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9565 - val_loss: 4.9612\n","Epoch 50/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9563 - val_loss: 4.9616\n","Epoch 51/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9562 - val_loss: 4.9601\n","Epoch 52/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9561 - val_loss: 4.9594\n","Epoch 53/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9559 - val_loss: 4.9588\n","Epoch 54/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9558 - val_loss: 4.9583\n","Epoch 55/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9560 - val_loss: 4.9590\n","Epoch 56/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9560 - val_loss: 4.9592\n","Epoch 57/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9556 - val_loss: 4.9582\n","Epoch 58/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9582 - val_loss: 4.9637\n","Epoch 59/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9568 - val_loss: 4.9604\n","Epoch 60/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9562 - val_loss: 4.9586\n","Epoch 61/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9557 - val_loss: 4.9584\n","Epoch 62/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9556 - val_loss: 4.9577\n","Epoch 63/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9556 - val_loss: 4.9576\n","Epoch 64/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9575\n","Epoch 65/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9553 - val_loss: 4.9575\n","Epoch 66/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 67/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9552 - val_loss: 4.9573\n","Epoch 68/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9552 - val_loss: 4.9573\n","Epoch 69/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9585\n","Epoch 70/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 71/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9550 - val_loss: 4.9575\n","Epoch 72/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9551 - val_loss: 4.9575\n","Epoch 73/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9554 - val_loss: 4.9601\n","Epoch 74/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9553 - val_loss: 4.9583\n","Epoch 75/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9549 - val_loss: 4.9577\n","Epoch 76/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 77/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9549 - val_loss: 4.9572\n","Epoch 78/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9548 - val_loss: 4.9572\n","Epoch 79/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 80/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9572\n","Epoch 81/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9549 - val_loss: 4.9591\n","Epoch 82/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9576 - val_loss: 4.9653\n","Epoch 83/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9560 - val_loss: 4.9596\n","Epoch 84/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9553 - val_loss: 4.9585\n","Epoch 85/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 86/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 87/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 88/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9551 - val_loss: 4.9573\n","Epoch 89/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9547 - val_loss: 4.9574\n","Epoch 90/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 91/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 92/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 93/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 94/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9570\n","Epoch 95/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 96/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 97/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 98/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 99/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 100/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9606\n","Epoch 101/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9608 - val_loss: 4.9867\n","Epoch 102/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9572 - val_loss: 4.9736\n","Epoch 103/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9561 - val_loss: 4.9636\n","Epoch 104/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9557 - val_loss: 4.9607\n","Epoch 105/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9554 - val_loss: 4.9597\n","Epoch 106/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9551 - val_loss: 4.9584\n","Epoch 107/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 108/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 109/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 110/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9546 - val_loss: 4.9571\n","Epoch 111/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 112/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9586\n","Epoch 113/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 114/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 115/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9567\n","Epoch 116/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 117/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9567\n","Epoch 118/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 119/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 120/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 121/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 122/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9626\n","Epoch 123/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9609\n","Epoch 124/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9546 - val_loss: 4.9585\n","Epoch 125/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 126/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 127/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 128/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 129/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 130/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 131/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 132/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 133/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 134/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 135/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 136/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 137/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 138/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 139/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 140/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 141/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 142/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 143/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 144/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 145/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 146/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 147/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 148/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 149/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 150/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 151/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 152/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 153/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 154/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 155/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 156/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 157/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 158/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 159/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 160/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 161/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 162/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 163/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 164/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 165/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 166/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 167/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 168/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 169/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 170/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 171/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 172/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 173/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 174/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 175/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 176/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 177/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 178/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9626\n","Epoch 179/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 180/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 181/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 182/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 183/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 184/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 185/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 186/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 187/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 188/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 189/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 190/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 191/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 192/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 193/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 194/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 195/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9537 - val_loss: 4.9765\n","Epoch 196/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9536 - val_loss: 4.9585\n","Epoch 197/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 198/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9595\n","Epoch 199/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 200/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 201/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 202/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 203/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 204/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 205/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 206/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 207/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9604\n","Epoch 208/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 209/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9597\n","Epoch 210/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 211/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 212/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 213/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 214/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9543 - val_loss: 4.9627\n","Epoch 215/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 216/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 217/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 218/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 219/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 220/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 221/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 222/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9535 - val_loss: 4.9681\n","Epoch 223/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9593\n","Epoch 224/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 225/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 226/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 227/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 228/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9606\n","Epoch 229/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 230/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 231/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 232/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 233/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 234/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 235/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 236/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 237/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 238/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 239/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9592\n","Epoch 240/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9543 - val_loss: 4.9608\n","Epoch 241/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9587\n","Epoch 242/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 243/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 244/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 245/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 246/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 247/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 248/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 249/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 250/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 251/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 252/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 253/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 254/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 255/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 256/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 257/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 258/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 259/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 260/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 261/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 262/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 263/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 264/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 265/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 266/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 267/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 268/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 269/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 270/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 271/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 272/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 273/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 274/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 275/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 276/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 277/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 278/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 279/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 280/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 281/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 282/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 283/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 284/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 285/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 286/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 287/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 288/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 289/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 290/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 291/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 292/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 293/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 294/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 295/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 296/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 297/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 298/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 299/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 300/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 301/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 302/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 303/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 304/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 305/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 306/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 307/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 308/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 309/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9583\n","Epoch 310/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 311/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 312/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 313/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 314/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 315/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 316/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 317/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 318/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 319/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 320/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 321/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 322/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9577\n","Epoch 323/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 324/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 325/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 326/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 327/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 328/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 329/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 330/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 331/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9607\n","Epoch 332/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 333/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 334/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 335/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 336/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 337/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 338/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 339/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9590\n","Epoch 340/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9588\n","Epoch 341/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 342/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 343/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 344/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 345/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 346/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 347/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9583\n","Epoch 348/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 349/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 350/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9589\n","Epoch 351/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9590\n","Epoch 352/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 353/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9583\n","Epoch 354/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9591\n","Epoch 355/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 356/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 357/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9589\n","Epoch 358/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 359/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9589\n","Epoch 360/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 361/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9579\n","Epoch 362/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 363/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9591\n","Epoch 364/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9588\n","Epoch 365/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 366/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9588\n","Epoch 367/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9589\n","Epoch 368/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9589\n","Epoch 369/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 370/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9588\n","Epoch 371/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9591\n","Epoch 372/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 373/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9590\n","Epoch 374/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9593\n","Epoch 375/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 376/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9614\n","Epoch 377/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9525 - val_loss: 4.9591\n","Epoch 378/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9590\n","Epoch 379/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 380/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 381/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 382/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9590\n","Epoch 383/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9590\n","Epoch 384/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 385/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9589\n","Epoch 386/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9596\n","Epoch 387/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9591\n","Epoch 388/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9586\n","Epoch 389/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9593\n","Epoch 390/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9591\n","Epoch 391/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9590\n","Epoch 392/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9592\n","Epoch 393/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9591\n","Epoch 394/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9591\n","Epoch 395/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9591\n","Epoch 396/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9593\n","Epoch 397/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9591\n","Epoch 398/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9593\n","Epoch 399/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9587\n","Epoch 400/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9584\n","Epoch 401/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9583\n","Epoch 402/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9583\n","Epoch 403/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 404/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9591\n","Epoch 405/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9582\n","Epoch 406/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9581\n","Epoch 407/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9582\n","Epoch 408/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9582\n","Epoch 409/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9621\n","Epoch 410/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9524 - val_loss: 4.9591\n","Epoch 411/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 412/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9583\n","Epoch 413/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9583\n","Epoch 414/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 415/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9581\n","Epoch 416/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9592\n","Epoch 417/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9582\n","Epoch 418/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 419/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9582\n","Epoch 420/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9585\n","Epoch 421/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9586\n","Epoch 422/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9586\n","Epoch 423/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9585\n","Epoch 424/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9584\n","Epoch 425/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9584\n","Epoch 426/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9583\n","Epoch 427/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9525 - val_loss: 4.9595\n","Epoch 428/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9598\n","Epoch 429/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9598\n","Epoch 430/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9598\n","Epoch 431/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9524 - val_loss: 4.9586\n","Epoch 432/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9584\n","Epoch 433/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9583\n","Epoch 434/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9524 - val_loss: 4.9585\n","Epoch 435/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 436/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9584\n","Epoch 437/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9584\n","Epoch 438/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9583\n","Epoch 439/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9582\n","Epoch 440/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9593\n","Epoch 441/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9595\n","Epoch 442/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9594\n","Epoch 443/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9588\n","Epoch 444/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9584\n","Epoch 445/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9523 - val_loss: 4.9583\n","Epoch 446/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9584\n","Epoch 447/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 448/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9523 - val_loss: 4.9596\n","Epoch 449/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9583\n","Epoch 450/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9582\n","Epoch 451/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9523 - val_loss: 4.9583\n","Epoch 452/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9582\n","Epoch 453/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9595\n","Epoch 454/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9595\n","Epoch 455/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9523 - val_loss: 4.9596\n","Epoch 456/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9594\n","Epoch 457/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9608\n","Epoch 458/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9523 - val_loss: 4.9594\n","Epoch 459/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9596\n","Epoch 460/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9595\n","Epoch 461/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9595\n","Epoch 462/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9595\n","Epoch 463/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9594\n","Epoch 464/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9594\n","Epoch 465/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9593\n","Epoch 466/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9523 - val_loss: 4.9583\n","Epoch 467/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9595\n","Epoch 468/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9595\n","Epoch 469/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9583\n","Epoch 470/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9584\n","Epoch 471/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9593\n","Epoch 472/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9583\n","Epoch 473/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9583\n","Epoch 474/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9594\n","Epoch 475/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9583\n","Epoch 476/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9585\n","Epoch 477/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9523 - val_loss: 4.9585\n","Epoch 478/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9583\n","Epoch 479/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9522 - val_loss: 4.9583\n","Epoch 480/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9593\n","Epoch 481/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9522 - val_loss: 4.9585\n","Epoch 482/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9596\n","Epoch 483/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9523 - val_loss: 4.9588\n","Epoch 484/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9586\n","Epoch 485/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9595\n","Epoch 486/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9584\n","Epoch 487/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9585\n","Epoch 488/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9582\n","Epoch 489/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9587\n","Epoch 490/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9584\n","Epoch 491/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9522 - val_loss: 4.9583\n","Epoch 492/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9583\n","Epoch 493/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9582\n","Epoch 494/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9583\n","Epoch 495/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9583\n","Epoch 496/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9599\n","Epoch 497/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9524 - val_loss: 4.9595\n","Epoch 498/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9585\n","Epoch 499/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9583\n","Epoch 500/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9586\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 43173 0.5\n","The shape of N (5908, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7496241927146912\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9843530798703213\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/500\n","5317/5317 [==============================] - 6s 1ms/step - loss: 5.0644 - val_loss: 5.2840\n","Epoch 2/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 5.0030 - val_loss: 5.0328\n","Epoch 3/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9906 - val_loss: 4.9981\n","Epoch 4/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9825 - val_loss: 4.9896\n","Epoch 5/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9770 - val_loss: 4.9815\n","Epoch 6/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9733 - val_loss: 4.9785\n","Epoch 7/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9718 - val_loss: 4.9950\n","Epoch 8/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9702 - val_loss: 4.9777\n","Epoch 9/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9671 - val_loss: 4.9729\n","Epoch 10/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9652 - val_loss: 4.9710\n","Epoch 11/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9644 - val_loss: 4.9702\n","Epoch 12/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9634 - val_loss: 4.9695\n","Epoch 13/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9631 - val_loss: 4.9739\n","Epoch 14/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9620 - val_loss: 4.9685\n","Epoch 15/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9620 - val_loss: 4.9690\n","Epoch 16/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9613 - val_loss: 4.9680\n","Epoch 17/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9607 - val_loss: 4.9763\n","Epoch 18/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9605 - val_loss: 4.9682\n","Epoch 19/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9610 - val_loss: 4.9700\n","Epoch 20/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9600 - val_loss: 4.9665\n","Epoch 21/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9596 - val_loss: 4.9722\n","Epoch 22/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9605 - val_loss: 4.9719\n","Epoch 23/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9594 - val_loss: 4.9641\n","Epoch 24/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9591 - val_loss: 4.9652\n","Epoch 25/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9587 - val_loss: 4.9627\n","Epoch 26/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9585 - val_loss: 4.9656\n","Epoch 27/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9584 - val_loss: 4.9664\n","Epoch 28/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9585 - val_loss: 4.9651\n","Epoch 29/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9579 - val_loss: 4.9625\n","Epoch 30/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9584 - val_loss: 4.9613\n","Epoch 31/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9580 - val_loss: 4.9621\n","Epoch 32/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9577 - val_loss: 4.9609\n","Epoch 33/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9575 - val_loss: 4.9606\n","Epoch 34/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9576 - val_loss: 4.9601\n","Epoch 35/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9572 - val_loss: 4.9599\n","Epoch 36/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9571 - val_loss: 4.9597\n","Epoch 37/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9571 - val_loss: 4.9598\n","Epoch 38/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9571 - val_loss: 4.9596\n","Epoch 39/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9569 - val_loss: 4.9591\n","Epoch 40/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9569 - val_loss: 4.9604\n","Epoch 41/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9569 - val_loss: 4.9599\n","Epoch 42/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9571 - val_loss: 4.9599\n","Epoch 43/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9566 - val_loss: 4.9598\n","Epoch 44/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9566 - val_loss: 4.9596\n","Epoch 45/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9564 - val_loss: 4.9591\n","Epoch 46/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9565 - val_loss: 4.9587\n","Epoch 47/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9562 - val_loss: 4.9585\n","Epoch 48/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9563 - val_loss: 4.9589\n","Epoch 49/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9574 - val_loss: 4.9602\n","Epoch 50/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9618 - val_loss: 4.9842\n","Epoch 51/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9588 - val_loss: 4.9672\n","Epoch 52/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9579 - val_loss: 4.9648\n","Epoch 53/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9573 - val_loss: 4.9608\n","Epoch 54/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9569 - val_loss: 4.9593\n","Epoch 55/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9573 - val_loss: 4.9608\n","Epoch 56/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9568 - val_loss: 4.9595\n","Epoch 57/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9565 - val_loss: 4.9607\n","Epoch 58/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9565 - val_loss: 4.9591\n","Epoch 59/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9565 - val_loss: 4.9587\n","Epoch 60/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9585\n","Epoch 61/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9560 - val_loss: 4.9581\n","Epoch 62/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9561 - val_loss: 4.9584\n","Epoch 63/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9559 - val_loss: 4.9645\n","Epoch 64/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9561 - val_loss: 4.9601\n","Epoch 65/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9558 - val_loss: 4.9585\n","Epoch 66/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9557 - val_loss: 4.9584\n","Epoch 67/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9557 - val_loss: 4.9582\n","Epoch 68/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9565 - val_loss: 4.9608\n","Epoch 69/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9563 - val_loss: 4.9588\n","Epoch 70/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9559 - val_loss: 4.9584\n","Epoch 71/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9558 - val_loss: 4.9582\n","Epoch 72/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9557 - val_loss: 4.9579\n","Epoch 73/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9557 - val_loss: 4.9579\n","Epoch 74/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9555 - val_loss: 4.9579\n","Epoch 75/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9554 - val_loss: 4.9577\n","Epoch 76/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9554 - val_loss: 4.9578\n","Epoch 77/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9557 - val_loss: 4.9744\n","Epoch 78/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9580 - val_loss: 4.9757\n","Epoch 79/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9570 - val_loss: 4.9648\n","Epoch 80/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9562 - val_loss: 4.9630\n","Epoch 81/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9559 - val_loss: 4.9726\n","Epoch 82/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9559 - val_loss: 4.9616\n","Epoch 83/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9560 - val_loss: 4.9592\n","Epoch 84/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9556 - val_loss: 4.9590\n","Epoch 85/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9557 - val_loss: 4.9584\n","Epoch 86/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9556 - val_loss: 4.9580\n","Epoch 87/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9556 - val_loss: 4.9583\n","Epoch 88/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9555 - val_loss: 4.9579\n","Epoch 89/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9554 - val_loss: 4.9579\n","Epoch 90/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 91/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 92/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 93/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9582\n","Epoch 94/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9553 - val_loss: 4.9579\n","Epoch 95/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9578\n","Epoch 96/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9580\n","Epoch 97/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9552 - val_loss: 4.9584\n","Epoch 98/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 99/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9551 - val_loss: 4.9576\n","Epoch 100/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9550 - val_loss: 4.9579\n","Epoch 101/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9550 - val_loss: 4.9583\n","Epoch 102/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9580\n","Epoch 103/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9554 - val_loss: 4.9587\n","Epoch 104/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9553 - val_loss: 4.9584\n","Epoch 105/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 106/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 107/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9584\n","Epoch 108/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 109/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 110/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 111/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 112/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9591\n","Epoch 113/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 114/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 115/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 116/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9548 - val_loss: 4.9607\n","Epoch 117/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9550 - val_loss: 4.9582\n","Epoch 118/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 119/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 120/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9547 - val_loss: 4.9582\n","Epoch 121/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 122/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 123/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 124/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 125/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 126/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 127/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9642\n","Epoch 128/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9587\n","Epoch 129/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 130/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 131/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 132/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 133/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9552 - val_loss: 4.9593\n","Epoch 134/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9558 - val_loss: 4.9593\n","Epoch 135/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9551 - val_loss: 4.9590\n","Epoch 136/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9583\n","Epoch 137/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 138/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 139/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 140/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 141/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 142/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 143/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9547 - val_loss: 4.9588\n","Epoch 144/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 145/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 146/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 147/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 148/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 149/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 150/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 151/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 152/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 153/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 154/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 155/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 156/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 157/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 158/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 159/500\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9586\n","Epoch 160/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 161/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 162/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 163/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 164/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 165/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 166/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 167/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 168/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 169/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 170/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 171/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 172/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 173/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 174/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 175/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 176/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 177/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 178/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 179/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 180/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9580 - val_loss: 5.0158\n","Epoch 181/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9572 - val_loss: 4.9772\n","Epoch 182/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9560 - val_loss: 4.9682\n","Epoch 183/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9555 - val_loss: 4.9649\n","Epoch 184/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9615\n","Epoch 185/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9552 - val_loss: 4.9605\n","Epoch 186/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9553 - val_loss: 4.9591\n","Epoch 187/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9550 - val_loss: 4.9585\n","Epoch 188/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9583\n","Epoch 189/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9549 - val_loss: 4.9587\n","Epoch 190/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 191/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 192/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 193/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 194/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 195/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 196/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 197/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 198/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 199/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 200/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 201/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 202/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9544 - val_loss: 4.9581\n","Epoch 203/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 204/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9544 - val_loss: 4.9581\n","Epoch 205/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 206/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 207/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 208/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 209/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 210/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 211/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 212/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 213/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 214/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 215/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 216/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 217/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 218/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 219/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 220/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 221/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 222/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 223/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 224/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 225/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 226/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 227/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 228/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9604\n","Epoch 229/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9545 - val_loss: 4.9595\n","Epoch 230/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9584\n","Epoch 231/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 232/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 233/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 234/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 235/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 236/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 237/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 238/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 239/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 240/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 241/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 242/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 243/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 244/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 245/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 246/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 247/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 248/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 249/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 250/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 251/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 252/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 253/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 254/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 255/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 256/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 257/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 258/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 259/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 260/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 261/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 262/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 263/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 264/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 265/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 266/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 267/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 268/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 269/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 270/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 271/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 272/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 273/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 274/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 275/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 276/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 277/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 278/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 279/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 280/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 281/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 282/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 283/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 284/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 285/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 286/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 287/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 288/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 289/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 290/500\n","5317/5317 [==============================] - 2s 307us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 291/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 292/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 293/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 294/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 295/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 296/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 297/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 298/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 299/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 300/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 301/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 302/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 303/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 304/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 305/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 306/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 307/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 308/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 309/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 310/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 311/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 312/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 313/500\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 314/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 315/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 316/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 317/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 318/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 319/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9584\n","Epoch 320/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 321/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 322/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 323/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 324/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 325/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 326/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 327/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 328/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 329/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 330/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 331/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 332/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 333/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 334/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 335/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 336/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 337/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 338/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 339/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 340/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 341/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9585\n","Epoch 342/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9678\n","Epoch 343/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9544 - val_loss: 4.9622\n","Epoch 344/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9594\n","Epoch 345/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9588\n","Epoch 346/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9584\n","Epoch 347/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 348/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 349/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 350/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 351/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 352/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 353/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 354/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 355/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 356/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 357/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 358/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 359/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 360/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 361/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 362/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 363/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 364/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 365/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 366/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 367/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 368/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 369/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 370/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 371/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 372/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 373/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 374/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 375/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 376/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 377/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 378/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 379/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 380/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 381/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 382/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 383/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 384/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 385/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 386/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 387/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 388/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 389/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 390/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 391/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 392/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 393/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 394/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9585\n","Epoch 395/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 396/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 397/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 398/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 399/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9583\n","Epoch 400/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 401/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9585\n","Epoch 402/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9583\n","Epoch 403/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 404/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 405/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 406/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 407/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 408/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 409/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 410/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 411/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 412/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 413/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 414/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 415/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 416/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 417/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 418/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 419/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9594\n","Epoch 420/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9585\n","Epoch 421/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 422/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 423/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 424/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 425/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 426/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9585\n","Epoch 427/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 428/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 429/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 430/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 431/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 432/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 433/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 434/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 435/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 436/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 437/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 438/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 439/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 440/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 441/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 442/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 443/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 444/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9586\n","Epoch 445/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 446/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 447/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 448/500\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9585\n","Epoch 449/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9585\n","Epoch 450/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 451/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 452/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9585\n","Epoch 453/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 454/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 455/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 456/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 457/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 458/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 459/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 460/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 461/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9585\n","Epoch 462/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 463/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9585\n","Epoch 464/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 465/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 466/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 467/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9585\n","Epoch 468/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 469/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9585\n","Epoch 470/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 471/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9585\n","Epoch 472/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 473/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 474/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 475/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 476/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 477/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 478/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 479/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 480/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 481/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 482/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 483/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 484/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 485/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 486/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 487/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 488/500\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 489/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 490/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 491/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 492/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 493/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 494/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 495/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 496/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 497/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9583\n","Epoch 498/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9661\n","Epoch 499/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 500/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9583\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 42406 0.5\n","The shape of N (5908, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9816475095785441\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/500\n","5317/5317 [==============================] - 7s 1ms/step - loss: 5.0775 - val_loss: 5.1591\n","Epoch 2/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 5.0059 - val_loss: 5.0279\n","Epoch 3/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9914 - val_loss: 5.0029\n","Epoch 4/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9836 - val_loss: 4.9872\n","Epoch 5/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9780 - val_loss: 4.9833\n","Epoch 6/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9739 - val_loss: 4.9765\n","Epoch 7/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9705 - val_loss: 4.9748\n","Epoch 8/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9690 - val_loss: 4.9772\n","Epoch 9/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9672 - val_loss: 4.9726\n","Epoch 10/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9670 - val_loss: 4.9841\n","Epoch 11/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9650 - val_loss: 4.9705\n","Epoch 12/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9636 - val_loss: 4.9707\n","Epoch 13/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9627 - val_loss: 4.9671\n","Epoch 14/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9623 - val_loss: 4.9682\n","Epoch 15/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9625 - val_loss: 4.9784\n","Epoch 16/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9616 - val_loss: 4.9687\n","Epoch 17/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9608 - val_loss: 4.9703\n","Epoch 18/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9605 - val_loss: 4.9660\n","Epoch 19/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9603 - val_loss: 4.9788\n","Epoch 20/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9600 - val_loss: 4.9681\n","Epoch 21/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9594 - val_loss: 4.9653\n","Epoch 22/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9591 - val_loss: 4.9648\n","Epoch 23/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9593 - val_loss: 4.9682\n","Epoch 24/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9606 - val_loss: 4.9642\n","Epoch 25/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9592 - val_loss: 4.9656\n","Epoch 26/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9587 - val_loss: 4.9623\n","Epoch 27/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9583 - val_loss: 4.9625\n","Epoch 28/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9582 - val_loss: 4.9616\n","Epoch 29/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9577 - val_loss: 4.9609\n","Epoch 30/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9577 - val_loss: 4.9837\n","Epoch 31/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9613 - val_loss: 4.9810\n","Epoch 32/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9593 - val_loss: 4.9690\n","Epoch 33/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9584 - val_loss: 4.9632\n","Epoch 34/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9578 - val_loss: 4.9624\n","Epoch 35/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9578 - val_loss: 4.9615\n","Epoch 36/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9579 - val_loss: 4.9610\n","Epoch 37/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9575 - val_loss: 4.9616\n","Epoch 38/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9573 - val_loss: 4.9606\n","Epoch 39/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9572 - val_loss: 4.9600\n","Epoch 40/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9571 - val_loss: 4.9600\n","Epoch 41/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9569 - val_loss: 4.9594\n","Epoch 42/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9568 - val_loss: 4.9597\n","Epoch 43/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9567 - val_loss: 4.9594\n","Epoch 44/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9565 - val_loss: 4.9590\n","Epoch 45/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9565 - val_loss: 4.9588\n","Epoch 46/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9565 - val_loss: 4.9588\n","Epoch 47/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9571 - val_loss: 4.9614\n","Epoch 48/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9568 - val_loss: 4.9597\n","Epoch 49/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9563 - val_loss: 4.9588\n","Epoch 50/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9561 - val_loss: 4.9592\n","Epoch 51/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9563 - val_loss: 4.9606\n","Epoch 52/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9563 - val_loss: 4.9591\n","Epoch 53/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9563 - val_loss: 4.9594\n","Epoch 54/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9577 - val_loss: 4.9615\n","Epoch 55/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9566 - val_loss: 4.9596\n","Epoch 56/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9573 - val_loss: 4.9595\n","Epoch 57/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9564 - val_loss: 4.9588\n","Epoch 58/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9559 - val_loss: 4.9585\n","Epoch 59/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9566 - val_loss: 4.9784\n","Epoch 60/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9569 - val_loss: 4.9625\n","Epoch 61/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9562 - val_loss: 4.9595\n","Epoch 62/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9561 - val_loss: 4.9587\n","Epoch 63/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9560 - val_loss: 4.9585\n","Epoch 64/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9558 - val_loss: 4.9585\n","Epoch 65/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9560 - val_loss: 4.9583\n","Epoch 66/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9557 - val_loss: 4.9582\n","Epoch 67/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9557 - val_loss: 4.9581\n","Epoch 68/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9555 - val_loss: 4.9579\n","Epoch 69/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9555 - val_loss: 4.9581\n","Epoch 70/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9558 - val_loss: 4.9581\n","Epoch 71/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9555 - val_loss: 4.9579\n","Epoch 72/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9556 - val_loss: 4.9621\n","Epoch 73/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9561 - val_loss: 4.9605\n","Epoch 74/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9558 - val_loss: 4.9592\n","Epoch 75/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9555 - val_loss: 4.9584\n","Epoch 76/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9555 - val_loss: 4.9586\n","Epoch 77/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9554 - val_loss: 4.9583\n","Epoch 78/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 79/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9553 - val_loss: 4.9579\n","Epoch 80/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9552 - val_loss: 4.9579\n","Epoch 81/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 82/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9553 - val_loss: 4.9578\n","Epoch 83/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9552 - val_loss: 4.9580\n","Epoch 84/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9551 - val_loss: 4.9580\n","Epoch 85/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 86/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9553 - val_loss: 4.9579\n","Epoch 87/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9561 - val_loss: 4.9588\n","Epoch 88/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9565 - val_loss: 4.9594\n","Epoch 89/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9558 - val_loss: 4.9588\n","Epoch 90/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9556 - val_loss: 4.9581\n","Epoch 91/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9555 - val_loss: 4.9580\n","Epoch 92/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9554 - val_loss: 4.9581\n","Epoch 93/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9552 - val_loss: 4.9577\n","Epoch 94/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 95/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 96/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9575\n","Epoch 97/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 98/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9579\n","Epoch 99/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9550 - val_loss: 4.9584\n","Epoch 100/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9579\n","Epoch 101/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 102/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 103/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 104/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 105/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 106/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 107/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 108/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 109/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 110/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 111/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 112/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 113/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 114/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 115/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 116/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 117/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9547 - val_loss: 4.9582\n","Epoch 118/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9547 - val_loss: 4.9582\n","Epoch 119/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 120/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 121/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 122/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 123/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 124/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 125/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 126/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 127/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 128/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 129/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 130/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 131/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 132/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 133/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9546 - val_loss: 4.9583\n","Epoch 134/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 135/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 136/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 137/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 138/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 139/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 140/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 141/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 142/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 143/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 144/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 145/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 146/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 147/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 148/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 149/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 150/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 151/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 152/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 153/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 154/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 155/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 156/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 157/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 158/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 159/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 160/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 161/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 162/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 163/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 164/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 165/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 166/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 167/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 168/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 169/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 170/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 171/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 172/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 173/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9583\n","Epoch 174/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 175/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 176/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 177/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 178/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 179/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 180/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 181/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 182/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9613\n","Epoch 183/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9588\n","Epoch 184/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 185/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 186/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 187/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 188/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 189/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 190/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 191/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 192/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 193/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 194/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 195/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 196/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 197/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 198/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 199/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 200/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 201/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 202/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 203/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 204/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 205/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 206/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 207/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 208/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 209/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 210/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 211/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 212/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 213/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 214/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 215/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 216/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 217/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 218/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 219/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 220/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9583\n","Epoch 221/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 222/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 223/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 224/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9587\n","Epoch 225/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9583\n","Epoch 226/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 227/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 228/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 229/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 230/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 231/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 232/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 233/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 234/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9583\n","Epoch 235/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 236/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 237/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9583\n","Epoch 238/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 239/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9583\n","Epoch 240/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9584\n","Epoch 241/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9583\n","Epoch 242/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9584\n","Epoch 243/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 244/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 245/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 246/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 247/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 248/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 249/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 250/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 251/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 252/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 253/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9585\n","Epoch 254/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 255/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 256/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 257/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 258/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 259/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9596\n","Epoch 260/500\n","5317/5317 [==============================] - 2s 310us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 261/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 262/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9600\n","Epoch 263/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9586\n","Epoch 264/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 265/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 266/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 267/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 268/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 269/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 270/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 271/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 272/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 273/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 274/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 275/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 276/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 277/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 278/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 279/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 280/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 281/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 282/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 283/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 284/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 285/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 286/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 287/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9590\n","Epoch 288/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9607\n","Epoch 289/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 290/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9590\n","Epoch 291/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 292/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 293/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 294/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 295/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 296/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 297/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 298/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 299/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 300/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 301/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 302/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 303/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 304/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 305/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 306/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 307/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 308/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 309/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 310/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 311/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 312/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 313/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 314/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9584\n","Epoch 315/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 316/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 317/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 318/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9583\n","Epoch 319/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 320/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 321/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 322/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 323/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 324/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 325/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 326/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 327/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 328/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 329/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 330/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 331/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 332/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 333/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 334/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 335/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 336/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 337/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 338/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 339/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 340/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 341/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 342/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 343/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 344/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 345/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 346/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 347/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 348/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 349/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 350/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 351/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 352/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 353/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 354/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 355/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 356/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 357/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 358/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 359/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 360/500\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 361/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9585\n","Epoch 362/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 363/500\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9589\n","Epoch 364/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9589\n","Epoch 365/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 366/500\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 367/500\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 368/500\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 369/500\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 370/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 371/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9532 - val_loss: 4.9621\n","Epoch 372/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 373/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9533 - val_loss: 4.9586\n","Epoch 374/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9533 - val_loss: 4.9585\n","Epoch 375/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 376/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 377/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 378/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 379/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 380/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9532 - val_loss: 4.9587\n","Epoch 381/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9585\n","Epoch 382/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 383/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 384/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9531 - val_loss: 4.9585\n","Epoch 385/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 386/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 387/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9532 - val_loss: 4.9593\n","Epoch 388/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9585\n","Epoch 389/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 390/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 391/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 392/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 393/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9586\n","Epoch 394/500\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9588\n","Epoch 395/500\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9588\n","Epoch 396/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 397/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9587\n","Epoch 398/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 399/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9531 - val_loss: 4.9588\n","Epoch 400/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9531 - val_loss: 4.9586\n","Epoch 401/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 402/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9532 - val_loss: 4.9588\n","Epoch 403/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9531 - val_loss: 4.9589\n","Epoch 404/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9588\n","Epoch 405/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 406/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9531 - val_loss: 4.9588\n","Epoch 407/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9588\n","Epoch 408/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 409/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9588\n","Epoch 410/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9589\n","Epoch 411/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9530 - val_loss: 4.9588\n","Epoch 412/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9532 - val_loss: 4.9594\n","Epoch 413/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9531 - val_loss: 4.9589\n","Epoch 414/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9589\n","Epoch 415/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9589\n","Epoch 416/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9588\n","Epoch 417/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 418/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9589\n","Epoch 419/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9530 - val_loss: 4.9588\n","Epoch 420/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9531 - val_loss: 4.9589\n","Epoch 421/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9530 - val_loss: 4.9593\n","Epoch 422/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9530 - val_loss: 4.9591\n","Epoch 423/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9532 - val_loss: 4.9590\n","Epoch 424/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9590\n","Epoch 425/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9531 - val_loss: 4.9588\n","Epoch 426/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 427/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9588\n","Epoch 428/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9592\n","Epoch 429/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9532 - val_loss: 4.9591\n","Epoch 430/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 431/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9530 - val_loss: 4.9588\n","Epoch 432/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 433/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9531 - val_loss: 4.9590\n","Epoch 434/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 435/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 436/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9530 - val_loss: 4.9588\n","Epoch 437/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9530 - val_loss: 4.9591\n","Epoch 438/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9592\n","Epoch 439/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9529 - val_loss: 4.9590\n","Epoch 440/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 441/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 442/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9530 - val_loss: 4.9589\n","Epoch 443/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9530 - val_loss: 4.9591\n","Epoch 444/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9529 - val_loss: 4.9590\n","Epoch 445/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 446/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9530 - val_loss: 4.9591\n","Epoch 447/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9591\n","Epoch 448/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9593\n","Epoch 449/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9530 - val_loss: 4.9593\n","Epoch 450/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9594\n","Epoch 451/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9594\n","Epoch 452/500\n","5317/5317 [==============================] - 1s 280us/step - loss: 4.9530 - val_loss: 4.9592\n","Epoch 453/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9529 - val_loss: 4.9593\n","Epoch 454/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9530 - val_loss: 4.9589\n","Epoch 455/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9592\n","Epoch 456/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9593\n","Epoch 457/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9592\n","Epoch 458/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9593\n","Epoch 459/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9592\n","Epoch 460/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9593\n","Epoch 461/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9594\n","Epoch 462/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9590\n","Epoch 463/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9593\n","Epoch 464/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9592\n","Epoch 465/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9594\n","Epoch 466/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9596\n","Epoch 467/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9594\n","Epoch 468/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9594\n","Epoch 469/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9596\n","Epoch 470/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9597\n","Epoch 471/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9595\n","Epoch 472/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9596\n","Epoch 473/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9597\n","Epoch 474/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9597\n","Epoch 475/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9598\n","Epoch 476/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9597\n","Epoch 477/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9598\n","Epoch 478/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9598\n","Epoch 479/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9530 - val_loss: 4.9597\n","Epoch 480/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9599\n","Epoch 481/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9530 - val_loss: 4.9597\n","Epoch 482/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9530 - val_loss: 4.9596\n","Epoch 483/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9596\n","Epoch 484/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9529 - val_loss: 4.9598\n","Epoch 485/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9600\n","Epoch 486/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9601\n","Epoch 487/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9602\n","Epoch 488/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9530 - val_loss: 4.9718\n","Epoch 489/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9607\n","Epoch 490/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9529 - val_loss: 4.9601\n","Epoch 491/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9600\n","Epoch 492/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9599\n","Epoch 493/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9529 - val_loss: 4.9605\n","Epoch 494/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9529 - val_loss: 4.9601\n","Epoch 495/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9602\n","Epoch 496/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9529 - val_loss: 4.9601\n","Epoch 497/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9599\n","Epoch 498/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9598\n","Epoch 499/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9600\n","Epoch 500/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9605\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 45565 0.5\n","The shape of N (5908, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7496737241744995\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9878868258178604\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/500\n","5317/5317 [==============================] - 6s 1ms/step - loss: 5.0660 - val_loss: 5.1792\n","Epoch 2/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 5.0025 - val_loss: 5.0122\n","Epoch 3/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9893 - val_loss: 4.9961\n","Epoch 4/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9825 - val_loss: 4.9900\n","Epoch 5/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9775 - val_loss: 4.9800\n","Epoch 6/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9737 - val_loss: 4.9762\n","Epoch 7/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9711 - val_loss: 4.9733\n","Epoch 8/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9687 - val_loss: 4.9712\n","Epoch 9/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9668 - val_loss: 4.9718\n","Epoch 10/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9660 - val_loss: 4.9821\n","Epoch 11/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9655 - val_loss: 4.9732\n","Epoch 12/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9640 - val_loss: 4.9692\n","Epoch 13/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9629 - val_loss: 4.9673\n","Epoch 14/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9626 - val_loss: 4.9682\n","Epoch 15/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9626 - val_loss: 4.9716\n","Epoch 16/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9616 - val_loss: 4.9659\n","Epoch 17/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9613 - val_loss: 4.9660\n","Epoch 18/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9611 - val_loss: 4.9682\n","Epoch 19/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9598 - val_loss: 4.9667\n","Epoch 20/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9594 - val_loss: 4.9707\n","Epoch 21/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9597 - val_loss: 4.9667\n","Epoch 22/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9594 - val_loss: 4.9635\n","Epoch 23/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9590 - val_loss: 4.9644\n","Epoch 24/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9588 - val_loss: 4.9640\n","Epoch 25/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9589 - val_loss: 4.9628\n","Epoch 26/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9587 - val_loss: 4.9680\n","Epoch 27/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9585 - val_loss: 4.9642\n","Epoch 28/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9604 - val_loss: 4.9817\n","Epoch 29/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9610 - val_loss: 4.9715\n","Epoch 30/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9612 - val_loss: 4.9850\n","Epoch 31/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9601 - val_loss: 4.9722\n","Epoch 32/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9588 - val_loss: 4.9653\n","Epoch 33/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9583 - val_loss: 4.9655\n","Epoch 34/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9584 - val_loss: 4.9617\n","Epoch 35/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9579 - val_loss: 4.9606\n","Epoch 36/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9578 - val_loss: 4.9599\n","Epoch 37/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9576 - val_loss: 4.9596\n","Epoch 38/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9572 - val_loss: 4.9591\n","Epoch 39/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9578 - val_loss: 4.9625\n","Epoch 40/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9573 - val_loss: 4.9600\n","Epoch 41/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9573 - val_loss: 4.9631\n","Epoch 42/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9576 - val_loss: 4.9627\n","Epoch 43/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9571 - val_loss: 4.9598\n","Epoch 44/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9569 - val_loss: 4.9598\n","Epoch 45/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9566 - val_loss: 4.9596\n","Epoch 46/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9566 - val_loss: 4.9596\n","Epoch 47/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9565 - val_loss: 4.9592\n","Epoch 48/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9565 - val_loss: 4.9593\n","Epoch 49/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9564 - val_loss: 4.9584\n","Epoch 50/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9562 - val_loss: 4.9584\n","Epoch 51/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9563 - val_loss: 4.9593\n","Epoch 52/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9561 - val_loss: 4.9588\n","Epoch 53/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9562 - val_loss: 4.9615\n","Epoch 54/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9560 - val_loss: 4.9588\n","Epoch 55/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9559 - val_loss: 4.9587\n","Epoch 56/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9559 - val_loss: 4.9584\n","Epoch 57/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9557 - val_loss: 4.9580\n","Epoch 58/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9558 - val_loss: 4.9585\n","Epoch 59/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9558 - val_loss: 4.9585\n","Epoch 60/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9556 - val_loss: 4.9582\n","Epoch 61/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9557 - val_loss: 4.9585\n","Epoch 62/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9555 - val_loss: 4.9581\n","Epoch 63/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9554 - val_loss: 4.9599\n","Epoch 64/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9555 - val_loss: 4.9602\n","Epoch 65/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9555 - val_loss: 4.9591\n","Epoch 66/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9554 - val_loss: 4.9585\n","Epoch 67/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9555 - val_loss: 4.9584\n","Epoch 68/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9555 - val_loss: 4.9597\n","Epoch 69/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9553 - val_loss: 4.9580\n","Epoch 70/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9553 - val_loss: 4.9579\n","Epoch 71/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 72/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9552 - val_loss: 4.9580\n","Epoch 73/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9553 - val_loss: 4.9587\n","Epoch 74/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9553 - val_loss: 4.9583\n","Epoch 75/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9552 - val_loss: 4.9588\n","Epoch 76/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 77/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 78/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9552 - val_loss: 4.9635\n","Epoch 79/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9552 - val_loss: 4.9592\n","Epoch 80/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 81/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9551 - val_loss: 4.9584\n","Epoch 82/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 83/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 84/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9615 - val_loss: 4.9766\n","Epoch 85/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9574 - val_loss: 4.9627\n","Epoch 86/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9565 - val_loss: 4.9599\n","Epoch 87/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9564 - val_loss: 4.9594\n","Epoch 88/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9560 - val_loss: 4.9583\n","Epoch 89/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9556 - val_loss: 4.9581\n","Epoch 90/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9554 - val_loss: 4.9578\n","Epoch 91/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 92/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9554 - val_loss: 4.9577\n","Epoch 93/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9553 - val_loss: 4.9578\n","Epoch 94/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9552 - val_loss: 4.9577\n","Epoch 95/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 96/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 97/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9550 - val_loss: 4.9583\n","Epoch 98/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 99/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 100/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 101/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 102/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 103/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 104/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 105/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 106/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 107/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9548 - val_loss: 4.9619\n","Epoch 108/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9550 - val_loss: 4.9583\n","Epoch 109/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 110/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9548 - val_loss: 4.9615\n","Epoch 111/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9549 - val_loss: 4.9586\n","Epoch 112/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 113/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 114/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 115/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 116/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 117/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9547 - val_loss: 4.9618\n","Epoch 118/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9549 - val_loss: 4.9583\n","Epoch 119/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 120/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 121/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 122/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 123/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 124/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 125/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 126/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 127/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 128/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 129/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 130/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 131/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 132/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 133/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 134/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 135/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 136/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 137/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 138/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 139/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 140/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 141/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 142/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 143/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 144/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 145/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9736\n","Epoch 146/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9545 - val_loss: 4.9590\n","Epoch 147/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 148/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9545 - val_loss: 4.9586\n","Epoch 149/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9548 - val_loss: 4.9585\n","Epoch 150/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 151/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 152/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 153/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 154/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 155/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 156/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 157/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 158/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 159/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 160/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 161/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 162/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 163/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 164/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 165/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 166/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 167/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 168/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9542 - val_loss: 4.9585\n","Epoch 169/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9544 - val_loss: 4.9613\n","Epoch 170/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 171/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 172/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 173/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 174/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 175/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 176/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 177/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 178/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 179/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 180/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 181/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9586\n","Epoch 182/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 183/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 184/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 185/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 186/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 187/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 188/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 189/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 190/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 191/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9583\n","Epoch 192/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9584\n","Epoch 193/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9589\n","Epoch 194/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9539 - val_loss: 4.9585\n","Epoch 195/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9537 - val_loss: 4.9585\n","Epoch 196/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 197/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9537 - val_loss: 4.9584\n","Epoch 198/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 199/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9537 - val_loss: 4.9583\n","Epoch 200/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9585\n","Epoch 201/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9586\n","Epoch 202/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 203/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 204/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9584\n","Epoch 205/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9587\n","Epoch 206/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9585\n","Epoch 207/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9537 - val_loss: 4.9583\n","Epoch 208/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 209/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9584\n","Epoch 210/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 211/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9585\n","Epoch 212/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9535 - val_loss: 4.9583\n","Epoch 213/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9584\n","Epoch 214/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9537 - val_loss: 4.9584\n","Epoch 215/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9536 - val_loss: 4.9585\n","Epoch 216/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9537 - val_loss: 4.9713\n","Epoch 217/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9546 - val_loss: 4.9596\n","Epoch 218/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9542 - val_loss: 4.9586\n","Epoch 219/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 220/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 221/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 222/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 223/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 224/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 225/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 226/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 227/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 228/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 229/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 230/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 231/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 232/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 233/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 234/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 235/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 236/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 237/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 238/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 239/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 240/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 241/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 242/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 243/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 244/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 245/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 246/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 247/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 248/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 249/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 250/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9535 - val_loss: 4.9583\n","Epoch 251/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 252/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 253/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 254/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 255/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 256/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 257/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 258/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 259/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 260/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 261/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 262/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 263/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 264/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 265/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9604\n","Epoch 266/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9537 - val_loss: 4.9591\n","Epoch 267/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9535 - val_loss: 4.9587\n","Epoch 268/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 269/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 270/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9584\n","Epoch 271/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 272/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 273/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 274/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 275/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 276/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 277/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 278/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 279/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 280/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 281/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 282/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9584\n","Epoch 283/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 284/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 285/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 286/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 287/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 288/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 289/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 290/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 291/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 292/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 293/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9533 - val_loss: 4.9586\n","Epoch 294/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 295/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 296/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 297/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9583\n","Epoch 298/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 299/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 300/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 301/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 302/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 303/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 304/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 305/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9585\n","Epoch 306/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9588\n","Epoch 307/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9588\n","Epoch 308/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9587\n","Epoch 309/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9533 - val_loss: 4.9589\n","Epoch 310/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9589\n","Epoch 311/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9589\n","Epoch 312/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9586\n","Epoch 313/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 314/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 315/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9588\n","Epoch 316/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9587\n","Epoch 317/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 318/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9588\n","Epoch 319/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9586\n","Epoch 320/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 321/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 322/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9532 - val_loss: 4.9588\n","Epoch 323/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9589\n","Epoch 324/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9588\n","Epoch 325/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9589\n","Epoch 326/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9593\n","Epoch 327/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9591\n","Epoch 328/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9531 - val_loss: 4.9589\n","Epoch 329/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9588\n","Epoch 330/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9590\n","Epoch 331/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9588\n","Epoch 332/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9591\n","Epoch 333/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9590\n","Epoch 334/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9589\n","Epoch 335/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9590\n","Epoch 336/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9532 - val_loss: 4.9589\n","Epoch 337/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9590\n","Epoch 338/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9593\n","Epoch 339/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9590\n","Epoch 340/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9589\n","Epoch 341/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9589\n","Epoch 342/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9589\n","Epoch 343/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9591\n","Epoch 344/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9532 - val_loss: 4.9589\n","Epoch 345/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9589\n","Epoch 346/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9589\n","Epoch 347/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9589\n","Epoch 348/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9589\n","Epoch 349/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9590\n","Epoch 350/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9589\n","Epoch 351/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 352/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9591\n","Epoch 353/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9593\n","Epoch 354/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9589\n","Epoch 355/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9592\n","Epoch 356/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9591\n","Epoch 357/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9591\n","Epoch 358/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9591\n","Epoch 359/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9589\n","Epoch 360/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9590\n","Epoch 361/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9589\n","Epoch 362/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 363/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9590\n","Epoch 364/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9591\n","Epoch 365/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 366/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 367/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9530 - val_loss: 4.9591\n","Epoch 368/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 369/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9529 - val_loss: 4.9589\n","Epoch 370/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9590\n","Epoch 371/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9591\n","Epoch 372/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9531 - val_loss: 4.9590\n","Epoch 373/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9591\n","Epoch 374/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9592\n","Epoch 375/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9591\n","Epoch 376/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9590\n","Epoch 377/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 378/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9530 - val_loss: 4.9592\n","Epoch 379/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9592\n","Epoch 380/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 381/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 382/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 383/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9591\n","Epoch 384/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9590\n","Epoch 385/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9591\n","Epoch 386/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9530 - val_loss: 4.9594\n","Epoch 387/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9591\n","Epoch 388/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9590\n","Epoch 389/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9589\n","Epoch 390/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 391/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9590\n","Epoch 392/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9589\n","Epoch 393/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9590\n","Epoch 394/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9591\n","Epoch 395/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9590\n","Epoch 396/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9591\n","Epoch 397/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9593\n","Epoch 398/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9591\n","Epoch 399/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9591\n","Epoch 400/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9591\n","Epoch 401/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9590\n","Epoch 402/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9590\n","Epoch 403/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9528 - val_loss: 4.9590\n","Epoch 404/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9591\n","Epoch 405/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9529 - val_loss: 4.9590\n","Epoch 406/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9529 - val_loss: 4.9590\n","Epoch 407/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9529 - val_loss: 4.9590\n","Epoch 408/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9591\n","Epoch 409/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9591\n","Epoch 410/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9589\n","Epoch 411/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9590\n","Epoch 412/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9594\n","Epoch 413/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9591\n","Epoch 414/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9593\n","Epoch 415/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9591\n","Epoch 416/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9590\n","Epoch 417/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9591\n","Epoch 418/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9592\n","Epoch 419/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9594\n","Epoch 420/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9592\n","Epoch 421/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9592\n","Epoch 422/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9592\n","Epoch 423/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9590\n","Epoch 424/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 425/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 426/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9589\n","Epoch 427/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9589\n","Epoch 428/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9586\n","Epoch 429/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9592\n","Epoch 430/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9590\n","Epoch 431/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9589\n","Epoch 432/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9590\n","Epoch 433/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9583\n","Epoch 434/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9588\n","Epoch 435/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9590\n","Epoch 436/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9584\n","Epoch 437/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9593\n","Epoch 438/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9607\n","Epoch 439/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9599\n","Epoch 440/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9594\n","Epoch 441/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9531 - val_loss: 4.9595\n","Epoch 442/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9592\n","Epoch 443/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9595\n","Epoch 444/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9594\n","Epoch 445/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9593\n","Epoch 446/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9593\n","Epoch 447/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9593\n","Epoch 448/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9593\n","Epoch 449/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9591\n","Epoch 450/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9591\n","Epoch 451/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9591\n","Epoch 452/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9593\n","Epoch 453/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9592\n","Epoch 454/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9592\n","Epoch 455/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9592\n","Epoch 456/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9593\n","Epoch 457/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9594\n","Epoch 458/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9594\n","Epoch 459/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9599\n","Epoch 460/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9594\n","Epoch 461/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9594\n","Epoch 462/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9594\n","Epoch 463/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9593\n","Epoch 464/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9596\n","Epoch 465/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 466/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9593\n","Epoch 467/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9594\n","Epoch 468/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9593\n","Epoch 469/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9583\n","Epoch 470/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9582\n","Epoch 471/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 472/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9585\n","Epoch 473/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9584\n","Epoch 474/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 475/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9591\n","Epoch 476/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9594\n","Epoch 477/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9589\n","Epoch 478/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 479/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9587\n","Epoch 480/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9593\n","Epoch 481/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 482/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 483/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 484/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 485/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 486/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 487/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 488/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 489/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 490/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 491/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 492/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9595\n","Epoch 493/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9589\n","Epoch 494/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9594\n","Epoch 495/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 496/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9594\n","Epoch 497/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9594\n","Epoch 498/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9591\n","Epoch 499/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9599\n","Epoch 500/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9594\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 42621 0.5\n","The shape of N (5908, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7480650544166565\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9873710580607132\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/500\n","5317/5317 [==============================] - 7s 1ms/step - loss: 5.0628 - val_loss: 5.1272\n","Epoch 2/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9999 - val_loss: 5.0190\n","Epoch 3/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9877 - val_loss: 4.9949\n","Epoch 4/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9814 - val_loss: 4.9944\n","Epoch 5/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9764 - val_loss: 4.9879\n","Epoch 6/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9723 - val_loss: 4.9819\n","Epoch 7/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9710 - val_loss: 4.9846\n","Epoch 8/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9705 - val_loss: 4.9807\n","Epoch 9/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9691 - val_loss: 4.9745\n","Epoch 10/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9658 - val_loss: 4.9713\n","Epoch 11/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9646 - val_loss: 4.9697\n","Epoch 12/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9637 - val_loss: 4.9798\n","Epoch 13/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9635 - val_loss: 4.9692\n","Epoch 14/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9629 - val_loss: 4.9698\n","Epoch 15/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9617 - val_loss: 4.9683\n","Epoch 16/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9613 - val_loss: 4.9667\n","Epoch 17/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9603 - val_loss: 4.9654\n","Epoch 18/500\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9599 - val_loss: 4.9654\n","Epoch 19/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9621 - val_loss: 4.9649\n","Epoch 20/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9599 - val_loss: 4.9634\n","Epoch 21/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9593 - val_loss: 4.9644\n","Epoch 22/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9589 - val_loss: 4.9645\n","Epoch 23/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9587 - val_loss: 4.9644\n","Epoch 24/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9588 - val_loss: 4.9641\n","Epoch 25/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9583 - val_loss: 4.9627\n","Epoch 26/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9580 - val_loss: 4.9616\n","Epoch 27/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9580 - val_loss: 4.9631\n","Epoch 28/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9578 - val_loss: 4.9615\n","Epoch 29/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9575 - val_loss: 4.9631\n","Epoch 30/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9593 - val_loss: 4.9712\n","Epoch 31/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9586 - val_loss: 4.9665\n","Epoch 32/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9584 - val_loss: 4.9624\n","Epoch 33/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9577 - val_loss: 4.9609\n","Epoch 34/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9572 - val_loss: 4.9600\n","Epoch 35/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9571 - val_loss: 4.9601\n","Epoch 36/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9570 - val_loss: 4.9602\n","Epoch 37/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9570 - val_loss: 4.9597\n","Epoch 38/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9568 - val_loss: 4.9594\n","Epoch 39/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9579 - val_loss: 4.9704\n","Epoch 40/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9631 - val_loss: 4.9927\n","Epoch 41/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9599 - val_loss: 4.9709\n","Epoch 42/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9584 - val_loss: 4.9680\n","Epoch 43/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9575 - val_loss: 4.9622\n","Epoch 44/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9572 - val_loss: 4.9604\n","Epoch 45/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9571 - val_loss: 4.9595\n","Epoch 46/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9569 - val_loss: 4.9585\n","Epoch 47/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9569 - val_loss: 4.9585\n","Epoch 48/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9573 - val_loss: 4.9599\n","Epoch 49/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9568 - val_loss: 4.9594\n","Epoch 50/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9562 - val_loss: 4.9590\n","Epoch 51/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9561 - val_loss: 4.9581\n","Epoch 52/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9560 - val_loss: 4.9578\n","Epoch 53/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9559 - val_loss: 4.9578\n","Epoch 54/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9559 - val_loss: 4.9578\n","Epoch 55/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9560 - val_loss: 4.9605\n","Epoch 56/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9560 - val_loss: 4.9583\n","Epoch 57/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9560 - val_loss: 4.9577\n","Epoch 58/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9558 - val_loss: 4.9579\n","Epoch 59/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9559 - val_loss: 4.9581\n","Epoch 60/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9557 - val_loss: 4.9580\n","Epoch 61/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9557 - val_loss: 4.9579\n","Epoch 62/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9555 - val_loss: 4.9576\n","Epoch 63/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9555 - val_loss: 4.9577\n","Epoch 64/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9561 - val_loss: 4.9709\n","Epoch 65/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9591 - val_loss: 4.9719\n","Epoch 66/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9569 - val_loss: 4.9627\n","Epoch 67/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9565 - val_loss: 4.9590\n","Epoch 68/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9559 - val_loss: 4.9584\n","Epoch 69/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9557 - val_loss: 4.9580\n","Epoch 70/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9556 - val_loss: 4.9576\n","Epoch 71/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9555 - val_loss: 4.9579\n","Epoch 72/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9556 - val_loss: 4.9603\n","Epoch 73/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9557 - val_loss: 4.9591\n","Epoch 74/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9555 - val_loss: 4.9580\n","Epoch 75/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9554 - val_loss: 4.9579\n","Epoch 76/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9553 - val_loss: 4.9575\n","Epoch 77/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9552 - val_loss: 4.9575\n","Epoch 78/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 79/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9552 - val_loss: 4.9573\n","Epoch 80/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9551 - val_loss: 4.9573\n","Epoch 81/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9550 - val_loss: 4.9574\n","Epoch 82/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9550 - val_loss: 4.9575\n","Epoch 83/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9556 - val_loss: 4.9627\n","Epoch 84/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9569 - val_loss: 4.9597\n","Epoch 85/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9556 - val_loss: 4.9581\n","Epoch 86/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9554 - val_loss: 4.9576\n","Epoch 87/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9554 - val_loss: 4.9584\n","Epoch 88/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9551 - val_loss: 4.9575\n","Epoch 89/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9551 - val_loss: 4.9576\n","Epoch 90/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9551 - val_loss: 4.9573\n","Epoch 91/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 92/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9548 - val_loss: 4.9571\n","Epoch 93/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9548 - val_loss: 4.9571\n","Epoch 94/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9551 - val_loss: 4.9617\n","Epoch 95/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9567 - val_loss: 4.9620\n","Epoch 96/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9555 - val_loss: 4.9584\n","Epoch 97/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 98/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9550 - val_loss: 4.9574\n","Epoch 99/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9549 - val_loss: 4.9572\n","Epoch 100/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9549 - val_loss: 4.9573\n","Epoch 101/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 102/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 103/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 104/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9546 - val_loss: 4.9571\n","Epoch 105/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 106/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 107/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 108/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9546 - val_loss: 4.9604\n","Epoch 109/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 110/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 111/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 112/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 113/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 114/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 115/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 116/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 117/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 118/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 119/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 120/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 121/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 122/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 123/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 124/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 125/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 126/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 127/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 128/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 129/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 130/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 131/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 132/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 133/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 134/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 135/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 136/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 137/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 138/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 139/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 140/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 141/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 142/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 143/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 144/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 145/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 146/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 147/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 148/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 149/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 150/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 151/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 152/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 153/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 154/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 155/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 156/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 157/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 158/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9552 - val_loss: 4.9650\n","Epoch 159/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9545 - val_loss: 4.9598\n","Epoch 160/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 161/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 162/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 163/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 164/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 165/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 166/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 167/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 168/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 169/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 170/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 171/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 172/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 173/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 174/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 175/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 176/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 177/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 178/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 179/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 180/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 181/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 182/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 183/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 184/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 185/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 186/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 187/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 188/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 189/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 190/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 191/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 192/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 193/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 194/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 195/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 196/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 197/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 198/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 199/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 200/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 201/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 202/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 203/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 204/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 205/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 206/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 207/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 208/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 209/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 210/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 211/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 212/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 213/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 214/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 215/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 216/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 217/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 218/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 219/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 220/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 221/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 222/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 223/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 224/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 225/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 226/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 227/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 228/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 229/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 230/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 231/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 232/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 233/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 234/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 235/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 236/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9592\n","Epoch 237/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 238/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 239/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 240/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 241/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 242/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 243/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 244/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 245/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 246/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 247/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 248/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 249/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 250/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 251/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 252/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 253/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 254/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 255/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 256/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9583\n","Epoch 257/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 258/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 259/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 260/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 261/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9596\n","Epoch 262/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 263/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 264/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9589\n","Epoch 265/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 266/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 267/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 268/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 269/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 270/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 271/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 272/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 273/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 274/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 275/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 276/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 277/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 278/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 279/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9592\n","Epoch 280/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 281/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 282/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 283/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 284/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 285/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 286/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 287/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 288/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 289/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 290/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 291/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 292/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 293/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 294/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 295/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 296/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 297/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 298/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 299/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 300/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 301/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 302/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 303/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 304/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 305/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 306/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 307/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 308/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 309/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 310/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9740\n","Epoch 311/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9542 - val_loss: 4.9647\n","Epoch 312/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9619\n","Epoch 313/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9592\n","Epoch 314/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9589\n","Epoch 315/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 316/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 317/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 318/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 319/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 320/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 321/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 322/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 323/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 324/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 325/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 326/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 327/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 328/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 329/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 330/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 331/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 332/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 333/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 334/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 335/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 336/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 337/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 338/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 339/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 340/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 341/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 342/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 343/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 344/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 345/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 346/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 347/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 348/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 349/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 350/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 351/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 352/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 353/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 354/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 355/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 356/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 357/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 358/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 359/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 360/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 361/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 362/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 363/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 364/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 365/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 366/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 367/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 368/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 369/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 370/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9576\n","Epoch 371/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 372/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 373/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 374/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 375/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 376/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9576\n","Epoch 377/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 378/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 379/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 380/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 381/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 382/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 383/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9576\n","Epoch 384/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9576\n","Epoch 385/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 386/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 387/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 388/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 389/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 390/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 391/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 392/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 393/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 394/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 395/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 396/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 397/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 398/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 399/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 400/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 401/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 402/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 403/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 404/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 405/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 406/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 407/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 408/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 409/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 410/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 411/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9577\n","Epoch 412/500\n","5317/5317 [==============================] - 1s 281us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 413/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 414/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 415/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 416/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 417/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 418/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 419/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 420/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 421/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 422/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 423/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 424/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9577\n","Epoch 425/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9577\n","Epoch 426/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 427/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 428/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 429/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 430/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9579\n","Epoch 431/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 432/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9579\n","Epoch 433/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 434/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 435/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 436/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 437/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 438/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 439/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9525 - val_loss: 4.9579\n","Epoch 440/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 441/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9525 - val_loss: 4.9579\n","Epoch 442/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9525 - val_loss: 4.9579\n","Epoch 443/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 444/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 445/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 446/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 447/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 448/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 449/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 450/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 451/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 452/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 453/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 454/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 455/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 456/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 457/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 458/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 459/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 460/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9525 - val_loss: 4.9579\n","Epoch 461/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 462/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 463/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 464/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 465/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 466/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 467/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 468/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 469/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 470/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 471/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 472/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 473/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 474/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 475/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 476/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 477/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 478/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9524 - val_loss: 4.9581\n","Epoch 479/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 480/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 481/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 482/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 483/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 484/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 485/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 486/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9524 - val_loss: 4.9581\n","Epoch 487/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 488/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 489/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 490/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 491/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 492/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 493/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 494/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 495/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 496/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 497/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 498/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 499/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9524 - val_loss: 4.9582\n","Epoch 500/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9581\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 33983 0.5\n","The shape of N (5908, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7459577322006226\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9912938402593575\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/500\n","5317/5317 [==============================] - 7s 1ms/step - loss: 5.0607 - val_loss: 5.1848\n","Epoch 2/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 5.0015 - val_loss: 5.0257\n","Epoch 3/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9909 - val_loss: 5.0032\n","Epoch 4/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9829 - val_loss: 4.9875\n","Epoch 5/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9781 - val_loss: 4.9808\n","Epoch 6/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9744 - val_loss: 4.9809\n","Epoch 7/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9711 - val_loss: 4.9746\n","Epoch 8/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9688 - val_loss: 4.9728\n","Epoch 9/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9675 - val_loss: 4.9785\n","Epoch 10/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9667 - val_loss: 4.9735\n","Epoch 11/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9678 - val_loss: 4.9794\n","Epoch 12/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9660 - val_loss: 4.9876\n","Epoch 13/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9643 - val_loss: 4.9716\n","Epoch 14/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9635 - val_loss: 4.9763\n","Epoch 15/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9632 - val_loss: 4.9758\n","Epoch 16/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9620 - val_loss: 4.9753\n","Epoch 17/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9613 - val_loss: 4.9740\n","Epoch 18/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9612 - val_loss: 4.9666\n","Epoch 19/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9609 - val_loss: 4.9660\n","Epoch 20/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9609 - val_loss: 4.9661\n","Epoch 21/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9604 - val_loss: 4.9654\n","Epoch 22/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9599 - val_loss: 4.9640\n","Epoch 23/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9620 - val_loss: 4.9737\n","Epoch 24/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9607 - val_loss: 4.9799\n","Epoch 25/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9598 - val_loss: 4.9708\n","Epoch 26/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9592 - val_loss: 4.9650\n","Epoch 27/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9587 - val_loss: 4.9662\n","Epoch 28/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9585 - val_loss: 4.9638\n","Epoch 29/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9587 - val_loss: 4.9629\n","Epoch 30/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9583 - val_loss: 4.9622\n","Epoch 31/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9584 - val_loss: 4.9619\n","Epoch 32/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9582 - val_loss: 4.9607\n","Epoch 33/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9579 - val_loss: 4.9608\n","Epoch 34/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9575 - val_loss: 4.9607\n","Epoch 35/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9605 - val_loss: 4.9900\n","Epoch 36/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9637 - val_loss: 5.0222\n","Epoch 37/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9599 - val_loss: 4.9953\n","Epoch 38/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9589 - val_loss: 4.9700\n","Epoch 39/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9597 - val_loss: 4.9832\n","Epoch 40/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9587 - val_loss: 4.9657\n","Epoch 41/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9580 - val_loss: 4.9620\n","Epoch 42/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9583 - val_loss: 4.9625\n","Epoch 43/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9582 - val_loss: 4.9617\n","Epoch 44/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9575 - val_loss: 4.9604\n","Epoch 45/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9573 - val_loss: 4.9598\n","Epoch 46/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9572 - val_loss: 4.9595\n","Epoch 47/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9570 - val_loss: 4.9596\n","Epoch 48/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9571 - val_loss: 4.9596\n","Epoch 49/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9597 - val_loss: 5.0245\n","Epoch 50/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9609 - val_loss: 4.9840\n","Epoch 51/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9602 - val_loss: 4.9718\n","Epoch 52/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9600 - val_loss: 4.9694\n","Epoch 53/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9597 - val_loss: 4.9629\n","Epoch 54/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9583 - val_loss: 4.9609\n","Epoch 55/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9578 - val_loss: 4.9606\n","Epoch 56/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9574 - val_loss: 4.9597\n","Epoch 57/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9572 - val_loss: 4.9593\n","Epoch 58/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9571 - val_loss: 4.9593\n","Epoch 59/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9572 - val_loss: 4.9596\n","Epoch 60/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9569 - val_loss: 4.9591\n","Epoch 61/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9569 - val_loss: 4.9590\n","Epoch 62/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9566 - val_loss: 4.9592\n","Epoch 63/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9566 - val_loss: 4.9587\n","Epoch 64/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9564 - val_loss: 4.9586\n","Epoch 65/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9564 - val_loss: 4.9593\n","Epoch 66/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9566 - val_loss: 4.9591\n","Epoch 67/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9562 - val_loss: 4.9585\n","Epoch 68/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9563 - val_loss: 4.9845\n","Epoch 69/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9565 - val_loss: 4.9702\n","Epoch 70/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9568 - val_loss: 4.9834\n","Epoch 71/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9580 - val_loss: 4.9662\n","Epoch 72/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9567 - val_loss: 4.9599\n","Epoch 73/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9565 - val_loss: 4.9595\n","Epoch 74/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9563 - val_loss: 4.9591\n","Epoch 75/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9561 - val_loss: 4.9585\n","Epoch 76/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9560 - val_loss: 4.9586\n","Epoch 77/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9559 - val_loss: 4.9587\n","Epoch 78/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9558 - val_loss: 4.9584\n","Epoch 79/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9558 - val_loss: 4.9583\n","Epoch 80/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9557 - val_loss: 4.9582\n","Epoch 81/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9558 - val_loss: 4.9585\n","Epoch 82/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9556 - val_loss: 4.9594\n","Epoch 83/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9556 - val_loss: 4.9586\n","Epoch 84/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9557 - val_loss: 4.9586\n","Epoch 85/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9557 - val_loss: 4.9590\n","Epoch 86/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9556 - val_loss: 4.9584\n","Epoch 87/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 88/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9554 - val_loss: 4.9581\n","Epoch 89/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 90/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9554 - val_loss: 4.9582\n","Epoch 91/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 92/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9553 - val_loss: 4.9583\n","Epoch 93/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9554 - val_loss: 4.9587\n","Epoch 94/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9554 - val_loss: 4.9582\n","Epoch 95/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9553 - val_loss: 4.9582\n","Epoch 96/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9553 - val_loss: 4.9583\n","Epoch 97/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9552 - val_loss: 4.9586\n","Epoch 98/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9555 - val_loss: 4.9584\n","Epoch 99/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 100/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 101/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 102/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9550 - val_loss: 4.9582\n","Epoch 103/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9551 - val_loss: 4.9587\n","Epoch 104/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9563 - val_loss: 4.9614\n","Epoch 105/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9556 - val_loss: 4.9594\n","Epoch 106/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9552 - val_loss: 4.9585\n","Epoch 107/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 108/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9552 - val_loss: 4.9583\n","Epoch 109/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 110/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 111/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9550 - val_loss: 4.9582\n","Epoch 112/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 113/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 114/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 115/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 116/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 117/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 118/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 119/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 120/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 121/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9549 - val_loss: 4.9583\n","Epoch 122/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 123/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 124/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 125/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 126/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9547 - val_loss: 4.9583\n","Epoch 127/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 128/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 129/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 130/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9547 - val_loss: 4.9590\n","Epoch 131/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 132/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9546 - val_loss: 4.9583\n","Epoch 133/500\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 134/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 135/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 136/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 137/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 138/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 139/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 140/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 141/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 142/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 143/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 144/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 145/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 146/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9546 - val_loss: 4.9589\n","Epoch 147/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9583\n","Epoch 148/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 149/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 150/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 151/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 152/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 153/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 154/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 155/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 156/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 157/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 158/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 159/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 160/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 161/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 162/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 163/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 164/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 165/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 166/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 167/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 168/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 169/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 170/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 171/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 172/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 173/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 174/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9542 - val_loss: 4.9583\n","Epoch 175/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 176/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 177/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 178/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 179/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 180/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 181/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 182/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 183/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 184/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 185/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 186/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 187/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 188/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 189/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 190/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9585\n","Epoch 191/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 192/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 193/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 194/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 195/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 196/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 197/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 198/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 199/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 200/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 201/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 202/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 203/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 204/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 205/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 206/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 207/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 208/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 209/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 210/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 211/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 212/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 213/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 214/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 215/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 216/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 217/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 218/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 219/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 220/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 221/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 222/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 223/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 224/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 225/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9586\n","Epoch 226/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 227/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 228/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 229/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 230/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 231/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9585\n","Epoch 232/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 233/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9585\n","Epoch 234/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 235/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 236/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 237/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 238/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 239/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 240/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 241/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 242/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 243/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 244/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 245/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 246/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 247/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 248/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 249/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 250/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 251/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 252/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 253/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 254/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 255/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 256/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 257/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 258/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 259/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 260/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 261/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 262/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 263/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 264/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 265/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 266/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 267/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 268/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 269/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 270/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 271/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 272/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 273/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 274/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9584\n","Epoch 275/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 276/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 277/500\n","5317/5317 [==============================] - 1s 282us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 278/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 279/500\n","5317/5317 [==============================] - 2s 282us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 280/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 281/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 282/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 283/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9583\n","Epoch 284/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 285/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 286/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 287/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 288/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 289/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 290/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 291/500\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 292/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 293/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 294/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9585\n","Epoch 295/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9583\n","Epoch 296/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 297/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 298/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9628\n","Epoch 299/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 300/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 301/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 302/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 303/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 304/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9584\n","Epoch 305/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 306/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9583\n","Epoch 307/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 308/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9538 - val_loss: 4.9585\n","Epoch 309/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 310/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 311/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 312/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 313/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 314/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 315/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 316/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 317/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 318/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 319/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9585\n","Epoch 320/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 321/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 322/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 323/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 324/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 325/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 326/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 327/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 328/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 329/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 330/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 331/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 332/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9554 - val_loss: 4.9677\n","Epoch 333/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9546 - val_loss: 4.9679\n","Epoch 334/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9542 - val_loss: 4.9622\n","Epoch 335/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9541 - val_loss: 4.9607\n","Epoch 336/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9539 - val_loss: 4.9592\n","Epoch 337/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9588\n","Epoch 338/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 339/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 340/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9584\n","Epoch 341/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 342/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 343/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 344/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9585\n","Epoch 345/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9584\n","Epoch 346/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 347/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 348/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 349/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9586\n","Epoch 350/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9587\n","Epoch 351/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9585\n","Epoch 352/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 353/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 354/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 355/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9587\n","Epoch 356/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 357/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 358/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 359/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 360/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 361/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 362/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 363/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 364/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9536 - val_loss: 4.9589\n","Epoch 365/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 366/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 367/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 368/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9585\n","Epoch 369/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 370/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 371/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9587\n","Epoch 372/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9586\n","Epoch 373/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9587\n","Epoch 374/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9585\n","Epoch 375/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9585\n","Epoch 376/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9586\n","Epoch 377/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9586\n","Epoch 378/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 379/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9586\n","Epoch 380/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9586\n","Epoch 381/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9586\n","Epoch 382/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9586\n","Epoch 383/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9650\n","Epoch 384/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9592\n","Epoch 385/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9587\n","Epoch 386/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 387/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 388/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 389/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 390/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 391/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 392/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 393/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 394/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 395/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 396/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 397/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 398/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 399/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 400/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 401/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 402/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9588\n","Epoch 403/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 404/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 405/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 406/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 407/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 408/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 409/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9598\n","Epoch 410/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9588\n","Epoch 411/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 412/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9586\n","Epoch 413/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9586\n","Epoch 414/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9586\n","Epoch 415/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9585\n","Epoch 416/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9531 - val_loss: 4.9586\n","Epoch 417/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9586\n","Epoch 418/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9586\n","Epoch 419/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9586\n","Epoch 420/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9586\n","Epoch 421/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9589\n","Epoch 422/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9591\n","Epoch 423/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9586\n","Epoch 424/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9586\n","Epoch 425/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9586\n","Epoch 426/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9587\n","Epoch 427/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9586\n","Epoch 428/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9585\n","Epoch 429/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9585\n","Epoch 430/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9585\n","Epoch 431/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9586\n","Epoch 432/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9585\n","Epoch 433/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9586\n","Epoch 434/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9587\n","Epoch 435/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9585\n","Epoch 436/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9586\n","Epoch 437/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 438/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9586\n","Epoch 439/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9586\n","Epoch 440/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 441/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 442/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 443/500\n","5317/5317 [==============================] - 2s 283us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 444/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9583\n","Epoch 445/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9530 - val_loss: 4.9583\n","Epoch 446/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9582\n","Epoch 447/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 448/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 449/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 450/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 451/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9642\n","Epoch 452/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9591\n","Epoch 453/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9587\n","Epoch 454/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9609\n","Epoch 455/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9535 - val_loss: 4.9644\n","Epoch 456/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9586\n","Epoch 457/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 458/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 459/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 460/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 461/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 462/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 463/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 464/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9583\n","Epoch 465/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 466/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 467/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9582\n","Epoch 468/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 469/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 470/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 471/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 472/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 473/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9584\n","Epoch 474/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 475/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9582\n","Epoch 476/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 477/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9585\n","Epoch 478/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9586\n","Epoch 479/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9587\n","Epoch 480/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9586\n","Epoch 481/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9585\n","Epoch 482/500\n","5317/5317 [==============================] - 2s 284us/step - loss: 4.9529 - val_loss: 4.9585\n","Epoch 483/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9586\n","Epoch 484/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9582\n","Epoch 485/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 486/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9583\n","Epoch 487/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9583\n","Epoch 488/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 489/500\n","5317/5317 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9582\n","Epoch 490/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9583\n","Epoch 491/500\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9582\n","Epoch 492/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 493/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9582\n","Epoch 494/500\n","5317/5317 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9583\n","Epoch 495/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 496/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 497/500\n","5317/5317 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9584\n","Epoch 498/500\n","5317/5317 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9584\n","Epoch 499/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9583\n","Epoch 500/500\n","5317/5317 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9584\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 46926 0.5\n","The shape of N (5908, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.749579906463623\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9725405246094901\n","=======================\n","Saved model to disk....\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.9788712054229296, 0.9887592101385204, 0.9865016209843795, 0.9914942528735632, 0.9843530798703213, 0.9816475095785441, 0.9878868258178604, 0.9873710580607132, 0.9912938402593575, 0.9725405246094901]\n","AUROC ===== 0.9850719127615679 +/- 0.005626158412934509\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7fafed2bed30>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"RG1WecQr-DUb","colab_type":"text"},"cell_type":"markdown","source":["# **RCAE-MNIST 6_Vs_all**"]},{"metadata":{"id":"An25LRa6-ONd","colab_type":"code","outputId":"1a808c89-2738-414b-c8b3-6c438a5c2a3c","executionInfo":{"status":"ok","timestamp":1541223659711,"user_tz":-660,"elapsed":47893,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":92946}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/250\n","5353/5353 [==============================] - 5s 857us/step - loss: 5.0708 - val_loss: 5.1378\n","Epoch 2/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9957 - val_loss: 5.0134\n","Epoch 3/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9822 - val_loss: 4.9870\n","Epoch 4/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9760 - val_loss: 4.9819\n","Epoch 5/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9716 - val_loss: 4.9798\n","Epoch 6/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9681 - val_loss: 4.9769\n","Epoch 7/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9658 - val_loss: 4.9729\n","Epoch 8/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9637 - val_loss: 4.9711\n","Epoch 9/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9625 - val_loss: 4.9703\n","Epoch 10/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9614 - val_loss: 4.9693\n","Epoch 11/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9611 - val_loss: 4.9690\n","Epoch 12/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9601 - val_loss: 4.9674\n","Epoch 13/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9596 - val_loss: 4.9666\n","Epoch 14/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9588 - val_loss: 4.9664\n","Epoch 15/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9586 - val_loss: 4.9657\n","Epoch 16/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9581 - val_loss: 4.9649\n","Epoch 17/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9581 - val_loss: 4.9641\n","Epoch 18/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9576 - val_loss: 4.9637\n","Epoch 19/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9574 - val_loss: 4.9650\n","Epoch 20/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9571 - val_loss: 4.9638\n","Epoch 21/250\n","5353/5353 [==============================] - 2s 285us/step - loss: 4.9569 - val_loss: 4.9639\n","Epoch 22/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9568 - val_loss: 4.9635\n","Epoch 23/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9568 - val_loss: 4.9631\n","Epoch 24/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9564 - val_loss: 4.9634\n","Epoch 25/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9565 - val_loss: 4.9621\n","Epoch 26/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9563 - val_loss: 4.9621\n","Epoch 27/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9562 - val_loss: 4.9657\n","Epoch 28/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9559 - val_loss: 4.9645\n","Epoch 29/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9558 - val_loss: 4.9637\n","Epoch 30/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9556 - val_loss: 4.9615\n","Epoch 31/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9555 - val_loss: 4.9616\n","Epoch 32/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9559 - val_loss: 4.9677\n","Epoch 33/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9556 - val_loss: 4.9617\n","Epoch 34/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9552 - val_loss: 4.9603\n","Epoch 35/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9564 - val_loss: 4.9705\n","Epoch 36/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9558 - val_loss: 4.9651\n","Epoch 37/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9553 - val_loss: 4.9641\n","Epoch 38/250\n","5353/5353 [==============================] - 2s 285us/step - loss: 4.9550 - val_loss: 4.9624\n","Epoch 39/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9549 - val_loss: 4.9610\n","Epoch 40/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9552 - val_loss: 4.9610\n","Epoch 41/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9549 - val_loss: 4.9613\n","Epoch 42/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9547 - val_loss: 4.9600\n","Epoch 43/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9545 - val_loss: 4.9602\n","Epoch 44/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9546 - val_loss: 4.9589\n","Epoch 45/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9567 - val_loss: 4.9746\n","Epoch 46/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9664\n","Epoch 47/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9563 - val_loss: 4.9916\n","Epoch 48/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9555 - val_loss: 4.9716\n","Epoch 49/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9552 - val_loss: 4.9644\n","Epoch 50/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9550 - val_loss: 4.9602\n","Epoch 51/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9549 - val_loss: 4.9616\n","Epoch 52/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9548 - val_loss: 4.9614\n","Epoch 53/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9597\n","Epoch 54/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9544 - val_loss: 4.9592\n","Epoch 55/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9544 - val_loss: 4.9590\n","Epoch 56/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9543 - val_loss: 4.9587\n","Epoch 57/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9586\n","Epoch 58/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9596\n","Epoch 59/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9587\n","Epoch 60/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9586\n","Epoch 61/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9587\n","Epoch 62/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9584\n","Epoch 63/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 64/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9589\n","Epoch 65/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9585\n","Epoch 66/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 67/250\n","5353/5353 [==============================] - 2s 285us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 68/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 69/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 70/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9585\n","Epoch 71/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 72/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 73/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 74/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 75/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 76/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 77/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 78/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 79/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 80/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9585\n","Epoch 81/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 82/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 83/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 84/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 85/250\n","5353/5353 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 86/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 87/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 88/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 89/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 90/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 91/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 92/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 93/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 94/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 95/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 96/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 97/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 98/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 99/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 100/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 101/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 102/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 103/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 104/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 105/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 106/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 107/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 108/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 109/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 110/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 111/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 112/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 113/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 114/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 115/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 116/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9576\n","Epoch 117/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 118/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 119/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 120/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 121/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 122/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 123/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 124/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 125/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 126/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 127/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 128/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 129/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 130/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 131/250\n","5353/5353 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 132/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 133/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 134/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 135/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 136/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9582\n","Epoch 137/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9576\n","Epoch 138/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 139/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 140/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 141/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 142/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 143/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 144/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 145/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 146/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 147/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 148/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9573\n","Epoch 149/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 150/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 151/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 152/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 153/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9577\n","Epoch 154/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 155/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 156/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9589\n","Epoch 157/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 158/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 159/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 160/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 161/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 162/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 163/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 164/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 165/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9573\n","Epoch 166/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9581\n","Epoch 167/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 168/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9573\n","Epoch 169/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 170/250\n","5353/5353 [==============================] - 2s 285us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 171/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 172/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 173/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 174/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 175/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 176/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9576\n","Epoch 177/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 178/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9588\n","Epoch 179/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 180/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 181/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 182/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 183/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9576\n","Epoch 184/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9576\n","Epoch 185/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9578\n","Epoch 186/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 187/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 188/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9580\n","Epoch 189/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 190/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9577\n","Epoch 191/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 192/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9574\n","Epoch 193/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 194/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 195/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9576\n","Epoch 196/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 197/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9522 - val_loss: 4.9574\n","Epoch 198/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9573\n","Epoch 199/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 200/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 201/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 202/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 203/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9577\n","Epoch 204/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9590\n","Epoch 205/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9521 - val_loss: 4.9577\n","Epoch 206/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9521 - val_loss: 4.9576\n","Epoch 207/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9574\n","Epoch 208/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9576\n","Epoch 209/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 210/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9576\n","Epoch 211/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9577\n","Epoch 212/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 213/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 214/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 215/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9576\n","Epoch 216/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9577\n","Epoch 217/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9521 - val_loss: 4.9575\n","Epoch 218/250\n","5353/5353 [==============================] - 2s 284us/step - loss: 4.9521 - val_loss: 4.9577\n","Epoch 219/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 220/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9521 - val_loss: 4.9577\n","Epoch 221/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 222/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9522 - val_loss: 4.9577\n","Epoch 223/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 224/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9521 - val_loss: 4.9577\n","Epoch 225/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9577\n","Epoch 226/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9576\n","Epoch 227/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9577\n","Epoch 228/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9521 - val_loss: 4.9576\n","Epoch 229/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9577\n","Epoch 230/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9521 - val_loss: 4.9576\n","Epoch 231/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9577\n","Epoch 232/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9576\n","Epoch 233/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9576\n","Epoch 234/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9521 - val_loss: 4.9577\n","Epoch 235/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9576\n","Epoch 236/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9577\n","Epoch 237/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9577\n","Epoch 238/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9519 - val_loss: 4.9576\n","Epoch 239/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9520 - val_loss: 4.9577\n","Epoch 240/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9521 - val_loss: 4.9576\n","Epoch 241/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9578\n","Epoch 242/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9577\n","Epoch 243/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9576\n","Epoch 244/250\n","5353/5353 [==============================] - 2s 285us/step - loss: 4.9521 - val_loss: 4.9579\n","Epoch 245/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9576\n","Epoch 246/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9578\n","Epoch 247/250\n","5353/5353 [==============================] - 2s 285us/step - loss: 4.9520 - val_loss: 4.9578\n","Epoch 248/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9577\n","Epoch 249/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9521 - val_loss: 4.9577\n","Epoch 250/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9520 - val_loss: 4.9578\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 40352 0.5\n","The shape of N (5948, 784)\n","The minimum value of N  -0.7499964237213135\n","The max value of N 0.749525785446167\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9995755517826825\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/250\n","5353/5353 [==============================] - 4s 718us/step - loss: 5.0657 - val_loss: 5.1575\n","Epoch 2/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9925 - val_loss: 5.0172\n","Epoch 3/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9802 - val_loss: 4.9940\n","Epoch 4/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9749 - val_loss: 4.9836\n","Epoch 5/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9707 - val_loss: 4.9781\n","Epoch 6/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9676 - val_loss: 4.9762\n","Epoch 7/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9657 - val_loss: 4.9727\n","Epoch 8/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9641 - val_loss: 4.9724\n","Epoch 9/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9626 - val_loss: 4.9690\n","Epoch 10/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9610 - val_loss: 4.9704\n","Epoch 11/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9602 - val_loss: 4.9686\n","Epoch 12/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9601 - val_loss: 4.9662\n","Epoch 13/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9595 - val_loss: 4.9659\n","Epoch 14/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9586 - val_loss: 4.9662\n","Epoch 15/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9583 - val_loss: 4.9652\n","Epoch 16/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9583 - val_loss: 4.9656\n","Epoch 17/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9576 - val_loss: 4.9637\n","Epoch 18/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9574 - val_loss: 4.9657\n","Epoch 19/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9570 - val_loss: 4.9642\n","Epoch 20/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9588 - val_loss: 4.9822\n","Epoch 21/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9580 - val_loss: 4.9679\n","Epoch 22/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9576 - val_loss: 4.9648\n","Epoch 23/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9566 - val_loss: 4.9627\n","Epoch 24/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9562 - val_loss: 4.9618\n","Epoch 25/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9561 - val_loss: 4.9612\n","Epoch 26/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9558 - val_loss: 4.9640\n","Epoch 27/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9566 - val_loss: 4.9688\n","Epoch 28/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9561 - val_loss: 4.9687\n","Epoch 29/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9558 - val_loss: 4.9635\n","Epoch 30/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9554 - val_loss: 4.9621\n","Epoch 31/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9552 - val_loss: 4.9610\n","Epoch 32/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9552 - val_loss: 4.9614\n","Epoch 33/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9551 - val_loss: 4.9610\n","Epoch 34/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9551 - val_loss: 4.9612\n","Epoch 35/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9547 - val_loss: 4.9601\n","Epoch 36/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9546 - val_loss: 4.9597\n","Epoch 37/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9545 - val_loss: 4.9599\n","Epoch 38/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9598\n","Epoch 39/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9596\n","Epoch 40/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9590\n","Epoch 41/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9589\n","Epoch 42/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9546 - val_loss: 4.9598\n","Epoch 43/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9603\n","Epoch 44/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9593\n","Epoch 45/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9597\n","Epoch 46/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 47/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9592\n","Epoch 48/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9588\n","Epoch 49/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9585\n","Epoch 50/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9597\n","Epoch 51/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9588\n","Epoch 52/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9541 - val_loss: 4.9649\n","Epoch 53/250\n","5353/5353 [==============================] - 2s 285us/step - loss: 4.9539 - val_loss: 4.9607\n","Epoch 54/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9588\n","Epoch 55/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9536 - val_loss: 4.9585\n","Epoch 56/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9591\n","Epoch 57/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9585\n","Epoch 58/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9597\n","Epoch 59/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 60/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9535 - val_loss: 4.9592\n","Epoch 61/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 62/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9642\n","Epoch 63/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9590\n","Epoch 64/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 65/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 66/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 67/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 68/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 69/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 70/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 71/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9959\n","Epoch 72/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9553 - val_loss: 4.9703\n","Epoch 73/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9620\n","Epoch 74/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9586\n","Epoch 75/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 76/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 77/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 78/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 79/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 80/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9598\n","Epoch 81/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 82/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9595\n","Epoch 83/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 84/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 85/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 86/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 87/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 88/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 89/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9595\n","Epoch 90/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 91/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 92/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 93/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 94/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 95/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 96/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 97/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 98/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 99/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9603\n","Epoch 100/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 101/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 102/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 103/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 104/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 105/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 106/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 107/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9567\n","Epoch 108/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 109/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9571\n","Epoch 110/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 111/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 112/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 113/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 114/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9570\n","Epoch 115/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9570\n","Epoch 116/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9569\n","Epoch 117/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9569\n","Epoch 118/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9569\n","Epoch 119/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9569\n","Epoch 120/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 121/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9567\n","Epoch 122/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9566\n","Epoch 123/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9568\n","Epoch 124/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9569\n","Epoch 125/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9569\n","Epoch 126/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9573\n","Epoch 127/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9569\n","Epoch 128/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9571\n","Epoch 129/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9566\n","Epoch 130/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 131/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9521 - val_loss: 4.9572\n","Epoch 132/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 133/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 134/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 135/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9566\n","Epoch 136/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9567\n","Epoch 137/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9567\n","Epoch 138/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9568\n","Epoch 139/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9521 - val_loss: 4.9569\n","Epoch 140/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9566\n","Epoch 141/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9519 - val_loss: 4.9567\n","Epoch 142/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9568\n","Epoch 143/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9569\n","Epoch 144/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 145/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9566\n","Epoch 146/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9565\n","Epoch 147/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9520 - val_loss: 4.9566\n","Epoch 148/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9566\n","Epoch 149/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9567\n","Epoch 150/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9570\n","Epoch 151/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9567\n","Epoch 152/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9564\n","Epoch 153/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9565\n","Epoch 154/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 5.0372\n","Epoch 155/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9550 - val_loss: 4.9824\n","Epoch 156/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9651\n","Epoch 157/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9624\n","Epoch 158/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9595\n","Epoch 159/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 160/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 161/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 162/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 163/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 164/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 165/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 166/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 167/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 168/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 169/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 170/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9584\n","Epoch 171/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 172/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 173/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 174/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 175/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 176/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9571\n","Epoch 177/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 178/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 179/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 180/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 181/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 182/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 183/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 184/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 185/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 186/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9521 - val_loss: 4.9561\n","Epoch 187/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 188/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 189/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 190/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 191/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9566\n","Epoch 192/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9565\n","Epoch 193/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9564\n","Epoch 194/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 195/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9565\n","Epoch 196/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9520 - val_loss: 4.9565\n","Epoch 197/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9518 - val_loss: 4.9565\n","Epoch 198/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9562\n","Epoch 199/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9563\n","Epoch 200/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9563\n","Epoch 201/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9565\n","Epoch 202/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9564\n","Epoch 203/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9562\n","Epoch 204/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9564\n","Epoch 205/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9519 - val_loss: 4.9565\n","Epoch 206/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9519 - val_loss: 4.9566\n","Epoch 207/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9520 - val_loss: 4.9564\n","Epoch 208/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9564\n","Epoch 209/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9565\n","Epoch 210/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9519 - val_loss: 4.9564\n","Epoch 211/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9564\n","Epoch 212/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9564\n","Epoch 213/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9565\n","Epoch 214/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9567\n","Epoch 215/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9564\n","Epoch 216/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9565\n","Epoch 217/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9563\n","Epoch 218/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9569\n","Epoch 219/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9566\n","Epoch 220/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9565\n","Epoch 221/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9518 - val_loss: 4.9564\n","Epoch 222/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9564\n","Epoch 223/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9564\n","Epoch 224/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9564\n","Epoch 225/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9517 - val_loss: 4.9564\n","Epoch 226/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9564\n","Epoch 227/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9519 - val_loss: 4.9566\n","Epoch 228/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9565\n","Epoch 229/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9518 - val_loss: 4.9566\n","Epoch 230/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9565\n","Epoch 231/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9563\n","Epoch 232/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9564\n","Epoch 233/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9518 - val_loss: 4.9564\n","Epoch 234/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9517 - val_loss: 4.9565\n","Epoch 235/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9517 - val_loss: 4.9565\n","Epoch 236/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9517 - val_loss: 4.9566\n","Epoch 237/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9565\n","Epoch 238/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9564\n","Epoch 239/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9564\n","Epoch 240/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9517 - val_loss: 4.9564\n","Epoch 241/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9517 - val_loss: 4.9563\n","Epoch 242/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9516 - val_loss: 4.9565\n","Epoch 243/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9564\n","Epoch 244/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9565\n","Epoch 245/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9566\n","Epoch 246/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9566\n","Epoch 247/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9516 - val_loss: 4.9564\n","Epoch 248/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9566\n","Epoch 249/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9564\n","Epoch 250/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9517 - val_loss: 4.9564\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 41898 0.5\n","The shape of N (5948, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7497287392616272\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9990808500673262\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/250\n","5353/5353 [==============================] - 4s 825us/step - loss: 5.0724 - val_loss: 5.1533\n","Epoch 2/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9980 - val_loss: 5.0197\n","Epoch 3/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9880 - val_loss: 4.9960\n","Epoch 4/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9830 - val_loss: 4.9887\n","Epoch 5/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9805 - val_loss: 4.9862\n","Epoch 6/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9784 - val_loss: 4.9832\n","Epoch 7/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9773 - val_loss: 4.9843\n","Epoch 8/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9759 - val_loss: 4.9803\n","Epoch 9/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9737 - val_loss: 4.9787\n","Epoch 10/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9709 - val_loss: 4.9789\n","Epoch 11/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9674 - val_loss: 4.9817\n","Epoch 12/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9648 - val_loss: 4.9772\n","Epoch 13/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9628 - val_loss: 4.9777\n","Epoch 14/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9613 - val_loss: 4.9754\n","Epoch 15/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9602 - val_loss: 4.9780\n","Epoch 16/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9595 - val_loss: 4.9670\n","Epoch 17/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9587 - val_loss: 4.9673\n","Epoch 18/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9582 - val_loss: 4.9684\n","Epoch 19/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9579 - val_loss: 4.9676\n","Epoch 20/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9577 - val_loss: 4.9689\n","Epoch 21/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9573 - val_loss: 4.9661\n","Epoch 22/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9569 - val_loss: 4.9621\n","Epoch 23/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9570 - val_loss: 4.9643\n","Epoch 24/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9565 - val_loss: 4.9622\n","Epoch 25/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9563 - val_loss: 4.9617\n","Epoch 26/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9559 - val_loss: 4.9618\n","Epoch 27/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9557 - val_loss: 4.9623\n","Epoch 28/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9556 - val_loss: 4.9607\n","Epoch 29/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9559 - val_loss: 4.9750\n","Epoch 30/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9562 - val_loss: 4.9654\n","Epoch 31/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9557 - val_loss: 4.9623\n","Epoch 32/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9559 - val_loss: 5.0081\n","Epoch 33/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9555 - val_loss: 4.9652\n","Epoch 34/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9555 - val_loss: 4.9644\n","Epoch 35/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9551 - val_loss: 4.9612\n","Epoch 36/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9564 - val_loss: 4.9697\n","Epoch 37/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9558 - val_loss: 4.9639\n","Epoch 38/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9554 - val_loss: 4.9597\n","Epoch 39/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9554 - val_loss: 4.9604\n","Epoch 40/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9551 - val_loss: 4.9601\n","Epoch 41/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9652\n","Epoch 42/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9554 - val_loss: 4.9606\n","Epoch 43/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9550 - val_loss: 4.9595\n","Epoch 44/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9549 - val_loss: 4.9586\n","Epoch 45/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9547 - val_loss: 4.9584\n","Epoch 46/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 47/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9583\n","Epoch 48/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9587\n","Epoch 49/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 50/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 51/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 52/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 53/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 54/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 55/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 56/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 57/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 58/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 59/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 60/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 61/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 62/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 63/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 64/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 65/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 66/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 67/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 68/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 69/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 70/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 71/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 72/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 73/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 74/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 75/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 76/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 77/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 78/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 79/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 80/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 81/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 82/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 83/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 84/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 85/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 86/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 87/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 88/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 89/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 90/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 91/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 92/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 93/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 94/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 95/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 96/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 97/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 98/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 99/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 100/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 101/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 102/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 103/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 104/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 105/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 106/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 107/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 108/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 109/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 110/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 111/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 112/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 113/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 114/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 115/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 116/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 117/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 118/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 119/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 120/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 121/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 122/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 123/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 124/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 125/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 126/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 127/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 128/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 129/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 130/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 131/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 132/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 133/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9571\n","Epoch 134/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 135/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 136/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 137/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 138/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 139/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 140/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 141/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 142/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 143/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 144/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 145/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 146/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 147/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 148/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 149/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 150/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 151/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 152/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9569\n","Epoch 153/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 154/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9569\n","Epoch 155/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 156/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9570\n","Epoch 157/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9567\n","Epoch 158/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 159/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 160/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9568\n","Epoch 161/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 162/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9571\n","Epoch 163/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9569\n","Epoch 164/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9570\n","Epoch 165/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 166/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9569\n","Epoch 167/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9571\n","Epoch 168/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9569\n","Epoch 169/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 170/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 171/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9569\n","Epoch 172/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9569\n","Epoch 173/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9568\n","Epoch 174/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9569\n","Epoch 175/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 176/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 177/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 178/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9569\n","Epoch 179/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 180/250\n","5353/5353 [==============================] - 2s 285us/step - loss: 4.9523 - val_loss: 4.9567\n","Epoch 181/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 182/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 183/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9568\n","Epoch 184/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9568\n","Epoch 185/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 186/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 187/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9571\n","Epoch 188/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 189/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9572\n","Epoch 190/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9567\n","Epoch 191/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9568\n","Epoch 192/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 193/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9569\n","Epoch 194/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9571\n","Epoch 195/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 196/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9571\n","Epoch 197/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 198/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9521 - val_loss: 4.9567\n","Epoch 199/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9568\n","Epoch 200/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9569\n","Epoch 201/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 202/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 203/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 204/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 205/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9569\n","Epoch 206/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 207/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 208/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 209/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 210/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9521 - val_loss: 4.9568\n","Epoch 211/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9566\n","Epoch 212/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9521 - val_loss: 4.9567\n","Epoch 213/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9567\n","Epoch 214/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9572\n","Epoch 215/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9568\n","Epoch 216/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9521 - val_loss: 4.9566\n","Epoch 217/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9567\n","Epoch 218/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9568\n","Epoch 219/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9567\n","Epoch 220/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9567\n","Epoch 221/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9569\n","Epoch 222/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9568\n","Epoch 223/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9521 - val_loss: 4.9570\n","Epoch 224/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9568\n","Epoch 225/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9569\n","Epoch 226/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9567\n","Epoch 227/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9568\n","Epoch 228/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9568\n","Epoch 229/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9568\n","Epoch 230/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9568\n","Epoch 231/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9568\n","Epoch 232/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9570\n","Epoch 233/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9569\n","Epoch 234/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9617\n","Epoch 235/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9568\n","Epoch 236/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9569\n","Epoch 237/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9569\n","Epoch 238/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9567\n","Epoch 239/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9569\n","Epoch 240/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9568\n","Epoch 241/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9520 - val_loss: 4.9568\n","Epoch 242/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9568\n","Epoch 243/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9567\n","Epoch 244/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9569\n","Epoch 245/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9568\n","Epoch 246/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9569\n","Epoch 247/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9519 - val_loss: 4.9569\n","Epoch 248/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9569\n","Epoch 249/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9519 - val_loss: 4.9568\n","Epoch 250/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9520 - val_loss: 4.9570\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 42981 0.5\n","The shape of N (5948, 784)\n","The minimum value of N  -0.7398636937141418\n","The max value of N 0.749998927116394\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9984105146068731\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/250\n","5353/5353 [==============================] - 5s 992us/step - loss: 5.0560 - val_loss: 5.1236\n","Epoch 2/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9930 - val_loss: 5.0119\n","Epoch 3/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9812 - val_loss: 4.9916\n","Epoch 4/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9750 - val_loss: 4.9853\n","Epoch 5/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9708 - val_loss: 4.9775\n","Epoch 6/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9676 - val_loss: 4.9727\n","Epoch 7/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9655 - val_loss: 4.9734\n","Epoch 8/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9639 - val_loss: 4.9699\n","Epoch 9/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9620 - val_loss: 4.9698\n","Epoch 10/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9612 - val_loss: 4.9692\n","Epoch 11/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9603 - val_loss: 4.9671\n","Epoch 12/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9597 - val_loss: 4.9664\n","Epoch 13/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9589 - val_loss: 4.9650\n","Epoch 14/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9593 - val_loss: 4.9903\n","Epoch 15/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9590 - val_loss: 4.9697\n","Epoch 16/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9580 - val_loss: 4.9678\n","Epoch 17/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9574 - val_loss: 4.9642\n","Epoch 18/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9569 - val_loss: 4.9644\n","Epoch 19/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9570 - val_loss: 4.9648\n","Epoch 20/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9568 - val_loss: 4.9640\n","Epoch 21/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9567 - val_loss: 4.9632\n","Epoch 22/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9574 - val_loss: 4.9858\n","Epoch 23/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9573 - val_loss: 4.9811\n","Epoch 24/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9568 - val_loss: 4.9673\n","Epoch 25/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9564 - val_loss: 4.9693\n","Epoch 26/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9563 - val_loss: 4.9657\n","Epoch 27/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9559 - val_loss: 4.9636\n","Epoch 28/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9559 - val_loss: 4.9852\n","Epoch 29/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9561 - val_loss: 4.9654\n","Epoch 30/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9554 - val_loss: 4.9620\n","Epoch 31/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9613\n","Epoch 32/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9552 - val_loss: 4.9611\n","Epoch 33/250\n","5353/5353 [==============================] - 2s 286us/step - loss: 4.9552 - val_loss: 4.9600\n","Epoch 34/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9549 - val_loss: 4.9598\n","Epoch 35/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9546 - val_loss: 4.9597\n","Epoch 36/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9549 - val_loss: 4.9619\n","Epoch 37/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9547 - val_loss: 4.9603\n","Epoch 38/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9544 - val_loss: 4.9590\n","Epoch 39/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9545 - val_loss: 4.9601\n","Epoch 40/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9588\n","Epoch 41/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9587\n","Epoch 42/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9542 - val_loss: 4.9584\n","Epoch 43/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 44/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 45/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 46/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 47/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 48/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 49/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 50/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9619\n","Epoch 51/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9551 - val_loss: 4.9591\n","Epoch 52/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9585\n","Epoch 53/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 54/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 55/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 56/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 57/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 58/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 59/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 60/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 61/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 62/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 63/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 64/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 65/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 66/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 67/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 68/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 69/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 70/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 71/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 72/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 73/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 74/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 75/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 76/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 77/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 78/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 79/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 80/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 81/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 82/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 83/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9590\n","Epoch 84/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 85/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 86/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 87/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 88/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 89/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 90/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 91/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 92/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 93/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 94/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 95/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 96/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 97/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9582\n","Epoch 98/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 99/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 100/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 101/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 102/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 103/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 104/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 105/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 106/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 107/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9586\n","Epoch 108/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 109/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 110/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9718\n","Epoch 111/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9606\n","Epoch 112/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9585\n","Epoch 113/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 114/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 115/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9618\n","Epoch 116/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9596\n","Epoch 117/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 118/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 119/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9597\n","Epoch 120/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 121/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 122/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 123/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 124/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 125/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 126/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 127/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 128/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 129/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 130/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 131/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9592\n","Epoch 132/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9596\n","Epoch 133/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 134/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 135/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 136/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 137/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 138/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 139/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 140/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9573\n","Epoch 141/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9569\n","Epoch 142/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9587\n","Epoch 143/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9568\n","Epoch 144/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 145/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9569\n","Epoch 146/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 147/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9566\n","Epoch 148/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9596\n","Epoch 149/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 150/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9571\n","Epoch 151/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9570\n","Epoch 152/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 153/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 154/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 155/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9571\n","Epoch 156/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9568\n","Epoch 157/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 158/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 159/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 160/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 161/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9578\n","Epoch 162/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 163/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 164/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 165/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 166/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 167/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9570\n","Epoch 168/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 169/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 170/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 171/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 172/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 173/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 174/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 175/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9562\n","Epoch 176/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 177/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 178/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 179/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 180/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 181/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 182/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 183/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 184/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 185/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 186/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 187/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9759\n","Epoch 188/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9607\n","Epoch 189/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 190/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9568\n","Epoch 191/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9567\n","Epoch 192/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 193/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 194/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 195/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 196/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 197/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 198/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 199/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9564\n","Epoch 200/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9572\n","Epoch 201/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9576\n","Epoch 202/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9567\n","Epoch 203/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 204/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9569\n","Epoch 205/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9573\n","Epoch 206/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9571\n","Epoch 207/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9561\n","Epoch 208/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9561\n","Epoch 209/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9563\n","Epoch 210/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 211/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9573\n","Epoch 212/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9562\n","Epoch 213/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9564\n","Epoch 214/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9562\n","Epoch 215/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9567\n","Epoch 216/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9563\n","Epoch 217/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9563\n","Epoch 218/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9561\n","Epoch 219/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9578\n","Epoch 220/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9563\n","Epoch 221/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9562\n","Epoch 222/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9518 - val_loss: 4.9562\n","Epoch 223/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 224/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9562\n","Epoch 225/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9565\n","Epoch 226/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9564\n","Epoch 227/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9519 - val_loss: 4.9562\n","Epoch 228/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9563\n","Epoch 229/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9518 - val_loss: 4.9562\n","Epoch 230/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9518 - val_loss: 4.9563\n","Epoch 231/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9519 - val_loss: 4.9561\n","Epoch 232/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9565\n","Epoch 233/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9562\n","Epoch 234/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9563\n","Epoch 235/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9567\n","Epoch 236/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9571\n","Epoch 237/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9519 - val_loss: 4.9564\n","Epoch 238/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9519 - val_loss: 4.9565\n","Epoch 239/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9518 - val_loss: 4.9564\n","Epoch 240/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9518 - val_loss: 4.9562\n","Epoch 241/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9567\n","Epoch 242/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9562\n","Epoch 243/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9567\n","Epoch 244/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9565\n","Epoch 245/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9562\n","Epoch 246/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9518 - val_loss: 4.9564\n","Epoch 247/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9564\n","Epoch 248/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9518 - val_loss: 4.9565\n","Epoch 249/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9563\n","Epoch 250/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9565\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 40676 0.5\n","The shape of N (5948, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.997962648556876\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/250\n","5353/5353 [==============================] - 6s 1ms/step - loss: 5.0661 - val_loss: 5.1627\n","Epoch 2/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9951 - val_loss: 5.0143\n","Epoch 3/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9838 - val_loss: 4.9918\n","Epoch 4/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9784 - val_loss: 4.9866\n","Epoch 5/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9747 - val_loss: 4.9852\n","Epoch 6/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9717 - val_loss: 4.9789\n","Epoch 7/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9695 - val_loss: 4.9770\n","Epoch 8/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9674 - val_loss: 4.9791\n","Epoch 9/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9658 - val_loss: 4.9774\n","Epoch 10/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9642 - val_loss: 4.9733\n","Epoch 11/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9629 - val_loss: 4.9748\n","Epoch 12/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9620 - val_loss: 4.9717\n","Epoch 13/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9610 - val_loss: 4.9711\n","Epoch 14/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9603 - val_loss: 4.9686\n","Epoch 15/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9599 - val_loss: 4.9693\n","Epoch 16/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9594 - val_loss: 4.9674\n","Epoch 17/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9590 - val_loss: 4.9658\n","Epoch 18/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9583 - val_loss: 4.9653\n","Epoch 19/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9579 - val_loss: 4.9653\n","Epoch 20/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9575 - val_loss: 4.9647\n","Epoch 21/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9572 - val_loss: 4.9666\n","Epoch 22/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9585 - val_loss: 4.9746\n","Epoch 23/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9574 - val_loss: 4.9675\n","Epoch 24/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9568 - val_loss: 4.9636\n","Epoch 25/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9574 - val_loss: 4.9696\n","Epoch 26/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9569 - val_loss: 4.9642\n","Epoch 27/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9565 - val_loss: 4.9622\n","Epoch 28/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9563 - val_loss: 4.9616\n","Epoch 29/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9563 - val_loss: 4.9620\n","Epoch 30/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9561 - val_loss: 4.9607\n","Epoch 31/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9562 - val_loss: 4.9606\n","Epoch 32/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9563 - val_loss: 4.9624\n","Epoch 33/250\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9559 - val_loss: 4.9601\n","Epoch 34/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9558 - val_loss: 4.9599\n","Epoch 35/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9579 - val_loss: 4.9637\n","Epoch 36/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9562 - val_loss: 4.9621\n","Epoch 37/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9559 - val_loss: 4.9608\n","Epoch 38/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9556 - val_loss: 4.9595\n","Epoch 39/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9553 - val_loss: 4.9593\n","Epoch 40/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9556 - val_loss: 4.9681\n","Epoch 41/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9564 - val_loss: 4.9717\n","Epoch 42/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9558 - val_loss: 4.9631\n","Epoch 43/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9555 - val_loss: 4.9597\n","Epoch 44/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9552 - val_loss: 4.9592\n","Epoch 45/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9565 - val_loss: 4.9762\n","Epoch 46/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9560 - val_loss: 4.9685\n","Epoch 47/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9556 - val_loss: 4.9639\n","Epoch 48/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9555 - val_loss: 4.9637\n","Epoch 49/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9613\n","Epoch 50/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9551 - val_loss: 4.9597\n","Epoch 51/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9548 - val_loss: 4.9594\n","Epoch 52/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9549 - val_loss: 4.9598\n","Epoch 53/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9594\n","Epoch 54/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9546 - val_loss: 4.9591\n","Epoch 55/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9548 - val_loss: 4.9592\n","Epoch 56/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9548 - val_loss: 4.9590\n","Epoch 57/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 58/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9584\n","Epoch 59/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9584\n","Epoch 60/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 61/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 62/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9584\n","Epoch 63/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9606\n","Epoch 64/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9584\n","Epoch 65/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9543 - val_loss: 4.9585\n","Epoch 66/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 67/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 68/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 69/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9587\n","Epoch 70/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 71/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 72/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 73/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 74/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 75/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9588\n","Epoch 76/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9544 - val_loss: 4.9599\n","Epoch 77/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9542 - val_loss: 4.9588\n","Epoch 78/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9586\n","Epoch 79/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 80/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 81/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 82/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 83/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 84/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9623\n","Epoch 85/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 86/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 87/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 88/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 89/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 90/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 91/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 92/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 93/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 94/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 95/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 96/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 97/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 98/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 99/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9585\n","Epoch 100/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 101/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 102/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 103/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 104/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9585\n","Epoch 105/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 106/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 107/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 108/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 109/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 110/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 111/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 112/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 113/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 114/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 115/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 116/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 117/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 118/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 119/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 120/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 121/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 122/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 123/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 124/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 125/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 126/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9611\n","Epoch 127/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9585\n","Epoch 128/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 129/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 130/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 131/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 132/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 133/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 134/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9590\n","Epoch 135/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 136/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 137/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 138/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 139/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9544 - val_loss: 4.9588\n","Epoch 140/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 141/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 142/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 143/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 144/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 145/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 146/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 147/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 148/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 149/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 150/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 151/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 152/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 153/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 154/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 155/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 156/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 157/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 158/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 159/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 160/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 161/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 162/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 163/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 164/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9789\n","Epoch 165/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9636\n","Epoch 166/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9544 - val_loss: 4.9599\n","Epoch 167/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 168/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 169/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 170/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 171/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 172/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 173/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 174/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 175/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9609\n","Epoch 176/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 177/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 178/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 179/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 180/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 181/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 182/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 183/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 184/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 185/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 186/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 187/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 188/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 189/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 190/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 191/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 192/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 193/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 194/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 195/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 196/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 197/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 198/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 199/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 200/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9597\n","Epoch 201/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9596\n","Epoch 202/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 203/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 204/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 205/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 206/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 207/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 208/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 209/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 210/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 211/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 212/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 213/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 214/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 215/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 216/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 217/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 218/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 219/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 220/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 221/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9576\n","Epoch 222/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 223/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 224/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 225/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 226/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 227/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 228/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 229/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 230/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9576\n","Epoch 231/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 232/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 233/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 234/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 235/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 236/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 237/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 238/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 239/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 240/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 241/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 242/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9577\n","Epoch 243/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 244/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 245/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 246/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 247/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9577\n","Epoch 248/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 249/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 250/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9582\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 59416 0.5\n","The shape of N (5948, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7495222687721252\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9959838416954511\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/250\n","5353/5353 [==============================] - 7s 1ms/step - loss: 5.0557 - val_loss: 5.1100\n","Epoch 2/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9930 - val_loss: 5.0142\n","Epoch 3/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9813 - val_loss: 4.9873\n","Epoch 4/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9748 - val_loss: 4.9797\n","Epoch 5/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9705 - val_loss: 4.9779\n","Epoch 6/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9676 - val_loss: 4.9744\n","Epoch 7/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9656 - val_loss: 4.9747\n","Epoch 8/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9640 - val_loss: 4.9725\n","Epoch 9/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9624 - val_loss: 4.9722\n","Epoch 10/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9616 - val_loss: 4.9691\n","Epoch 11/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9610 - val_loss: 4.9709\n","Epoch 12/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9603 - val_loss: 4.9673\n","Epoch 13/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9594 - val_loss: 4.9693\n","Epoch 14/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9595 - val_loss: 4.9674\n","Epoch 15/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9594 - val_loss: 4.9762\n","Epoch 16/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9587 - val_loss: 4.9660\n","Epoch 17/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9582 - val_loss: 4.9643\n","Epoch 18/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9579 - val_loss: 4.9635\n","Epoch 19/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9577 - val_loss: 4.9632\n","Epoch 20/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9577 - val_loss: 4.9649\n","Epoch 21/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9570 - val_loss: 4.9622\n","Epoch 22/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9574 - val_loss: 4.9717\n","Epoch 23/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9572 - val_loss: 4.9658\n","Epoch 24/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9571 - val_loss: 4.9635\n","Epoch 25/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9587 - val_loss: 5.0108\n","Epoch 26/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9580 - val_loss: 4.9772\n","Epoch 27/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9575 - val_loss: 4.9687\n","Epoch 28/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9569 - val_loss: 4.9625\n","Epoch 29/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9563 - val_loss: 4.9618\n","Epoch 30/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9567 - val_loss: 4.9611\n","Epoch 31/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9562 - val_loss: 4.9607\n","Epoch 32/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9569 - val_loss: 4.9701\n","Epoch 33/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9583 - val_loss: 4.9771\n","Epoch 34/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9565 - val_loss: 4.9649\n","Epoch 35/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9559 - val_loss: 4.9628\n","Epoch 36/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9559 - val_loss: 4.9608\n","Epoch 37/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9555 - val_loss: 4.9596\n","Epoch 38/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9553 - val_loss: 4.9598\n","Epoch 39/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9553 - val_loss: 4.9595\n","Epoch 40/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9598\n","Epoch 41/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9553 - val_loss: 4.9597\n","Epoch 42/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9551 - val_loss: 4.9591\n","Epoch 43/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9551 - val_loss: 4.9594\n","Epoch 44/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9591\n","Epoch 45/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9547 - val_loss: 4.9584\n","Epoch 46/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 47/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9584\n","Epoch 48/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9587\n","Epoch 49/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9546 - val_loss: 4.9588\n","Epoch 50/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 51/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9544 - val_loss: 4.9584\n","Epoch 52/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 53/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9708\n","Epoch 54/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9546 - val_loss: 4.9597\n","Epoch 55/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9626\n","Epoch 56/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9587\n","Epoch 57/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9589\n","Epoch 58/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 59/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9545 - val_loss: 4.9600\n","Epoch 60/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9548 - val_loss: 4.9585\n","Epoch 61/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 62/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 63/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 64/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 65/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 66/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 67/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 68/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 69/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 70/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 71/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 72/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 73/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 74/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 75/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 76/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9588\n","Epoch 77/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 78/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 79/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 80/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 81/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 82/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 83/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 84/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 85/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 86/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 87/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 88/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 89/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 90/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 91/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 92/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 93/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 94/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 95/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 96/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 97/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 98/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 99/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 100/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 101/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 102/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 103/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 104/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 105/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 106/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 107/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 108/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 109/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 110/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 111/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9638\n","Epoch 112/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9589\n","Epoch 113/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 114/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 115/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 116/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 117/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 118/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 119/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 120/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 121/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 122/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 123/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 124/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 125/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 126/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 127/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 128/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 129/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 130/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 131/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 132/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 133/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 134/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 135/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 136/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 137/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 138/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 139/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 140/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 141/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 142/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 143/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9574 - val_loss: 4.9956\n","Epoch 144/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9560 - val_loss: 4.9748\n","Epoch 145/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9637\n","Epoch 146/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9614\n","Epoch 147/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9605\n","Epoch 148/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9585\n","Epoch 149/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 150/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 151/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 152/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 153/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 154/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9619\n","Epoch 155/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9607\n","Epoch 156/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 157/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 158/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 159/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 160/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 161/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 162/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 163/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 164/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 165/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 166/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 167/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 168/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 169/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 170/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 171/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 172/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 173/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 174/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9595\n","Epoch 175/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 176/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 177/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 178/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 179/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 180/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 181/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 182/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 183/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 184/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 185/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 186/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 187/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 188/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 189/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 190/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 191/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9836\n","Epoch 192/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 193/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 194/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 195/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 196/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9842\n","Epoch 197/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9693\n","Epoch 198/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9627\n","Epoch 199/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9590\n","Epoch 200/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 201/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 202/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 203/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 204/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 205/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 206/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 207/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 208/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 209/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 210/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 211/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 212/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 213/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 214/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9576\n","Epoch 215/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 216/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9576\n","Epoch 217/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 218/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 219/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 220/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 221/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 222/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9577\n","Epoch 223/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 224/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9576\n","Epoch 225/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 226/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 227/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9576\n","Epoch 228/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 229/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9577\n","Epoch 230/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 231/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 232/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 233/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 234/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9576\n","Epoch 235/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 236/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9577\n","Epoch 237/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 238/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 239/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9577\n","Epoch 240/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 241/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 242/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 243/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 244/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9577\n","Epoch 245/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9576\n","Epoch 246/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9577\n","Epoch 247/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9576\n","Epoch 248/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9577\n","Epoch 249/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9577\n","Epoch 250/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9578\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 50083 0.5\n","The shape of N (5948, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7497126460075378\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9987032375153679\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/250\n","5353/5353 [==============================] - 7s 1ms/step - loss: 5.0604 - val_loss: 5.1537\n","Epoch 2/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9929 - val_loss: 5.0080\n","Epoch 3/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9821 - val_loss: 4.9914\n","Epoch 4/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9759 - val_loss: 4.9829\n","Epoch 5/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9715 - val_loss: 4.9768\n","Epoch 6/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9683 - val_loss: 4.9782\n","Epoch 7/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9656 - val_loss: 4.9740\n","Epoch 8/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9642 - val_loss: 4.9711\n","Epoch 9/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9632 - val_loss: 4.9704\n","Epoch 10/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9617 - val_loss: 4.9666\n","Epoch 11/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9609 - val_loss: 4.9677\n","Epoch 12/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9599 - val_loss: 4.9652\n","Epoch 13/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9596 - val_loss: 4.9652\n","Epoch 14/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9592 - val_loss: 4.9645\n","Epoch 15/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9589 - val_loss: 4.9650\n","Epoch 16/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9582 - val_loss: 4.9634\n","Epoch 17/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9579 - val_loss: 4.9627\n","Epoch 18/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9575 - val_loss: 4.9633\n","Epoch 19/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9572 - val_loss: 4.9663\n","Epoch 20/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9569 - val_loss: 4.9614\n","Epoch 21/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9567 - val_loss: 4.9619\n","Epoch 22/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9566 - val_loss: 4.9613\n","Epoch 23/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9570 - val_loss: 4.9784\n","Epoch 24/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9568 - val_loss: 4.9658\n","Epoch 25/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9563 - val_loss: 4.9642\n","Epoch 26/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9561 - val_loss: 4.9621\n","Epoch 27/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9558 - val_loss: 4.9622\n","Epoch 28/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9556 - val_loss: 4.9620\n","Epoch 29/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9558 - val_loss: 4.9605\n","Epoch 30/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9553 - val_loss: 4.9599\n","Epoch 31/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9554 - val_loss: 4.9598\n","Epoch 32/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9555 - val_loss: 4.9592\n","Epoch 33/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9556 - val_loss: 4.9598\n","Epoch 34/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9551 - val_loss: 4.9594\n","Epoch 35/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9552 - val_loss: 4.9595\n","Epoch 36/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9550 - val_loss: 4.9590\n","Epoch 37/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9551 - val_loss: 4.9590\n","Epoch 38/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9550 - val_loss: 4.9603\n","Epoch 39/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9550 - val_loss: 4.9585\n","Epoch 40/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9547 - val_loss: 4.9590\n","Epoch 41/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9583\n","Epoch 42/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9547 - val_loss: 4.9586\n","Epoch 43/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9582\n","Epoch 44/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 45/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9585\n","Epoch 46/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9584\n","Epoch 47/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 48/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 49/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9774\n","Epoch 50/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9608\n","Epoch 51/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9587\n","Epoch 52/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9594\n","Epoch 53/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 54/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 55/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 56/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 57/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 58/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 59/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 60/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 61/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 62/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 63/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 64/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 65/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 66/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 67/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 68/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 69/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 70/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9844\n","Epoch 71/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9575 - val_loss: 4.9774\n","Epoch 72/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9560 - val_loss: 4.9662\n","Epoch 73/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9551 - val_loss: 4.9614\n","Epoch 74/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9598\n","Epoch 75/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9586\n","Epoch 76/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 77/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 78/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 79/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9585\n","Epoch 80/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 81/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 82/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9603\n","Epoch 83/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 84/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 85/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 86/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 87/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 88/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 89/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 90/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 91/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 92/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 93/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 94/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 95/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 96/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 97/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 98/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 99/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 100/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 101/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 102/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 103/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 104/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 105/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 106/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 107/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 108/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 109/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 110/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 111/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 112/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 113/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 114/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 115/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 116/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 117/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 118/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 119/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 120/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 121/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 122/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 123/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 124/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 125/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 126/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 127/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 128/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 129/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 130/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 131/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 132/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 133/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 134/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 135/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 136/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 137/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 138/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 139/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 140/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 141/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 142/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 143/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 144/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 145/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 146/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 147/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 148/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9570\n","Epoch 149/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 150/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 151/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 152/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 153/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 154/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 155/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 156/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 157/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9571\n","Epoch 158/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 159/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 160/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 161/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 162/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 163/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 164/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 165/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 166/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 167/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 168/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9570\n","Epoch 169/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9572\n","Epoch 170/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9571\n","Epoch 171/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9573\n","Epoch 172/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9571\n","Epoch 173/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9572\n","Epoch 174/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 175/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9573\n","Epoch 176/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9571\n","Epoch 177/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9572\n","Epoch 178/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9572\n","Epoch 179/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 180/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 181/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9572\n","Epoch 182/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9573\n","Epoch 183/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 184/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 185/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 186/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9573\n","Epoch 187/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9570\n","Epoch 188/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9574\n","Epoch 189/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9573\n","Epoch 190/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9571\n","Epoch 191/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9573\n","Epoch 192/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 193/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9572\n","Epoch 194/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 195/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9571\n","Epoch 196/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9572\n","Epoch 197/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 198/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 199/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9572\n","Epoch 200/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9574\n","Epoch 201/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9573\n","Epoch 202/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9572\n","Epoch 203/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9572\n","Epoch 204/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9571\n","Epoch 205/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9572\n","Epoch 206/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9572\n","Epoch 207/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9572\n","Epoch 208/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 209/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9571\n","Epoch 210/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9571\n","Epoch 211/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 212/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9572\n","Epoch 213/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9572\n","Epoch 214/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9572\n","Epoch 215/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9573\n","Epoch 216/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9572\n","Epoch 217/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9573\n","Epoch 218/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9574\n","Epoch 219/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9571\n","Epoch 220/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9572\n","Epoch 221/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9572\n","Epoch 222/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9572\n","Epoch 223/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9575\n","Epoch 224/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9572\n","Epoch 225/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9573\n","Epoch 226/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9572\n","Epoch 227/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9572\n","Epoch 228/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9521 - val_loss: 4.9574\n","Epoch 229/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9573\n","Epoch 230/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9573\n","Epoch 231/250\n","5353/5353 [==============================] - 2s 285us/step - loss: 4.9521 - val_loss: 4.9571\n","Epoch 232/250\n","5353/5353 [==============================] - 2s 364us/step - loss: 4.9521 - val_loss: 4.9571\n","Epoch 233/250\n","5353/5353 [==============================] - 3s 612us/step - loss: 4.9521 - val_loss: 4.9573\n","Epoch 234/250\n","5353/5353 [==============================] - 3s 616us/step - loss: 4.9521 - val_loss: 4.9573\n","Epoch 235/250\n","5353/5353 [==============================] - 3s 553us/step - loss: 4.9520 - val_loss: 4.9573\n","Epoch 236/250\n","5353/5353 [==============================] - 3s 473us/step - loss: 4.9521 - val_loss: 4.9571\n","Epoch 237/250\n","5353/5353 [==============================] - 2s 457us/step - loss: 4.9521 - val_loss: 4.9573\n","Epoch 238/250\n","5353/5353 [==============================] - 2s 393us/step - loss: 4.9521 - val_loss: 4.9573\n","Epoch 239/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9573\n","Epoch 240/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9521 - val_loss: 4.9571\n","Epoch 241/250\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9521 - val_loss: 4.9572\n","Epoch 242/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9572\n","Epoch 243/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9520 - val_loss: 4.9572\n","Epoch 244/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9572\n","Epoch 245/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9576\n","Epoch 246/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9573\n","Epoch 247/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9573\n","Epoch 248/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9571\n","Epoch 249/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9520 - val_loss: 4.9573\n","Epoch 250/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9521 - val_loss: 4.9575\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 48004 0.5\n","The shape of N (5948, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7458815574645996\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9977635969790996\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/250\n","5353/5353 [==============================] - 8s 1ms/step - loss: 5.0614 - val_loss: 5.1293\n","Epoch 2/250\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9918 - val_loss: 5.0065\n","Epoch 3/250\n","5353/5353 [==============================] - 2s 311us/step - loss: 4.9804 - val_loss: 4.9867\n","Epoch 4/250\n","5353/5353 [==============================] - 2s 311us/step - loss: 4.9743 - val_loss: 4.9808\n","Epoch 5/250\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9703 - val_loss: 4.9777\n","Epoch 6/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9671 - val_loss: 4.9807\n","Epoch 7/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9655 - val_loss: 4.9755\n","Epoch 8/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9635 - val_loss: 4.9735\n","Epoch 9/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9625 - val_loss: 4.9750\n","Epoch 10/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9612 - val_loss: 4.9727\n","Epoch 11/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9604 - val_loss: 4.9735\n","Epoch 12/250\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9596 - val_loss: 4.9696\n","Epoch 13/250\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9596 - val_loss: 4.9692\n","Epoch 14/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9588 - val_loss: 4.9732\n","Epoch 15/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9588 - val_loss: 4.9683\n","Epoch 16/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9598 - val_loss: 5.0112\n","Epoch 17/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9594 - val_loss: 4.9721\n","Epoch 18/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9580 - val_loss: 4.9660\n","Epoch 19/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9582 - val_loss: 4.9792\n","Epoch 20/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9581 - val_loss: 4.9657\n","Epoch 21/250\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9573 - val_loss: 4.9644\n","Epoch 22/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9569 - val_loss: 4.9643\n","Epoch 23/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9570 - val_loss: 4.9624\n","Epoch 24/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9566 - val_loss: 4.9623\n","Epoch 25/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9566 - val_loss: 4.9630\n","Epoch 26/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9562 - val_loss: 4.9619\n","Epoch 27/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9567 - val_loss: 4.9836\n","Epoch 28/250\n","5353/5353 [==============================] - 2s 311us/step - loss: 4.9567 - val_loss: 4.9668\n","Epoch 29/250\n","5353/5353 [==============================] - 2s 310us/step - loss: 4.9561 - val_loss: 4.9626\n","Epoch 30/250\n","5353/5353 [==============================] - 2s 311us/step - loss: 4.9559 - val_loss: 4.9613\n","Epoch 31/250\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9559 - val_loss: 4.9604\n","Epoch 32/250\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9565 - val_loss: 4.9796\n","Epoch 33/250\n","5353/5353 [==============================] - 2s 310us/step - loss: 4.9572 - val_loss: 4.9696\n","Epoch 34/250\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9559 - val_loss: 4.9614\n","Epoch 35/250\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9557 - val_loss: 4.9600\n","Epoch 36/250\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9554 - val_loss: 4.9604\n","Epoch 37/250\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9552 - val_loss: 4.9597\n","Epoch 38/250\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9552 - val_loss: 4.9595\n","Epoch 39/250\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9557 - val_loss: 4.9643\n","Epoch 40/250\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9554 - val_loss: 4.9617\n","Epoch 41/250\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9550 - val_loss: 4.9600\n","Epoch 42/250\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9550 - val_loss: 4.9616\n","Epoch 43/250\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9549 - val_loss: 4.9597\n","Epoch 44/250\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9548 - val_loss: 4.9600\n","Epoch 45/250\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9550 - val_loss: 4.9606\n","Epoch 46/250\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9551 - val_loss: 4.9591\n","Epoch 47/250\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9545 - val_loss: 4.9588\n","Epoch 48/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9589\n","Epoch 49/250\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 50/250\n","5353/5353 [==============================] - 2s 310us/step - loss: 4.9544 - val_loss: 4.9584\n","Epoch 51/250\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9548 - val_loss: 4.9875\n","Epoch 52/250\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9548 - val_loss: 4.9623\n","Epoch 53/250\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9545 - val_loss: 4.9594\n","Epoch 54/250\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9544 - val_loss: 4.9586\n","Epoch 55/250\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9544 - val_loss: 4.9591\n","Epoch 56/250\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 57/250\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 58/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 59/250\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 60/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9586\n","Epoch 61/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 62/250\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 63/250\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 64/250\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 65/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 66/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9603\n","Epoch 67/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9588\n","Epoch 68/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 69/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 70/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 71/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 72/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 73/250\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9585\n","Epoch 74/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 75/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 76/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 77/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9599\n","Epoch 78/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 79/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 80/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9588\n","Epoch 81/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 82/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 83/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 84/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 85/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9547 - val_loss: 5.0217\n","Epoch 86/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9767\n","Epoch 87/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9632\n","Epoch 88/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9577 - val_loss: 4.9985\n","Epoch 89/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9566 - val_loss: 4.9776\n","Epoch 90/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9550 - val_loss: 4.9629\n","Epoch 91/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9597\n","Epoch 92/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9586\n","Epoch 93/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 94/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 95/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 96/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 97/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 98/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 99/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 100/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9624\n","Epoch 101/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 102/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 103/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 104/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 105/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 106/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 107/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 108/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9594\n","Epoch 109/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 110/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 111/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 112/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 113/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 114/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 115/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 116/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 117/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 118/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 119/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 120/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9595\n","Epoch 121/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 122/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 123/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 124/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 125/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 126/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 127/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 128/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 129/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 130/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 131/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9588\n","Epoch 132/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9719\n","Epoch 133/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 134/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9583\n","Epoch 135/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 136/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 137/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 138/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 139/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 140/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 141/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 142/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 143/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 144/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 145/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 146/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 147/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 148/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 149/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 150/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 151/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 152/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 153/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 154/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 155/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 156/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 157/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 158/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 159/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 160/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 161/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 162/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 163/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 164/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 165/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 166/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 167/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 168/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 169/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 170/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 171/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 172/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 173/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 5.0123\n","Epoch 174/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9778\n","Epoch 175/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9661\n","Epoch 176/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9625\n","Epoch 177/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9597\n","Epoch 178/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 179/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 180/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 181/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 182/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 183/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 184/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 185/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 5.0034\n","Epoch 186/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9695\n","Epoch 187/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9606\n","Epoch 188/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9594\n","Epoch 189/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9588\n","Epoch 190/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9596\n","Epoch 191/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9595\n","Epoch 192/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9594\n","Epoch 193/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 194/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 195/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 196/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 197/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 198/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 199/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 200/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 201/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 202/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 203/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 204/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9571\n","Epoch 205/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 206/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9593\n","Epoch 207/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9587\n","Epoch 208/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9576\n","Epoch 209/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9579\n","Epoch 210/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9578\n","Epoch 211/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 212/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 213/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 214/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 215/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 216/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 217/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 218/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 219/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 220/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 221/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 222/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9577\n","Epoch 223/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 224/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 225/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9577\n","Epoch 226/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9578\n","Epoch 227/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 228/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9576\n","Epoch 229/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9578\n","Epoch 230/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9577\n","Epoch 231/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9579\n","Epoch 232/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9578\n","Epoch 233/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9579\n","Epoch 234/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9581\n","Epoch 235/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9582\n","Epoch 236/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 237/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9585\n","Epoch 238/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9577\n","Epoch 239/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9580\n","Epoch 240/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9577\n","Epoch 241/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9580\n","Epoch 242/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9578\n","Epoch 243/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9582\n","Epoch 244/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9522 - val_loss: 4.9576\n","Epoch 245/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9578\n","Epoch 246/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9521 - val_loss: 4.9580\n","Epoch 247/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9591\n","Epoch 248/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9591\n","Epoch 249/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9582\n","Epoch 250/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9580\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 41314 0.5\n","The shape of N (5948, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9979714302441309\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/250\n","5353/5353 [==============================] - 8s 2ms/step - loss: 5.0604 - val_loss: 5.2051\n","Epoch 2/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9944 - val_loss: 5.0238\n","Epoch 3/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9828 - val_loss: 4.9940\n","Epoch 4/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9769 - val_loss: 4.9828\n","Epoch 5/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9726 - val_loss: 4.9790\n","Epoch 6/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9695 - val_loss: 4.9765\n","Epoch 7/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9670 - val_loss: 4.9757\n","Epoch 8/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9651 - val_loss: 4.9724\n","Epoch 9/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9638 - val_loss: 4.9700\n","Epoch 10/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9628 - val_loss: 4.9692\n","Epoch 11/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9619 - val_loss: 4.9687\n","Epoch 12/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9614 - val_loss: 4.9668\n","Epoch 13/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9610 - val_loss: 4.9681\n","Epoch 14/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9604 - val_loss: 4.9667\n","Epoch 15/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9601 - val_loss: 4.9742\n","Epoch 16/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9597 - val_loss: 4.9676\n","Epoch 17/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9592 - val_loss: 4.9706\n","Epoch 18/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9595 - val_loss: 4.9841\n","Epoch 19/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9592 - val_loss: 4.9695\n","Epoch 20/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9588 - val_loss: 4.9681\n","Epoch 21/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9584 - val_loss: 4.9658\n","Epoch 22/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9582 - val_loss: 4.9637\n","Epoch 23/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9577 - val_loss: 4.9653\n","Epoch 24/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9575 - val_loss: 4.9638\n","Epoch 25/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9576 - val_loss: 4.9643\n","Epoch 26/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9574 - val_loss: 4.9631\n","Epoch 27/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9577 - val_loss: 4.9661\n","Epoch 28/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9572 - val_loss: 4.9630\n","Epoch 29/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9569 - val_loss: 4.9624\n","Epoch 30/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9572 - val_loss: 4.9717\n","Epoch 31/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9568 - val_loss: 4.9639\n","Epoch 32/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9566 - val_loss: 4.9613\n","Epoch 33/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9565 - val_loss: 4.9605\n","Epoch 34/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9567 - val_loss: 4.9710\n","Epoch 35/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9563 - val_loss: 4.9625\n","Epoch 36/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9560 - val_loss: 4.9608\n","Epoch 37/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9560 - val_loss: 4.9601\n","Epoch 38/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9561 - val_loss: 4.9623\n","Epoch 39/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9557 - val_loss: 4.9606\n","Epoch 40/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9558 - val_loss: 4.9603\n","Epoch 41/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9557 - val_loss: 4.9610\n","Epoch 42/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9558 - val_loss: 4.9607\n","Epoch 43/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9559 - val_loss: 4.9602\n","Epoch 44/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9564 - val_loss: 4.9657\n","Epoch 45/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9561 - val_loss: 4.9618\n","Epoch 46/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9556 - val_loss: 4.9605\n","Epoch 47/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9553 - val_loss: 4.9599\n","Epoch 48/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9553 - val_loss: 4.9596\n","Epoch 49/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9552 - val_loss: 4.9594\n","Epoch 50/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9594\n","Epoch 51/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9550 - val_loss: 4.9591\n","Epoch 52/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9551 - val_loss: 4.9586\n","Epoch 53/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9561 - val_loss: 4.9616\n","Epoch 54/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9561 - val_loss: 4.9649\n","Epoch 55/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9568 - val_loss: 4.9657\n","Epoch 56/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9556 - val_loss: 4.9601\n","Epoch 57/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9552 - val_loss: 4.9611\n","Epoch 58/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9550 - val_loss: 4.9601\n","Epoch 59/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9548 - val_loss: 4.9587\n","Epoch 60/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9547 - val_loss: 4.9587\n","Epoch 61/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9558 - val_loss: 4.9681\n","Epoch 62/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9553 - val_loss: 4.9608\n","Epoch 63/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9552 - val_loss: 4.9605\n","Epoch 64/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9550 - val_loss: 4.9587\n","Epoch 65/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9548 - val_loss: 4.9586\n","Epoch 66/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9551 - val_loss: 4.9600\n","Epoch 67/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9547 - val_loss: 4.9584\n","Epoch 68/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9548 - val_loss: 4.9630\n","Epoch 69/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9588\n","Epoch 70/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9547 - val_loss: 4.9583\n","Epoch 71/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 72/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 73/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 74/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9547 - val_loss: 4.9588\n","Epoch 75/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9547 - val_loss: 4.9582\n","Epoch 76/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 77/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 78/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 79/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 80/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9586\n","Epoch 81/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 82/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9544 - val_loss: 4.9584\n","Epoch 83/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 84/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 85/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 86/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 87/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 88/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 89/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 90/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 91/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 92/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 93/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 94/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 95/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 96/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9790\n","Epoch 97/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 98/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 99/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 100/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 101/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9595\n","Epoch 102/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9586\n","Epoch 103/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 104/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 105/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 106/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 107/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9588\n","Epoch 108/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 109/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 110/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 111/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 112/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 113/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 114/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 115/250\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 116/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9697\n","Epoch 117/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9563 - val_loss: 4.9649\n","Epoch 118/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9597\n","Epoch 119/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9584\n","Epoch 120/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9584\n","Epoch 121/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 122/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 123/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 124/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9585\n","Epoch 125/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 126/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 127/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 128/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 129/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 130/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 131/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 132/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 133/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 134/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 135/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 136/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 137/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 138/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 139/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 140/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 141/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 142/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 143/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 144/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 145/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 146/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 147/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9611\n","Epoch 148/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9586\n","Epoch 149/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 150/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 151/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 152/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 153/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 154/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 155/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 156/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 157/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 158/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 159/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 160/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 161/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 162/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 163/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 164/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 165/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 166/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 167/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 168/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 169/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9627\n","Epoch 170/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 171/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 172/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 173/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 174/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 175/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 176/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 177/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 178/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 179/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 180/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 181/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9606\n","Epoch 182/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9585\n","Epoch 183/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 184/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 185/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 186/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 187/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 188/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9592\n","Epoch 189/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 190/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 191/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 192/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 193/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 194/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 195/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 196/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 197/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 198/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 199/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 200/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 201/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 202/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 203/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 204/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 205/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 206/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9774\n","Epoch 207/250\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9590\n","Epoch 208/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 209/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 210/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 211/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 212/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 213/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 214/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 215/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 216/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 217/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 218/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 219/250\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 220/250\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 221/250\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 222/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 223/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 224/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 225/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 226/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 227/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 228/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 229/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 230/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 231/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 232/250\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 233/250\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 234/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 235/250\n","5353/5353 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 236/250\n","5353/5353 [==============================] - 2s 287us/step - loss: 4.9539 - val_loss: 4.9759\n","Epoch 237/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9651\n","Epoch 238/250\n","5353/5353 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9597\n","Epoch 239/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 240/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 241/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 242/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 243/250\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 244/250\n","5353/5353 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 245/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 246/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 247/250\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9587\n","Epoch 248/250\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 249/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 250/250\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9575\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 49052 0.5\n","The shape of N (5948, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7494409680366516\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9976933434810609\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/250\n","5353/5353 [==============================] - 9s 2ms/step - loss: 5.0702 - val_loss: 5.1548\n","Epoch 2/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9953 - val_loss: 5.0112\n","Epoch 3/250\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9819 - val_loss: 4.9888\n","Epoch 4/250\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9753 - val_loss: 4.9816\n","Epoch 5/250\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9709 - val_loss: 4.9756\n","Epoch 6/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9680 - val_loss: 4.9737\n","Epoch 7/250\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9659 - val_loss: 4.9714\n","Epoch 8/250\n","1600/5353 [=======>......................] - ETA: 1s - loss: 4.9644Buffered data was truncated after reaching the output size limit."],"name":"stdout"}]},{"metadata":{"id":"P6zv6Ya8-Gyy","colab_type":"text"},"cell_type":"markdown","source":["# **RCAE-MNIST 5_Vs_all**"]},{"metadata":{"id":"tfqRYOa695b9","colab_type":"code","outputId":"a77716a2-cddd-400e-956b-00bdc8dcfc8b","executionInfo":{"status":"ok","timestamp":1541246491147,"user_tz":-660,"elapsed":3843665,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":99917}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/250\n","4917/4917 [==============================] - 6s 1ms/step - loss: 5.0701 - val_loss: 5.1637\n","Epoch 2/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 5.0032 - val_loss: 5.0226\n","Epoch 3/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9880 - val_loss: 4.9997\n","Epoch 4/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9796 - val_loss: 4.9859\n","Epoch 5/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9744 - val_loss: 4.9793\n","Epoch 6/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9708 - val_loss: 4.9766\n","Epoch 7/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9701 - val_loss: 4.9746\n","Epoch 8/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9660 - val_loss: 4.9748\n","Epoch 9/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9640 - val_loss: 4.9710\n","Epoch 10/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9636 - val_loss: 4.9708\n","Epoch 11/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9629 - val_loss: 4.9689\n","Epoch 12/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9617 - val_loss: 4.9682\n","Epoch 13/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9614 - val_loss: 4.9715\n","Epoch 14/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9612 - val_loss: 4.9758\n","Epoch 15/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9606 - val_loss: 4.9755\n","Epoch 16/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9603 - val_loss: 4.9712\n","Epoch 17/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9619 - val_loss: 4.9714\n","Epoch 18/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9599 - val_loss: 4.9701\n","Epoch 19/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9590 - val_loss: 4.9738\n","Epoch 20/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9598 - val_loss: 4.9774\n","Epoch 21/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9612 - val_loss: 4.9906\n","Epoch 22/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9605 - val_loss: 4.9782\n","Epoch 23/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9593 - val_loss: 4.9737\n","Epoch 24/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9591 - val_loss: 4.9696\n","Epoch 25/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9590 - val_loss: 4.9798\n","Epoch 26/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9684 - val_loss: 5.0127\n","Epoch 27/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9610 - val_loss: 4.9775\n","Epoch 28/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9588 - val_loss: 4.9683\n","Epoch 29/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9579 - val_loss: 4.9645\n","Epoch 30/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9576 - val_loss: 4.9620\n","Epoch 31/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9588 - val_loss: 4.9617\n","Epoch 32/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9582 - val_loss: 4.9654\n","Epoch 33/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9586 - val_loss: 4.9652\n","Epoch 34/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9580 - val_loss: 4.9639\n","Epoch 35/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9572 - val_loss: 4.9614\n","Epoch 36/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9571 - val_loss: 4.9616\n","Epoch 37/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9568 - val_loss: 4.9600\n","Epoch 38/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9565 - val_loss: 4.9590\n","Epoch 39/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9563 - val_loss: 4.9590\n","Epoch 40/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9563 - val_loss: 4.9593\n","Epoch 41/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9562 - val_loss: 4.9596\n","Epoch 42/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9558 - val_loss: 4.9589\n","Epoch 43/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9564 - val_loss: 4.9710\n","Epoch 44/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9565 - val_loss: 4.9739\n","Epoch 45/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9564 - val_loss: 4.9624\n","Epoch 46/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9560 - val_loss: 4.9666\n","Epoch 47/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9567 - val_loss: 4.9714\n","Epoch 48/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9564 - val_loss: 4.9639\n","Epoch 49/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9561 - val_loss: 4.9608\n","Epoch 50/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9558 - val_loss: 4.9594\n","Epoch 51/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9557 - val_loss: 4.9592\n","Epoch 52/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9556 - val_loss: 4.9597\n","Epoch 53/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9555 - val_loss: 4.9592\n","Epoch 54/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9553 - val_loss: 4.9586\n","Epoch 55/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9553 - val_loss: 4.9584\n","Epoch 56/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9552 - val_loss: 4.9584\n","Epoch 57/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9551 - val_loss: 4.9584\n","Epoch 58/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9553 - val_loss: 4.9584\n","Epoch 59/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9552 - val_loss: 4.9580\n","Epoch 60/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 61/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 62/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 63/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 64/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 65/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 66/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 67/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 68/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 69/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 70/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 71/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 72/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 73/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 74/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 75/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 76/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 77/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 78/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 79/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 80/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 81/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 82/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9552 - val_loss: 4.9587\n","Epoch 83/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 84/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 85/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 86/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 87/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 88/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 89/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9541 - val_loss: 4.9605\n","Epoch 90/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9546 - val_loss: 4.9608\n","Epoch 91/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9543 - val_loss: 4.9589\n","Epoch 92/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 93/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 94/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 95/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 96/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 97/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 98/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 99/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 100/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 101/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 102/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 103/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 104/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 105/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 106/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 107/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 108/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 109/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 110/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 111/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 112/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 113/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 114/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 115/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 116/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 117/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 118/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 119/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 120/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 121/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 122/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 123/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 124/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 125/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 126/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 127/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 128/250\n","4917/4917 [==============================] - 1s 287us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 129/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 130/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 131/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 132/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 133/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 134/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 135/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 136/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 137/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 138/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 139/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 140/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 141/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 142/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 143/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 144/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 145/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 146/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 147/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 148/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 149/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 150/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 151/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 152/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 153/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 154/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 155/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 156/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 157/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 158/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 159/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 160/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 161/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 162/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 163/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 164/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 165/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 166/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 167/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 168/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9527 - val_loss: 4.9606\n","Epoch 169/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 170/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 171/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 172/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 173/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 174/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 175/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 176/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 177/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 178/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 179/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 180/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 181/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 182/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 183/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 184/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 185/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 186/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 187/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 188/250\n","4917/4917 [==============================] - 1s 287us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 189/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 190/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 191/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 192/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 193/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 194/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 195/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 196/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 197/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 198/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 199/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 200/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 201/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 202/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 203/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 204/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 205/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 206/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 207/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 208/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 209/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 210/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 211/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 212/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 213/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 214/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 215/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 216/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 217/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 218/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9524 - val_loss: 4.9567\n","Epoch 219/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9524 - val_loss: 4.9567\n","Epoch 220/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9525 - val_loss: 4.9571\n","Epoch 221/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 222/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 223/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 224/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 225/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 226/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 227/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9524 - val_loss: 4.9566\n","Epoch 228/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9524 - val_loss: 4.9567\n","Epoch 229/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 230/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 231/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 232/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9524 - val_loss: 4.9569\n","Epoch 233/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9524 - val_loss: 4.9566\n","Epoch 234/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9523 - val_loss: 4.9566\n","Epoch 235/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 236/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9523 - val_loss: 4.9566\n","Epoch 237/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 238/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9523 - val_loss: 4.9566\n","Epoch 239/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9523 - val_loss: 4.9566\n","Epoch 240/250\n","4917/4917 [==============================] - 1s 287us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 241/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 242/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 243/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 244/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9524 - val_loss: 4.9569\n","Epoch 245/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 246/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 247/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 248/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9523 - val_loss: 4.9568\n","Epoch 249/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9523 - val_loss: 4.9567\n","Epoch 250/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9523 - val_loss: 4.9566\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 46534 0.5\n","The shape of N (5464, 784)\n","The minimum value of N  -0.7495759129524231\n","The max value of N 0.7486129403114319\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9966796741288424\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/250\n","4917/4917 [==============================] - 4s 777us/step - loss: 5.0641 - val_loss: 5.2092\n","Epoch 2/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 5.0005 - val_loss: 5.0376\n","Epoch 3/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9868 - val_loss: 4.9948\n","Epoch 4/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9798 - val_loss: 4.9909\n","Epoch 5/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9747 - val_loss: 4.9838\n","Epoch 6/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9718 - val_loss: 4.9847\n","Epoch 7/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9724 - val_loss: 4.9858\n","Epoch 8/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9698 - val_loss: 4.9766\n","Epoch 9/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9675 - val_loss: 4.9741\n","Epoch 10/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9656 - val_loss: 4.9731\n","Epoch 11/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9645 - val_loss: 4.9727\n","Epoch 12/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9685 - val_loss: 4.9836\n","Epoch 13/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9645 - val_loss: 4.9840\n","Epoch 14/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9632 - val_loss: 4.9733\n","Epoch 15/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9630 - val_loss: 4.9729\n","Epoch 16/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9626 - val_loss: 4.9697\n","Epoch 17/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9613 - val_loss: 4.9703\n","Epoch 18/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9609 - val_loss: 4.9687\n","Epoch 19/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9605 - val_loss: 4.9743\n","Epoch 20/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9623 - val_loss: 4.9858\n","Epoch 21/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9644 - val_loss: 4.9759\n","Epoch 22/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9614 - val_loss: 4.9683\n","Epoch 23/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9601 - val_loss: 4.9683\n","Epoch 24/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9593 - val_loss: 4.9648\n","Epoch 25/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9594 - val_loss: 4.9636\n","Epoch 26/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9590 - val_loss: 4.9642\n","Epoch 27/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9588 - val_loss: 4.9620\n","Epoch 28/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9589 - val_loss: 4.9612\n","Epoch 29/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9580 - val_loss: 4.9608\n","Epoch 30/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9581 - val_loss: 4.9612\n","Epoch 31/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9581 - val_loss: 4.9615\n","Epoch 32/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9618 - val_loss: 4.9674\n","Epoch 33/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9590 - val_loss: 4.9632\n","Epoch 34/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9583 - val_loss: 4.9618\n","Epoch 35/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9577 - val_loss: 4.9603\n","Epoch 36/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9578 - val_loss: 4.9601\n","Epoch 37/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9575 - val_loss: 4.9598\n","Epoch 38/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9574 - val_loss: 4.9608\n","Epoch 39/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9574 - val_loss: 4.9598\n","Epoch 40/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9569 - val_loss: 4.9592\n","Epoch 41/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9566 - val_loss: 4.9597\n","Epoch 42/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9567 - val_loss: 4.9590\n","Epoch 43/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9568 - val_loss: 4.9585\n","Epoch 44/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9565 - val_loss: 4.9586\n","Epoch 45/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9567 - val_loss: 4.9588\n","Epoch 46/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9564 - val_loss: 4.9585\n","Epoch 47/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9563 - val_loss: 4.9582\n","Epoch 48/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9563 - val_loss: 4.9585\n","Epoch 49/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9558 - val_loss: 4.9583\n","Epoch 50/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9558 - val_loss: 4.9580\n","Epoch 51/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9558 - val_loss: 4.9578\n","Epoch 52/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9560 - val_loss: 4.9582\n","Epoch 53/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9561 - val_loss: 4.9583\n","Epoch 54/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9560 - val_loss: 4.9590\n","Epoch 55/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9561 - val_loss: 4.9587\n","Epoch 56/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9557 - val_loss: 4.9583\n","Epoch 57/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9557 - val_loss: 4.9578\n","Epoch 58/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9555 - val_loss: 4.9575\n","Epoch 59/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9554 - val_loss: 4.9578\n","Epoch 60/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9554 - val_loss: 4.9576\n","Epoch 61/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9552 - val_loss: 4.9575\n","Epoch 62/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9553 - val_loss: 4.9574\n","Epoch 63/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9552 - val_loss: 4.9573\n","Epoch 64/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9551 - val_loss: 4.9587\n","Epoch 65/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9551 - val_loss: 4.9573\n","Epoch 66/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9550 - val_loss: 4.9573\n","Epoch 67/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9551 - val_loss: 4.9576\n","Epoch 68/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9550 - val_loss: 4.9574\n","Epoch 69/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 70/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9558 - val_loss: 4.9599\n","Epoch 71/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9554 - val_loss: 4.9582\n","Epoch 72/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 73/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9553 - val_loss: 4.9576\n","Epoch 74/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9550 - val_loss: 4.9576\n","Epoch 75/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 76/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 77/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9570 - val_loss: 4.9654\n","Epoch 78/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9569 - val_loss: 4.9605\n","Epoch 79/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9557 - val_loss: 4.9586\n","Epoch 80/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9552 - val_loss: 4.9574\n","Epoch 81/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9553 - val_loss: 4.9574\n","Epoch 82/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9552 - val_loss: 4.9574\n","Epoch 83/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 84/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9549 - val_loss: 4.9572\n","Epoch 85/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9549 - val_loss: 4.9571\n","Epoch 86/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9549 - val_loss: 4.9572\n","Epoch 87/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9548 - val_loss: 4.9569\n","Epoch 88/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9546 - val_loss: 4.9570\n","Epoch 89/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9547 - val_loss: 4.9571\n","Epoch 90/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 91/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 92/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 93/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 94/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9545 - val_loss: 4.9569\n","Epoch 95/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9544 - val_loss: 4.9568\n","Epoch 96/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 97/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 98/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 99/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 100/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 101/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 102/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 103/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 104/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 105/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 106/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 107/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 108/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 109/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 110/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 111/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 112/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 113/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 114/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9545 - val_loss: 4.9604\n","Epoch 115/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 116/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 117/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 118/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 119/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 120/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 121/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 122/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 123/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 124/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 125/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 126/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 127/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 128/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 129/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 130/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 131/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 132/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 133/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 134/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 135/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 136/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 137/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 138/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 139/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 140/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 141/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 142/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 143/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 144/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 145/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 146/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 147/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 148/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 149/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 150/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 151/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 152/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 153/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 154/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 155/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 156/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 157/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 158/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 159/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 160/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 161/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9551 - val_loss: 4.9587\n","Epoch 162/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 163/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 164/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 165/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 166/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 167/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 168/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 169/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 170/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 171/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 172/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 173/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 174/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 175/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 176/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 177/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 178/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 179/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 180/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 181/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 182/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 183/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 184/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 185/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 186/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 187/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 188/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 189/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 190/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 191/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 192/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 193/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 194/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 195/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 196/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 197/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 198/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 199/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 200/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 201/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 202/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 203/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 204/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 205/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 206/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 207/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 208/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 209/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 210/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 211/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 212/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 213/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 214/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 215/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 216/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 217/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 218/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 219/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 220/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 221/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 222/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 223/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 224/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 225/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 226/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 227/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 228/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 229/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 230/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 231/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 232/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 233/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 234/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 235/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 236/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 237/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 238/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 239/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 240/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 241/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 242/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 243/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 244/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 245/250\n","4917/4917 [==============================] - 1s 288us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 246/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 247/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 248/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 249/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 250/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9526 - val_loss: 4.9564\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 42591 0.5\n","The shape of N (5464, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9927911275415897\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/250\n","4917/4917 [==============================] - 4s 872us/step - loss: 5.0732 - val_loss: 5.2111\n","Epoch 2/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 5.0049 - val_loss: 5.0318\n","Epoch 3/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9896 - val_loss: 4.9990\n","Epoch 4/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9812 - val_loss: 4.9902\n","Epoch 5/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9756 - val_loss: 4.9881\n","Epoch 6/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9721 - val_loss: 4.9823\n","Epoch 7/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9697 - val_loss: 4.9776\n","Epoch 8/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9673 - val_loss: 4.9794\n","Epoch 9/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9662 - val_loss: 4.9754\n","Epoch 10/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9644 - val_loss: 4.9713\n","Epoch 11/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9634 - val_loss: 4.9775\n","Epoch 12/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9632 - val_loss: 4.9720\n","Epoch 13/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9618 - val_loss: 4.9706\n","Epoch 14/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9612 - val_loss: 4.9712\n","Epoch 15/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9617 - val_loss: 4.9731\n","Epoch 16/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9607 - val_loss: 4.9715\n","Epoch 17/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9607 - val_loss: 4.9720\n","Epoch 18/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9600 - val_loss: 4.9702\n","Epoch 19/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9590 - val_loss: 4.9683\n","Epoch 20/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9603 - val_loss: 5.0148\n","Epoch 21/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9603 - val_loss: 4.9879\n","Epoch 22/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9592 - val_loss: 4.9771\n","Epoch 23/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9587 - val_loss: 4.9710\n","Epoch 24/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9583 - val_loss: 4.9711\n","Epoch 25/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9598 - val_loss: 4.9845\n","Epoch 26/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9632 - val_loss: 4.9995\n","Epoch 27/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9605 - val_loss: 4.9773\n","Epoch 28/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9600 - val_loss: 4.9691\n","Epoch 29/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9588 - val_loss: 4.9657\n","Epoch 30/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9581 - val_loss: 4.9634\n","Epoch 31/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9581 - val_loss: 4.9622\n","Epoch 32/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9577 - val_loss: 4.9629\n","Epoch 33/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9575 - val_loss: 4.9620\n","Epoch 34/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9574 - val_loss: 4.9610\n","Epoch 35/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9578 - val_loss: 4.9729\n","Epoch 36/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9585 - val_loss: 4.9708\n","Epoch 37/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9577 - val_loss: 4.9669\n","Epoch 38/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9575 - val_loss: 4.9665\n","Epoch 39/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9570 - val_loss: 4.9623\n","Epoch 40/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9570 - val_loss: 4.9677\n","Epoch 41/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9570 - val_loss: 4.9644\n","Epoch 42/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9567 - val_loss: 4.9615\n","Epoch 43/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9568 - val_loss: 4.9625\n","Epoch 44/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9567 - val_loss: 4.9607\n","Epoch 45/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9562 - val_loss: 4.9605\n","Epoch 46/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9561 - val_loss: 4.9598\n","Epoch 47/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9562 - val_loss: 4.9599\n","Epoch 48/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9568 - val_loss: 4.9607\n","Epoch 49/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9562 - val_loss: 4.9606\n","Epoch 50/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9560 - val_loss: 4.9599\n","Epoch 51/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9556 - val_loss: 4.9593\n","Epoch 52/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9556 - val_loss: 4.9590\n","Epoch 53/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9557 - val_loss: 4.9585\n","Epoch 54/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 55/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9554 - val_loss: 4.9583\n","Epoch 56/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9553 - val_loss: 4.9582\n","Epoch 57/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9552 - val_loss: 4.9583\n","Epoch 58/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9552 - val_loss: 4.9582\n","Epoch 59/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9551 - val_loss: 4.9580\n","Epoch 60/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9550 - val_loss: 4.9576\n","Epoch 61/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 62/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 63/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 64/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 65/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 66/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 67/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 68/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 69/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 70/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 71/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 72/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 73/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 74/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 75/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 76/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 77/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 78/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 79/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 80/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 81/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 82/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 83/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 84/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 85/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 86/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 87/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 88/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 89/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 90/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 91/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 92/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 93/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 94/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 95/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 96/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 97/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 98/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 99/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 100/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 101/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 102/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 103/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 104/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 105/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 106/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 107/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 108/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 109/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 110/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 111/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 112/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 113/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 114/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 115/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 116/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 117/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 118/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 119/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 120/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 121/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 122/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 123/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 124/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 125/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 126/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 127/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 128/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 129/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 130/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 131/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 132/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 133/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 134/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 135/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 136/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 137/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 138/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 139/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 140/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 141/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 142/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 143/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 144/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 145/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 146/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 147/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 148/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 149/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 150/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 151/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 152/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 153/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 154/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 155/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 156/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 157/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 158/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 159/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 160/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 161/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 162/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 163/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 164/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 165/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 166/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 167/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 168/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 169/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 170/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 171/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 172/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 173/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 174/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 175/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 176/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 177/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 178/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 179/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 180/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 181/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 182/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 183/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 184/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 185/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 186/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 187/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 188/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 189/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 190/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 191/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 192/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 193/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 194/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 195/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 196/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 197/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 198/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 199/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 200/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 201/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 202/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 203/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 204/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 205/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 206/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 207/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 208/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 209/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 210/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 211/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 212/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 213/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 214/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 215/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 216/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 217/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 218/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 219/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 220/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 221/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 222/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 223/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 224/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 225/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 226/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 227/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 228/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 229/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 230/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 231/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 232/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 233/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 234/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 235/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 236/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 237/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 238/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 239/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9525 - val_loss: 4.9570\n","Epoch 240/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 241/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 242/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9525 - val_loss: 4.9570\n","Epoch 243/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 244/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 245/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 246/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 247/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 248/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 249/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 250/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9525 - val_loss: 4.9571\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 45821 0.5\n","The shape of N (5464, 784)\n","The minimum value of N  -0.7429049611091614\n","The max value of N 0.7483798861503601\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9971178202231806\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/250\n","4917/4917 [==============================] - 5s 968us/step - loss: 5.0748 - val_loss: 5.1506\n","Epoch 2/250\n","4917/4917 [==============================] - 1s 289us/step - loss: 5.0066 - val_loss: 5.0234\n","Epoch 3/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9914 - val_loss: 4.9957\n","Epoch 4/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9829 - val_loss: 4.9864\n","Epoch 5/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9783 - val_loss: 4.9887\n","Epoch 6/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9730 - val_loss: 4.9816\n","Epoch 7/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9707 - val_loss: 4.9749\n","Epoch 8/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9682 - val_loss: 4.9730\n","Epoch 9/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9675 - val_loss: 4.9801\n","Epoch 10/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9662 - val_loss: 4.9738\n","Epoch 11/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9646 - val_loss: 4.9749\n","Epoch 12/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9637 - val_loss: 4.9704\n","Epoch 13/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9656 - val_loss: 4.9826\n","Epoch 14/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9643 - val_loss: 4.9751\n","Epoch 15/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9625 - val_loss: 4.9746\n","Epoch 16/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9614 - val_loss: 4.9722\n","Epoch 17/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9616 - val_loss: 4.9864\n","Epoch 18/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9621 - val_loss: 4.9735\n","Epoch 19/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9612 - val_loss: 4.9731\n","Epoch 20/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9607 - val_loss: 4.9751\n","Epoch 21/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9601 - val_loss: 4.9737\n","Epoch 22/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9605 - val_loss: 4.9694\n","Epoch 23/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9592 - val_loss: 4.9680\n","Epoch 24/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9588 - val_loss: 4.9663\n","Epoch 25/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9585 - val_loss: 4.9645\n","Epoch 26/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9583 - val_loss: 4.9643\n","Epoch 27/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9583 - val_loss: 4.9627\n","Epoch 28/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9579 - val_loss: 4.9619\n","Epoch 29/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9575 - val_loss: 4.9624\n","Epoch 30/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9612 - val_loss: 4.9652\n","Epoch 31/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9587 - val_loss: 4.9646\n","Epoch 32/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9577 - val_loss: 4.9632\n","Epoch 33/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9574 - val_loss: 4.9611\n","Epoch 34/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9572 - val_loss: 4.9614\n","Epoch 35/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9571 - val_loss: 4.9605\n","Epoch 36/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9570 - val_loss: 4.9601\n","Epoch 37/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9568 - val_loss: 4.9611\n","Epoch 38/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9573 - val_loss: 4.9767\n","Epoch 39/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9572 - val_loss: 4.9671\n","Epoch 40/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9570 - val_loss: 4.9641\n","Epoch 41/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9567 - val_loss: 4.9607\n","Epoch 42/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9567 - val_loss: 4.9617\n","Epoch 43/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9565 - val_loss: 4.9600\n","Epoch 44/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9564 - val_loss: 4.9600\n","Epoch 45/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9562 - val_loss: 4.9598\n","Epoch 46/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9562 - val_loss: 4.9593\n","Epoch 47/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9561 - val_loss: 4.9592\n","Epoch 48/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9559 - val_loss: 4.9589\n","Epoch 49/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9558 - val_loss: 4.9588\n","Epoch 50/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9558 - val_loss: 4.9584\n","Epoch 51/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9558 - val_loss: 4.9586\n","Epoch 52/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9557 - val_loss: 4.9592\n","Epoch 53/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9556 - val_loss: 4.9584\n","Epoch 54/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9557 - val_loss: 4.9590\n","Epoch 55/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9556 - val_loss: 4.9581\n","Epoch 56/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 57/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9554 - val_loss: 4.9584\n","Epoch 58/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9555 - val_loss: 4.9580\n","Epoch 59/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9556 - val_loss: 4.9586\n","Epoch 60/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9555 - val_loss: 4.9589\n","Epoch 61/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9552 - val_loss: 4.9582\n","Epoch 62/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9552 - val_loss: 4.9584\n","Epoch 63/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9551 - val_loss: 4.9580\n","Epoch 64/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 65/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9570 - val_loss: 4.9593\n","Epoch 66/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9561 - val_loss: 4.9588\n","Epoch 67/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9556 - val_loss: 4.9594\n","Epoch 68/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9553 - val_loss: 4.9627\n","Epoch 69/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9552 - val_loss: 4.9595\n","Epoch 70/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 71/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9552 - val_loss: 4.9745\n","Epoch 72/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9551 - val_loss: 4.9580\n","Epoch 73/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 74/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9552 - val_loss: 4.9586\n","Epoch 75/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 76/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 77/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9553 - val_loss: 4.9636\n","Epoch 78/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9554 - val_loss: 4.9646\n","Epoch 79/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9550 - val_loss: 4.9613\n","Epoch 80/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9555 - val_loss: 4.9622\n","Epoch 81/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9553 - val_loss: 4.9591\n","Epoch 82/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9550 - val_loss: 4.9582\n","Epoch 83/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 84/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 85/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 86/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 87/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 88/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 89/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9566 - val_loss: 4.9766\n","Epoch 90/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9566 - val_loss: 4.9691\n","Epoch 91/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9558 - val_loss: 4.9632\n","Epoch 92/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9555 - val_loss: 4.9637\n","Epoch 93/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9552 - val_loss: 4.9595\n","Epoch 94/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9551 - val_loss: 4.9589\n","Epoch 95/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9550 - val_loss: 4.9583\n","Epoch 96/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9550 - val_loss: 4.9585\n","Epoch 97/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9548 - val_loss: 4.9586\n","Epoch 98/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 99/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 100/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 101/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 102/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 103/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 104/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 105/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 106/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 107/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 108/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 109/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 110/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 111/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 112/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 113/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 114/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 115/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 116/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 117/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 118/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 119/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 120/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 121/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 122/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 123/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 124/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 125/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 126/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 127/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 128/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 129/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 130/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 131/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 132/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 133/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 134/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 135/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 136/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 137/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 138/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 139/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 140/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 141/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 142/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 143/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 144/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 145/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 146/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 147/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 148/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 149/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 150/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 151/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 152/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 153/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 154/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 155/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 156/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 157/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 158/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 159/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 160/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 161/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 162/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 163/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 164/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 165/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 166/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 167/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 168/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 169/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 170/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 171/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 172/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 173/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 174/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 175/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 176/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 177/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 178/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 179/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 180/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 181/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 182/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9542 - val_loss: 4.9947\n","Epoch 183/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9564 - val_loss: 4.9886\n","Epoch 184/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9554 - val_loss: 4.9744\n","Epoch 185/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9549 - val_loss: 4.9708\n","Epoch 186/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9548 - val_loss: 4.9645\n","Epoch 187/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9545 - val_loss: 4.9678\n","Epoch 188/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9544 - val_loss: 4.9591\n","Epoch 189/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 190/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9541 - val_loss: 4.9586\n","Epoch 191/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 192/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 193/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 194/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 195/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9540 - val_loss: 4.9587\n","Epoch 196/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 197/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 198/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 199/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 200/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 201/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 202/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 203/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 204/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 205/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 206/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 207/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 208/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 209/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 210/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 211/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 212/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 213/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 214/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 215/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 216/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 217/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 218/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 219/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 220/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 221/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 222/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 223/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 224/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 225/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 226/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 227/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 228/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 229/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 230/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 231/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 232/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 233/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 234/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 235/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 236/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 237/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 238/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 239/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 240/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 241/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 242/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 243/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 244/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 245/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 246/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 247/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 248/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 249/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 250/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9534 - val_loss: 4.9571\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 43072 0.5\n","The shape of N (5464, 784)\n","The minimum value of N  -0.7198274731636047\n","The max value of N 0.7462969422340393\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9877147942767166\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/250\n","4917/4917 [==============================] - 5s 1ms/step - loss: 5.0675 - val_loss: 5.3906\n","Epoch 2/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9997 - val_loss: 5.0525\n","Epoch 3/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9864 - val_loss: 5.0059\n","Epoch 4/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9788 - val_loss: 4.9824\n","Epoch 5/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9759 - val_loss: 4.9815\n","Epoch 6/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9740 - val_loss: 4.9765\n","Epoch 7/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9696 - val_loss: 4.9759\n","Epoch 8/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9681 - val_loss: 4.9759\n","Epoch 9/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9680 - val_loss: 4.9730\n","Epoch 10/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9653 - val_loss: 4.9731\n","Epoch 11/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9664 - val_loss: 4.9729\n","Epoch 12/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9660 - val_loss: 4.9775\n","Epoch 13/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9665 - val_loss: 4.9742\n","Epoch 14/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9643 - val_loss: 4.9690\n","Epoch 15/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9629 - val_loss: 4.9690\n","Epoch 16/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9620 - val_loss: 4.9673\n","Epoch 17/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9613 - val_loss: 4.9660\n","Epoch 18/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9613 - val_loss: 4.9657\n","Epoch 19/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9643 - val_loss: 4.9729\n","Epoch 20/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9614 - val_loss: 4.9681\n","Epoch 21/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9603 - val_loss: 4.9664\n","Epoch 22/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9599 - val_loss: 4.9647\n","Epoch 23/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9592 - val_loss: 4.9650\n","Epoch 24/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9592 - val_loss: 4.9645\n","Epoch 25/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9587 - val_loss: 4.9638\n","Epoch 26/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9583 - val_loss: 4.9634\n","Epoch 27/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9581 - val_loss: 4.9628\n","Epoch 28/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9577 - val_loss: 4.9618\n","Epoch 29/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9576 - val_loss: 4.9619\n","Epoch 30/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9578 - val_loss: 4.9621\n","Epoch 31/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9572 - val_loss: 4.9624\n","Epoch 32/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9570 - val_loss: 4.9605\n","Epoch 33/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9569 - val_loss: 4.9601\n","Epoch 34/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9565 - val_loss: 4.9602\n","Epoch 35/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9573 - val_loss: 4.9597\n","Epoch 36/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9566 - val_loss: 4.9609\n","Epoch 37/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9579 - val_loss: 4.9612\n","Epoch 38/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9569 - val_loss: 4.9701\n","Epoch 39/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9572 - val_loss: 4.9646\n","Epoch 40/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9568 - val_loss: 4.9604\n","Epoch 41/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9564 - val_loss: 4.9604\n","Epoch 42/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9560 - val_loss: 4.9602\n","Epoch 43/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9560 - val_loss: 4.9596\n","Epoch 44/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9556 - val_loss: 4.9586\n","Epoch 45/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9559 - val_loss: 4.9589\n","Epoch 46/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9554 - val_loss: 4.9579\n","Epoch 47/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9553 - val_loss: 4.9587\n","Epoch 48/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9554 - val_loss: 4.9581\n","Epoch 49/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9554 - val_loss: 4.9582\n","Epoch 50/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9555 - val_loss: 4.9586\n","Epoch 51/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9551 - val_loss: 4.9576\n","Epoch 52/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9556 - val_loss: 4.9593\n","Epoch 53/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 54/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 55/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9555 - val_loss: 4.9607\n","Epoch 56/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9562 - val_loss: 4.9589\n","Epoch 57/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 58/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9552 - val_loss: 4.9576\n","Epoch 59/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9549 - val_loss: 4.9583\n","Epoch 60/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 61/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 62/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 63/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 64/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9546 - val_loss: 4.9569\n","Epoch 65/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9546 - val_loss: 4.9567\n","Epoch 66/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9561 - val_loss: 4.9582\n","Epoch 67/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9579 - val_loss: 4.9699\n","Epoch 68/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9566 - val_loss: 4.9609\n","Epoch 69/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9561 - val_loss: 4.9612\n","Epoch 70/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9560 - val_loss: 4.9582\n","Epoch 71/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9553 - val_loss: 4.9588\n","Epoch 72/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9551 - val_loss: 4.9570\n","Epoch 73/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9549 - val_loss: 4.9569\n","Epoch 74/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9549 - val_loss: 4.9570\n","Epoch 75/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 76/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9546 - val_loss: 4.9569\n","Epoch 77/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9547 - val_loss: 4.9566\n","Epoch 78/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9547 - val_loss: 4.9562\n","Epoch 79/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9544 - val_loss: 4.9563\n","Epoch 80/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9545 - val_loss: 4.9588\n","Epoch 81/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 82/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9545 - val_loss: 4.9567\n","Epoch 83/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 84/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9541 - val_loss: 4.9565\n","Epoch 85/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9541 - val_loss: 4.9562\n","Epoch 86/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9543 - val_loss: 4.9566\n","Epoch 87/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 88/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9543 - val_loss: 4.9593\n","Epoch 89/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9554 - val_loss: 4.9623\n","Epoch 90/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9547 - val_loss: 4.9607\n","Epoch 91/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9546 - val_loss: 4.9587\n","Epoch 92/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9547 - val_loss: 4.9682\n","Epoch 93/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9546 - val_loss: 4.9650\n","Epoch 94/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9545 - val_loss: 4.9602\n","Epoch 95/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9542 - val_loss: 4.9589\n","Epoch 96/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9541 - val_loss: 4.9602\n","Epoch 97/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9544 - val_loss: 4.9598\n","Epoch 98/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9572 - val_loss: 4.9703\n","Epoch 99/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9564 - val_loss: 4.9609\n","Epoch 100/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 101/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9547 - val_loss: 4.9569\n","Epoch 102/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9545 - val_loss: 4.9568\n","Epoch 103/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9544 - val_loss: 4.9568\n","Epoch 104/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9542 - val_loss: 4.9562\n","Epoch 105/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 106/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9547 - val_loss: 4.9565\n","Epoch 107/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9542 - val_loss: 4.9563\n","Epoch 108/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9541 - val_loss: 4.9562\n","Epoch 109/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9539 - val_loss: 4.9559\n","Epoch 110/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 111/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 112/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 113/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 114/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 115/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9556\n","Epoch 116/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 117/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9534 - val_loss: 4.9555\n","Epoch 118/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9556\n","Epoch 119/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9534 - val_loss: 4.9556\n","Epoch 120/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 121/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9534 - val_loss: 4.9555\n","Epoch 122/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 123/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 124/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 125/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 126/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 127/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9558\n","Epoch 128/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9557\n","Epoch 129/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 130/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 131/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9531 - val_loss: 4.9554\n","Epoch 132/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 133/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9534 - val_loss: 4.9592\n","Epoch 134/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 135/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 136/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 137/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 138/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 139/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 140/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9531 - val_loss: 4.9554\n","Epoch 141/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 142/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 143/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 144/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 145/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 146/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 147/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 148/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 149/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 150/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 151/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 152/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 153/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 154/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 155/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 156/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 157/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 158/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 159/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 160/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 161/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 162/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 163/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 164/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 165/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 166/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 167/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 168/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 169/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 170/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 171/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 172/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 173/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 174/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 175/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 176/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 177/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 178/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 179/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 180/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 181/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 182/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 183/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 184/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 185/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 186/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 187/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 188/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 189/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 190/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 191/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 192/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 193/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 194/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 195/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 196/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 197/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 198/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 199/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 200/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 201/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 202/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 203/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 204/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 205/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 206/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 207/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 208/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 209/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 210/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 211/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 212/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 213/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 214/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 215/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 216/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 217/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 218/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 219/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 220/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 221/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 222/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9525 - val_loss: 4.9587\n","Epoch 223/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 224/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 225/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 226/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 227/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 228/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 229/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 230/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 231/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 232/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 233/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 234/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 235/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 236/250\n","4917/4917 [==============================] - 1s 290us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 237/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 238/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 239/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 240/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 241/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 242/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 243/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 244/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 245/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 246/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 247/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 248/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 249/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 250/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9522 - val_loss: 4.9556\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 46544 0.5\n","The shape of N (5464, 784)\n","The minimum value of N  -0.7315675616264343\n","The max value of N 0.7499980330467224\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9804682686383241\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/250\n","4917/4917 [==============================] - 6s 1ms/step - loss: 5.0676 - val_loss: 5.2070\n","Epoch 2/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 5.0004 - val_loss: 5.0419\n","Epoch 3/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9850 - val_loss: 5.0054\n","Epoch 4/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9787 - val_loss: 4.9888\n","Epoch 5/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9746 - val_loss: 4.9791\n","Epoch 6/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9712 - val_loss: 4.9824\n","Epoch 7/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9708 - val_loss: 4.9902\n","Epoch 8/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9694 - val_loss: 4.9723\n","Epoch 9/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9662 - val_loss: 4.9711\n","Epoch 10/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9651 - val_loss: 4.9697\n","Epoch 11/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9641 - val_loss: 4.9705\n","Epoch 12/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9643 - val_loss: 4.9700\n","Epoch 13/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9639 - val_loss: 4.9669\n","Epoch 14/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9623 - val_loss: 4.9670\n","Epoch 15/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9614 - val_loss: 4.9667\n","Epoch 16/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9609 - val_loss: 4.9667\n","Epoch 17/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9603 - val_loss: 4.9658\n","Epoch 18/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9603 - val_loss: 4.9647\n","Epoch 19/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9597 - val_loss: 4.9647\n","Epoch 20/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9594 - val_loss: 4.9646\n","Epoch 21/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9596 - val_loss: 4.9638\n","Epoch 22/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9589 - val_loss: 4.9642\n","Epoch 23/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9586 - val_loss: 4.9641\n","Epoch 24/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9588 - val_loss: 4.9635\n","Epoch 25/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9587 - val_loss: 4.9638\n","Epoch 26/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9582 - val_loss: 4.9649\n","Epoch 27/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9577 - val_loss: 4.9633\n","Epoch 28/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9579 - val_loss: 4.9623\n","Epoch 29/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9572 - val_loss: 4.9605\n","Epoch 30/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9628 - val_loss: 4.9923\n","Epoch 31/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9608 - val_loss: 4.9764\n","Epoch 32/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9596 - val_loss: 4.9779\n","Epoch 33/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9585 - val_loss: 4.9703\n","Epoch 34/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9580 - val_loss: 4.9664\n","Epoch 35/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9574 - val_loss: 4.9632\n","Epoch 36/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9570 - val_loss: 4.9615\n","Epoch 37/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9570 - val_loss: 4.9625\n","Epoch 38/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9572 - val_loss: 4.9615\n","Epoch 39/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9569 - val_loss: 4.9616\n","Epoch 40/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9567 - val_loss: 4.9599\n","Epoch 41/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9562 - val_loss: 4.9587\n","Epoch 42/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9561 - val_loss: 4.9583\n","Epoch 43/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9562 - val_loss: 4.9584\n","Epoch 44/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9560 - val_loss: 4.9580\n","Epoch 45/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9559 - val_loss: 4.9590\n","Epoch 46/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9559 - val_loss: 4.9583\n","Epoch 47/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9560 - val_loss: 4.9583\n","Epoch 48/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9561 - val_loss: 4.9613\n","Epoch 49/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9557 - val_loss: 4.9579\n","Epoch 50/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9556 - val_loss: 4.9580\n","Epoch 51/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9555 - val_loss: 4.9579\n","Epoch 52/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9558 - val_loss: 4.9584\n","Epoch 53/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9555 - val_loss: 4.9580\n","Epoch 54/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9554 - val_loss: 4.9578\n","Epoch 55/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9555 - val_loss: 4.9576\n","Epoch 56/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9553 - val_loss: 4.9576\n","Epoch 57/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9552 - val_loss: 4.9575\n","Epoch 58/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9550 - val_loss: 4.9574\n","Epoch 59/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9550 - val_loss: 4.9575\n","Epoch 60/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 61/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9550 - val_loss: 4.9573\n","Epoch 62/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 63/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9550 - val_loss: 4.9575\n","Epoch 64/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9550 - val_loss: 4.9571\n","Epoch 65/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9553 - val_loss: 4.9694\n","Epoch 66/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9577 - val_loss: 4.9641\n","Epoch 67/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9560 - val_loss: 4.9600\n","Epoch 68/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9554 - val_loss: 4.9582\n","Epoch 69/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9567 - val_loss: 4.9601\n","Epoch 70/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 71/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 72/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9563 - val_loss: 4.9596\n","Epoch 73/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9554 - val_loss: 4.9583\n","Epoch 74/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9556 - val_loss: 4.9593\n","Epoch 75/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9552 - val_loss: 4.9579\n","Epoch 76/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9550 - val_loss: 4.9572\n","Epoch 77/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9549 - val_loss: 4.9571\n","Epoch 78/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9549 - val_loss: 4.9569\n","Epoch 79/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9546 - val_loss: 4.9567\n","Epoch 80/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9545 - val_loss: 4.9567\n","Epoch 81/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9545 - val_loss: 4.9568\n","Epoch 82/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9544 - val_loss: 4.9566\n","Epoch 83/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9544 - val_loss: 4.9566\n","Epoch 84/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 85/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 86/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 87/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 88/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9543 - val_loss: 4.9566\n","Epoch 89/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 90/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9543 - val_loss: 4.9566\n","Epoch 91/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9541 - val_loss: 4.9566\n","Epoch 92/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 93/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 94/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9540 - val_loss: 4.9563\n","Epoch 95/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 96/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9541 - val_loss: 4.9565\n","Epoch 97/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 98/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9549 - val_loss: 4.9765\n","Epoch 99/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9575 - val_loss: 4.9782\n","Epoch 100/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9555 - val_loss: 4.9679\n","Epoch 101/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9548 - val_loss: 4.9621\n","Epoch 102/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9545 - val_loss: 4.9599\n","Epoch 103/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9543 - val_loss: 4.9589\n","Epoch 104/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 105/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 106/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 107/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 108/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 109/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 110/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 111/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 112/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 113/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 114/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 115/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9539 - val_loss: 4.9563\n","Epoch 116/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 117/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 118/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 119/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 120/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 121/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 122/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 123/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 124/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 125/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 126/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 127/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 128/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 129/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 130/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 131/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 132/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 133/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 134/250\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 135/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 136/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 137/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 138/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 139/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 140/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 141/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 142/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 143/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 144/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 145/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9535 - val_loss: 4.9583\n","Epoch 146/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 147/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 148/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 149/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 150/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 151/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 152/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 153/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 154/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 155/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 156/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 157/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 158/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 159/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 160/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 161/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 162/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 163/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 164/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 165/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 166/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 167/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 168/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 169/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 170/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 171/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 172/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 173/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 174/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 175/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 176/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 177/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 178/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 179/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 180/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 181/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 182/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 183/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 184/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 185/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 186/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 187/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 188/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 189/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 190/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 191/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 192/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 193/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 194/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 195/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 196/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 197/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 198/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 199/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 200/250\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 201/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 202/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 203/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 204/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 205/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 206/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 207/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 208/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 209/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 210/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 211/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 212/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 213/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 214/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 215/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 216/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 217/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 218/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 219/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 220/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 221/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 222/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 223/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 224/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 225/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 226/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 227/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 228/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 229/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 230/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 231/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 232/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 233/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 234/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 235/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 236/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 237/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 238/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 239/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 240/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 241/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 242/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 243/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 244/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 245/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 246/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 247/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 248/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9524 - val_loss: 4.9567\n","Epoch 249/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 250/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9523 - val_loss: 4.9564\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 39291 0.5\n","The shape of N (5464, 784)\n","The minimum value of N  -0.7408620119094849\n","The max value of N 0.7498639822006226\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9953481207640171\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/250\n","4917/4917 [==============================] - 6s 1ms/step - loss: 5.0827 - val_loss: 5.2332\n","Epoch 2/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 5.0073 - val_loss: 5.0197\n","Epoch 3/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9902 - val_loss: 4.9929\n","Epoch 4/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9808 - val_loss: 4.9859\n","Epoch 5/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9745 - val_loss: 4.9814\n","Epoch 6/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9709 - val_loss: 4.9791\n","Epoch 7/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9682 - val_loss: 4.9785\n","Epoch 8/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9666 - val_loss: 4.9752\n","Epoch 9/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9652 - val_loss: 4.9803\n","Epoch 10/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9649 - val_loss: 4.9956\n","Epoch 11/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9647 - val_loss: 4.9794\n","Epoch 12/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9657 - val_loss: 5.0037\n","Epoch 13/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9676 - val_loss: 4.9958\n","Epoch 14/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9631 - val_loss: 4.9924\n","Epoch 15/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9636 - val_loss: 4.9938\n","Epoch 16/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9644 - val_loss: 4.9827\n","Epoch 17/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9621 - val_loss: 4.9693\n","Epoch 18/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9603 - val_loss: 4.9700\n","Epoch 19/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9600 - val_loss: 4.9698\n","Epoch 20/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9604 - val_loss: 4.9762\n","Epoch 21/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9621 - val_loss: 4.9678\n","Epoch 22/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9595 - val_loss: 4.9637\n","Epoch 23/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9586 - val_loss: 4.9631\n","Epoch 24/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9590 - val_loss: 4.9692\n","Epoch 25/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9588 - val_loss: 4.9684\n","Epoch 26/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9581 - val_loss: 4.9640\n","Epoch 27/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9588 - val_loss: 4.9661\n","Epoch 28/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9581 - val_loss: 4.9649\n","Epoch 29/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9578 - val_loss: 4.9642\n","Epoch 30/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9577 - val_loss: 4.9621\n","Epoch 31/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9575 - val_loss: 4.9618\n","Epoch 32/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9571 - val_loss: 4.9606\n","Epoch 33/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9568 - val_loss: 4.9602\n","Epoch 34/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9567 - val_loss: 4.9599\n","Epoch 35/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9567 - val_loss: 4.9685\n","Epoch 36/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9581 - val_loss: 4.9777\n","Epoch 37/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9576 - val_loss: 4.9638\n","Epoch 38/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9569 - val_loss: 4.9617\n","Epoch 39/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9569 - val_loss: 4.9603\n","Epoch 40/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9567 - val_loss: 4.9601\n","Epoch 41/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9565 - val_loss: 4.9604\n","Epoch 42/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9566 - val_loss: 4.9602\n","Epoch 43/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9564 - val_loss: 4.9601\n","Epoch 44/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9565 - val_loss: 4.9595\n","Epoch 45/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9562 - val_loss: 4.9598\n","Epoch 46/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9562 - val_loss: 4.9595\n","Epoch 47/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9562 - val_loss: 4.9589\n","Epoch 48/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9562 - val_loss: 4.9593\n","Epoch 49/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9564 - val_loss: 4.9590\n","Epoch 50/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9561 - val_loss: 4.9591\n","Epoch 51/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9560 - val_loss: 4.9590\n","Epoch 52/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9557 - val_loss: 4.9585\n","Epoch 53/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9556 - val_loss: 4.9587\n","Epoch 54/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9555 - val_loss: 4.9585\n","Epoch 55/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9554 - val_loss: 4.9584\n","Epoch 56/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9559 - val_loss: 4.9585\n","Epoch 57/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9573 - val_loss: 4.9604\n","Epoch 58/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9561 - val_loss: 4.9592\n","Epoch 59/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9559 - val_loss: 4.9599\n","Epoch 60/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9556 - val_loss: 4.9589\n","Epoch 61/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9554 - val_loss: 4.9584\n","Epoch 62/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9554 - val_loss: 4.9587\n","Epoch 63/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9553 - val_loss: 4.9582\n","Epoch 64/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9557 - val_loss: 4.9588\n","Epoch 65/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9558 - val_loss: 4.9817\n","Epoch 66/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9568 - val_loss: 4.9677\n","Epoch 67/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9560 - val_loss: 4.9617\n","Epoch 68/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9558 - val_loss: 4.9597\n","Epoch 69/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9554 - val_loss: 4.9587\n","Epoch 70/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9554 - val_loss: 4.9584\n","Epoch 71/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9553 - val_loss: 4.9589\n","Epoch 72/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9553 - val_loss: 4.9586\n","Epoch 73/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9552 - val_loss: 4.9584\n","Epoch 74/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9552 - val_loss: 4.9583\n","Epoch 75/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 76/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9551 - val_loss: 4.9587\n","Epoch 77/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9550 - val_loss: 4.9586\n","Epoch 78/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9551 - val_loss: 4.9586\n","Epoch 79/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9549 - val_loss: 4.9590\n","Epoch 80/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9548 - val_loss: 4.9586\n","Epoch 81/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 82/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9549 - val_loss: 4.9585\n","Epoch 83/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9549 - val_loss: 4.9591\n","Epoch 84/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9547 - val_loss: 4.9584\n","Epoch 85/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9546 - val_loss: 4.9583\n","Epoch 86/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9546 - val_loss: 4.9586\n","Epoch 87/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 88/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9546 - val_loss: 4.9587\n","Epoch 89/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 90/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 91/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9547 - val_loss: 4.9585\n","Epoch 92/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 93/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 94/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 95/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 96/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 97/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 98/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 99/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 100/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 101/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 102/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 103/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 104/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 105/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 106/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 107/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 108/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 109/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 110/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 111/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 112/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 113/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 114/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 115/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 116/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 117/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 118/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 119/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 120/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 121/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 122/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 123/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 124/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 125/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 126/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 127/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 128/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 129/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 130/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 131/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 132/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 133/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 134/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 135/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 136/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 137/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 138/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 139/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 140/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 141/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 142/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 143/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 144/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 145/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 146/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 147/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 148/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 149/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 150/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 151/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 152/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 153/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 154/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 155/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 156/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 157/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 158/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 159/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 160/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 161/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 162/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 163/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 164/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 165/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 166/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 167/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 168/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 169/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 170/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 171/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 172/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 173/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 174/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 175/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 176/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 177/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 178/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 179/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 180/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 181/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 182/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 183/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 184/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 185/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 186/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9588\n","Epoch 187/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 188/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 189/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 190/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 191/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 192/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 193/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 194/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 195/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 196/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 197/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 198/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 199/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 200/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 201/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 202/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 203/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 204/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 205/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 206/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 207/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 208/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 209/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 210/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 211/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 212/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 213/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 214/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 215/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 216/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 217/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 218/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 219/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 220/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 221/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 222/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 223/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 224/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 225/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 226/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 227/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 228/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 229/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9642\n","Epoch 230/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 231/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 232/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 233/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 234/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 235/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 236/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 237/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 238/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 239/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 240/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 241/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 242/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 243/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 244/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 245/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 246/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 247/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 248/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 249/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 250/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9529 - val_loss: 4.9579\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 44519 0.5\n","The shape of N (5464, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7492730021476746\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.996799479701513\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/250\n","4917/4917 [==============================] - 7s 1ms/step - loss: 5.0742 - val_loss: 5.2472\n","Epoch 2/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 5.0039 - val_loss: 5.0544\n","Epoch 3/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9884 - val_loss: 4.9982\n","Epoch 4/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9812 - val_loss: 4.9937\n","Epoch 5/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9749 - val_loss: 4.9848\n","Epoch 6/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9726 - val_loss: 4.9829\n","Epoch 7/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9703 - val_loss: 4.9823\n","Epoch 8/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9685 - val_loss: 4.9788\n","Epoch 9/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9670 - val_loss: 4.9748\n","Epoch 10/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9657 - val_loss: 4.9776\n","Epoch 11/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9671 - val_loss: 4.9814\n","Epoch 12/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9641 - val_loss: 4.9807\n","Epoch 13/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9627 - val_loss: 4.9748\n","Epoch 14/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9618 - val_loss: 4.9747\n","Epoch 15/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9624 - val_loss: 4.9708\n","Epoch 16/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9618 - val_loss: 4.9712\n","Epoch 17/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9611 - val_loss: 4.9689\n","Epoch 18/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9603 - val_loss: 4.9681\n","Epoch 19/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9601 - val_loss: 4.9670\n","Epoch 20/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9597 - val_loss: 4.9655\n","Epoch 21/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9594 - val_loss: 4.9657\n","Epoch 22/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9593 - val_loss: 4.9653\n","Epoch 23/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9588 - val_loss: 4.9642\n","Epoch 24/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9587 - val_loss: 4.9647\n","Epoch 25/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9585 - val_loss: 4.9628\n","Epoch 26/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9585 - val_loss: 4.9627\n","Epoch 27/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9580 - val_loss: 4.9627\n","Epoch 28/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9604 - val_loss: 4.9636\n","Epoch 29/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9585 - val_loss: 4.9628\n","Epoch 30/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9581 - val_loss: 4.9627\n","Epoch 31/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9578 - val_loss: 4.9623\n","Epoch 32/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9574 - val_loss: 4.9638\n","Epoch 33/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9574 - val_loss: 4.9638\n","Epoch 34/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9574 - val_loss: 4.9630\n","Epoch 35/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9572 - val_loss: 4.9618\n","Epoch 36/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9570 - val_loss: 4.9604\n","Epoch 37/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9570 - val_loss: 4.9617\n","Epoch 38/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9569 - val_loss: 4.9608\n","Epoch 39/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9566 - val_loss: 4.9597\n","Epoch 40/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9562 - val_loss: 4.9593\n","Epoch 41/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9561 - val_loss: 4.9599\n","Epoch 42/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9562 - val_loss: 4.9593\n","Epoch 43/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9575 - val_loss: 4.9641\n","Epoch 44/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9597 - val_loss: 4.9659\n","Epoch 45/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9572 - val_loss: 4.9616\n","Epoch 46/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9567 - val_loss: 4.9606\n","Epoch 47/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9567 - val_loss: 4.9602\n","Epoch 48/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9565 - val_loss: 4.9605\n","Epoch 49/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9567 - val_loss: 4.9684\n","Epoch 50/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9566 - val_loss: 4.9644\n","Epoch 51/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9563 - val_loss: 4.9618\n","Epoch 52/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9560 - val_loss: 4.9601\n","Epoch 53/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9558 - val_loss: 4.9599\n","Epoch 54/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9557 - val_loss: 4.9591\n","Epoch 55/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9558 - val_loss: 4.9587\n","Epoch 56/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9559 - val_loss: 4.9587\n","Epoch 57/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9556 - val_loss: 4.9579\n","Epoch 58/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9559 - val_loss: 4.9585\n","Epoch 59/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9557 - val_loss: 4.9585\n","Epoch 60/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9555 - val_loss: 4.9585\n","Epoch 61/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9555 - val_loss: 4.9585\n","Epoch 62/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9554 - val_loss: 4.9579\n","Epoch 63/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9553 - val_loss: 4.9580\n","Epoch 64/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9554 - val_loss: 4.9577\n","Epoch 65/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9552 - val_loss: 4.9579\n","Epoch 66/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9551 - val_loss: 4.9576\n","Epoch 67/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 68/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 69/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9550 - val_loss: 4.9579\n","Epoch 70/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9550 - val_loss: 4.9585\n","Epoch 71/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 72/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 73/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 74/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 75/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 76/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 77/250\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 78/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9547 - val_loss: 4.9574\n","Epoch 79/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 80/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 81/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 82/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9547 - val_loss: 4.9575\n","Epoch 83/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 84/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 85/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9557 - val_loss: 4.9588\n","Epoch 86/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9551 - val_loss: 4.9584\n","Epoch 87/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9547 - val_loss: 4.9573\n","Epoch 88/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9546 - val_loss: 4.9587\n","Epoch 89/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 90/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 91/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 92/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 93/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 94/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 95/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 96/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 97/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 98/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 99/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 100/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 101/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 102/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 103/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9544 - val_loss: 4.9599\n","Epoch 104/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9554 - val_loss: 4.9619\n","Epoch 105/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9550 - val_loss: 4.9598\n","Epoch 106/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9546 - val_loss: 4.9585\n","Epoch 107/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 108/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 109/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 110/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 111/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 112/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 113/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 114/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 115/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 116/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 117/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 118/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 119/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 120/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 121/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 122/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 123/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 124/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 125/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 126/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 127/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 128/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 129/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 130/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 131/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 132/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 133/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 134/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 135/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 136/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 137/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 138/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 139/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 140/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 141/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 142/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 143/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 144/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 145/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 146/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 147/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 148/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 149/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 150/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 151/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 152/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 153/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 154/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 155/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 156/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 157/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 158/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 159/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 160/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 161/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 162/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 163/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 164/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 165/250\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 166/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 167/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 168/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 169/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 170/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 171/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 172/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 173/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 174/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 175/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 176/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 177/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 178/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 179/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 180/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 181/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 182/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 183/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 184/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 185/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 186/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 187/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 188/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 189/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 190/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 191/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 192/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 193/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 194/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 195/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 196/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 197/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 198/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 199/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 200/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 201/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 202/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 203/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 204/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 205/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 206/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 207/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 208/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 209/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 210/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 211/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 212/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 213/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 214/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 215/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 216/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 217/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 218/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 219/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 220/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 221/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 222/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 223/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 224/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 225/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 226/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 227/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 228/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 229/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 230/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 231/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 232/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 233/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 234/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 235/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 236/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 237/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 238/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 239/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 240/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 241/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 242/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 243/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 244/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 245/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 246/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 247/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 248/250\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 249/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 250/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9530 - val_loss: 4.9571\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 50906 0.5\n","The shape of N (5464, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7493996024131775\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9850927637434106\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/250\n","4917/4917 [==============================] - 8s 2ms/step - loss: 5.0936 - val_loss: 5.2169\n","Epoch 2/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 5.0102 - val_loss: 5.0287\n","Epoch 3/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9920 - val_loss: 4.9951\n","Epoch 4/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9816 - val_loss: 4.9889\n","Epoch 5/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9751 - val_loss: 4.9857\n","Epoch 6/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9711 - val_loss: 4.9835\n","Epoch 7/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9690 - val_loss: 4.9850\n","Epoch 8/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9672 - val_loss: 4.9837\n","Epoch 9/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9655 - val_loss: 4.9786\n","Epoch 10/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9639 - val_loss: 4.9756\n","Epoch 11/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9633 - val_loss: 4.9766\n","Epoch 12/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9620 - val_loss: 4.9767\n","Epoch 13/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9617 - val_loss: 4.9741\n","Epoch 14/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9613 - val_loss: 4.9746\n","Epoch 15/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9621 - val_loss: 4.9955\n","Epoch 16/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9619 - val_loss: 4.9862\n","Epoch 17/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9621 - val_loss: 4.9757\n","Epoch 18/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9600 - val_loss: 4.9708\n","Epoch 19/250\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9598 - val_loss: 4.9689\n","Epoch 20/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9590 - val_loss: 4.9669\n","Epoch 21/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9587 - val_loss: 4.9659\n","Epoch 22/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9585 - val_loss: 4.9670\n","Epoch 23/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9589 - val_loss: 4.9666\n","Epoch 24/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9583 - val_loss: 4.9657\n","Epoch 25/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9579 - val_loss: 4.9639\n","Epoch 26/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9574 - val_loss: 4.9635\n","Epoch 27/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9575 - val_loss: 4.9625\n","Epoch 28/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9575 - val_loss: 4.9638\n","Epoch 29/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9576 - val_loss: 4.9629\n","Epoch 30/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9571 - val_loss: 4.9629\n","Epoch 31/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9571 - val_loss: 4.9628\n","Epoch 32/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9583 - val_loss: 4.9699\n","Epoch 33/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9574 - val_loss: 4.9640\n","Epoch 34/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9570 - val_loss: 4.9619\n","Epoch 35/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9567 - val_loss: 4.9612\n","Epoch 36/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9565 - val_loss: 4.9617\n","Epoch 37/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9566 - val_loss: 4.9605\n","Epoch 38/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9567 - val_loss: 4.9619\n","Epoch 39/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9567 - val_loss: 4.9636\n","Epoch 40/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9564 - val_loss: 4.9620\n","Epoch 41/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9561 - val_loss: 4.9608\n","Epoch 42/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9560 - val_loss: 4.9598\n","Epoch 43/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9559 - val_loss: 4.9589\n","Epoch 44/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9558 - val_loss: 4.9593\n","Epoch 45/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9558 - val_loss: 4.9589\n","Epoch 46/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9558 - val_loss: 4.9591\n","Epoch 47/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9558 - val_loss: 4.9590\n","Epoch 48/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9559 - val_loss: 4.9597\n","Epoch 49/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9567 - val_loss: 4.9700\n","Epoch 50/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9587 - val_loss: 4.9685\n","Epoch 51/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9567 - val_loss: 4.9654\n","Epoch 52/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9562 - val_loss: 4.9624\n","Epoch 53/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9561 - val_loss: 4.9594\n","Epoch 54/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9566 - val_loss: 4.9588\n","Epoch 55/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9557 - val_loss: 4.9590\n","Epoch 56/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9558 - val_loss: 4.9596\n","Epoch 57/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9553 - val_loss: 4.9585\n","Epoch 58/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9554 - val_loss: 4.9587\n","Epoch 59/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9555 - val_loss: 4.9580\n","Epoch 60/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9556 - val_loss: 4.9603\n","Epoch 61/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9554 - val_loss: 4.9590\n","Epoch 62/250\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9554 - val_loss: 4.9584\n","Epoch 63/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 64/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 65/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 66/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 67/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9549 - val_loss: 4.9586\n","Epoch 68/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 69/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 70/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 71/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 72/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 73/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 74/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 75/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 76/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 77/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 78/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 79/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 80/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 81/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 82/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 83/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 84/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 85/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 86/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 87/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 88/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 89/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 90/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 91/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 92/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9546 - val_loss: 4.9592\n","Epoch 93/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9543 - val_loss: 4.9586\n","Epoch 94/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 95/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 96/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 97/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 98/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 99/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 100/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 101/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 102/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 103/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9542 - val_loss: 4.9591\n","Epoch 104/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9547 - val_loss: 4.9591\n","Epoch 105/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9544 - val_loss: 4.9584\n","Epoch 106/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 107/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 108/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 109/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 110/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 111/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 112/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 113/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 114/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 115/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 116/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 117/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 118/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 119/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 120/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 121/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 122/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 123/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 124/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 125/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 126/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 127/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 128/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 129/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9541 - val_loss: 4.9672\n","Epoch 130/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9542 - val_loss: 4.9590\n","Epoch 131/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 132/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 133/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 134/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 135/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 136/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 137/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 138/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 139/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 140/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 141/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 142/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 143/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 144/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 145/250\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 146/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 147/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 148/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 149/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 150/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 151/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 152/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 153/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 154/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 155/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 156/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 157/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 158/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 159/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 160/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 161/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 162/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 163/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 164/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 165/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 166/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 167/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 168/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 169/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 170/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 171/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 172/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 173/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 174/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 175/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 176/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 177/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 178/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 179/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 180/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 181/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 182/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 183/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 184/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 185/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 186/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 187/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 188/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9543 - val_loss: 5.0054\n","Epoch 189/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9560 - val_loss: 4.9887\n","Epoch 190/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9547 - val_loss: 4.9723\n","Epoch 191/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9541 - val_loss: 4.9655\n","Epoch 192/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9539 - val_loss: 4.9617\n","Epoch 193/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9540 - val_loss: 4.9600\n","Epoch 194/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9538 - val_loss: 4.9592\n","Epoch 195/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9535 - val_loss: 4.9587\n","Epoch 196/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 197/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 198/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 199/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 200/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 201/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 202/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 203/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 204/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 205/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 206/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 207/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 208/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 209/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 210/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 211/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 212/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 213/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 214/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 215/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 216/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 217/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 218/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 219/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 220/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 221/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 222/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 223/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 224/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 225/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 226/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 227/250\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 228/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 229/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 230/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 231/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 232/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 233/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 234/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 235/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 236/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 237/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 238/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 239/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 240/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 241/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 242/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 243/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 244/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 245/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 246/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 247/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 248/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 249/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 250/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9529 - val_loss: 4.9577\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 42569 0.5\n","The shape of N (5464, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7499068975448608\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9957280755802013\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/250\n","4917/4917 [==============================] - 8s 2ms/step - loss: 5.0766 - val_loss: 5.2102\n","Epoch 2/250\n","4917/4917 [==============================] - 1s 300us/step - loss: 5.0057 - val_loss: 5.0251\n","Epoch 3/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9898 - val_loss: 4.9936\n","Epoch 4/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9815 - val_loss: 4.9904\n","Epoch 5/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9759 - val_loss: 4.9864\n","Epoch 6/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9709 - val_loss: 4.9815\n","Epoch 7/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9684 - val_loss: 4.9794\n","Epoch 8/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9665 - val_loss: 4.9791\n","Epoch 9/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9665 - val_loss: 4.9833\n","Epoch 10/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9654 - val_loss: 4.9781\n","Epoch 11/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9633 - val_loss: 4.9755\n","Epoch 12/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9621 - val_loss: 4.9753\n","Epoch 13/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9612 - val_loss: 4.9701\n","Epoch 14/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9630 - val_loss: 4.9810\n","Epoch 15/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9614 - val_loss: 4.9745\n","Epoch 16/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9601 - val_loss: 4.9742\n","Epoch 17/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9594 - val_loss: 4.9743\n","Epoch 18/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9599 - val_loss: 4.9763\n","Epoch 19/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9591 - val_loss: 4.9710\n","Epoch 20/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9587 - val_loss: 4.9657\n","Epoch 21/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9579 - val_loss: 4.9650\n","Epoch 22/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9582 - val_loss: 4.9641\n","Epoch 23/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9578 - val_loss: 4.9635\n","Epoch 24/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9575 - val_loss: 4.9681\n","Epoch 25/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9577 - val_loss: 4.9688\n","Epoch 26/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9575 - val_loss: 4.9689\n","Epoch 27/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9574 - val_loss: 4.9679\n","Epoch 28/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9570 - val_loss: 4.9653\n","Epoch 29/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9567 - val_loss: 4.9631\n","Epoch 30/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9568 - val_loss: 4.9625\n","Epoch 31/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9566 - val_loss: 4.9608\n","Epoch 32/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9565 - val_loss: 4.9593\n","Epoch 33/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9568 - val_loss: 4.9602\n","Epoch 34/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9569 - val_loss: 4.9625\n","Epoch 35/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9565 - val_loss: 4.9601\n","Epoch 36/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9561 - val_loss: 4.9604\n","Epoch 37/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9562 - val_loss: 4.9613\n","Epoch 38/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9564 - val_loss: 4.9619\n","Epoch 39/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9562 - val_loss: 4.9604\n","Epoch 40/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9559 - val_loss: 4.9595\n","Epoch 41/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9558 - val_loss: 4.9590\n","Epoch 42/250\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9556 - val_loss: 4.9586\n","Epoch 43/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9558 - val_loss: 4.9586\n","Epoch 44/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9556 - val_loss: 4.9583\n","Epoch 45/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9555 - val_loss: 4.9584\n","Epoch 46/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9553 - val_loss: 4.9578\n","Epoch 47/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9554 - val_loss: 4.9597\n","Epoch 48/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9553 - val_loss: 4.9587\n","Epoch 49/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9553 - val_loss: 4.9589\n","Epoch 50/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9555 - val_loss: 4.9587\n","Epoch 51/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 52/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 53/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 54/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9551 - val_loss: 4.9586\n","Epoch 55/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9552 - val_loss: 4.9582\n","Epoch 56/250\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9550 - val_loss: 4.9582\n","Epoch 57/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 58/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 59/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9550 - val_loss: 4.9602\n","Epoch 60/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9551 - val_loss: 4.9591\n","Epoch 61/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 62/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 63/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 64/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 65/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 66/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 67/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 68/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 69/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 70/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 71/250\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 72/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 73/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 74/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 75/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 76/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 77/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 78/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 79/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 80/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 81/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 82/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 83/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9563 - val_loss: 4.9630\n","Epoch 84/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9564 - val_loss: 4.9597\n","Epoch 85/250\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9552 - val_loss: 4.9589\n","Epoch 86/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 87/250\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 88/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 89/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 90/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9547 - val_loss: 4.9574\n","Epoch 91/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9550 - val_loss: 4.9586\n","Epoch 92/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9549 - val_loss: 4.9575\n","Epoch 93/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9546 - val_loss: 4.9590\n","Epoch 94/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9545 - val_loss: 4.9585\n","Epoch 95/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 96/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 97/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 98/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9542 - val_loss: 4.9569\n","Epoch 99/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 100/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 101/250\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 102/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 103/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9541 - val_loss: 4.9614\n","Epoch 104/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 105/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 106/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 107/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9541 - val_loss: 4.9587\n","Epoch 108/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 109/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 110/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 111/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 112/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 113/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 114/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 115/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 116/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 117/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 118/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 119/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 120/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 121/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 122/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 123/250\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 124/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 125/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 126/250\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 127/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 128/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 129/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 130/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 131/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 132/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 133/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 134/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 135/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 136/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 137/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 138/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 139/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 140/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 141/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 142/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 143/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 144/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 145/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 146/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 147/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 148/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9544 - val_loss: 5.1202\n","Epoch 149/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9568 - val_loss: 5.1861\n","Epoch 150/250\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9553 - val_loss: 5.1478\n","Epoch 151/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9547 - val_loss: 5.0871\n","Epoch 152/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9544 - val_loss: 5.0633\n","Epoch 153/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9543 - val_loss: 4.9762\n","Epoch 154/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9600\n","Epoch 155/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9540 - val_loss: 4.9585\n","Epoch 156/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 157/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 158/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9540 - val_loss: 4.9590\n","Epoch 159/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 160/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 161/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 162/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 163/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 164/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 165/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 166/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 167/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 168/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 169/250\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 170/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 171/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 172/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 173/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 174/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 175/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 176/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 177/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 178/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 179/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 180/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 181/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 182/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 183/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 184/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 185/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 186/250\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 187/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 188/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 189/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 190/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 191/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 192/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 193/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 194/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 195/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 196/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 197/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 198/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 199/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 200/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 201/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 202/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 203/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 204/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 205/250\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 206/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 207/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 208/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 209/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 210/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 211/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 212/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 213/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 214/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 215/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 216/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 217/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 218/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 219/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 220/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 221/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 222/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 223/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 224/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 225/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 226/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 227/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 228/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 229/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 230/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 231/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 232/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 233/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 234/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 235/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 236/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 237/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 238/250\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 239/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 240/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 241/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 242/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 243/250\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 244/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 245/250\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 246/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 247/250\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 248/250\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 249/250\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 250/250\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9526 - val_loss: 4.9568\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 43527 0.5\n","The shape of N (5464, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9941500650373108\n","=======================\n","Saved model to disk....\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.9966796741288424, 0.9927911275415897, 0.9971178202231806, 0.9877147942767166, 0.9804682686383241, 0.9953481207640171, 0.996799479701513, 0.9850927637434106, 0.9957280755802013, 0.9941500650373108]\n","AUROC ===== 0.9921890189635105 +/- 0.0054788685796927855\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xd8ZFd98P/PuXeK+kra1TZ3e+1j\nG2OKMc2Al2bgAf8SiEMS/CQkoSQQWhLy+6U+P5zwCiSB0AIBAtiUAIYAZgFjDC7rBmbBbd2OvX2l\nVe/S1HvPef64d1R2pbVW0uxoZr7v10svzdx75845Gs393tOVcw4hhBD1x6t0AoQQQlSGBAAhhKhT\nEgCEEKJOSQAQQog6JQFACCHqlAQAIYSoUxIAhFgCrfUXtNYfeIpj/lBr/bOlbhei0iQACCFEnUpU\nOgFCrDat9ZnAz4GPAW8BFPAHwD8AzwR+Yoz54/jY3wb+f6LvwhHgbcaYvVrr9cA3gHOBR4EM0B2/\n5kLgP4EtQB74I2PMr5aYtk7gs8AzgBD4sjHmX+J9HwR+O05vN/C/jTFHFtu+3L+PECVSAhC1agPQ\nZ4zRwEPA9cCbgYuBN2mtz9Fanw78F/CbxpjzgR8Bn4tf//8Bg8aYs4A/A14FoLX2gBuArxhjzgP+\nFPi+1nqpN1P/DIzG6XoR8E6t9Yu01k8D3ghcFJ/3e8ArFtu+/D+LELMkAIhalQC+HT/eDewyxgwZ\nY4aBXmAr8ErgNmPMnvi4LwAvjS/mLwG+BWCMOQDsjI85H9gIfCnedzcwCLxwiel6LfCZ+LUjwHeB\nK4AxoAu4WmvdYYz5lDHmK8fZLsSKSQAQtSo0xmRLj4GpufsAn+jCOlraaIwZJ6pm2QB0AuNzXlM6\nrh1oAh7TWj+utX6cKCCsX2K65r1n/HijMaYHeANRVc8hrfWPtNanLbZ9ie8lxHFJG4CoZ/3AC0pP\ntNYdgAWGiC7M6+Yc2wXsI2onmIirjObRWv/hEt9zPXAofr4+3oYx5jbgNq11M/AR4MPA1YttX3Iu\nhViElABEPfsp8BKt9dnx8z8FbjbGBESNyK8H0FqfQ1RfD3AQ6NZaXxXv26C1/kZ8cV6KHwJvL72W\n6O7+R1rrK7TWn9Zae8aYaeBBwC22faUZFwIkAIg6ZozpBt5K1Ij7OFG9/5/Euz8EnKG13g98iqiu\nHmOMA34XeFf8mjuAW+KL81L8PdAx57UfNsb8Mn7cBDyhtX4E+B3g/xxnuxArpmQ9ACGEqE9SAhBC\niDolAUAIIeqUBAAhhKhTEgCEEKJOVc04gMHByWW3Vnd0NDE6mlnN5FSFesy35Lk+SJ6XrqurVS22\nry5KAImEX+kkVEQ95lvyXB8kz6ujLgKAEEKIY5WtCkhrvZ1oMq5H4k27jTHvXuC4DwEvMMZsL1da\nhBBCHKvcbQA7jTFXLbYznlf9JUCxzOkQQghxlEpXAX0U+LsKp0EIIepSuUsAF2qtdxBNrXuNMean\npR3xzIk7gQNLOVFHR9OKGkG6ulqX/dpqVo/5ljzXB8nzypUzADwJXEO0qMbZRNPZbjPGFOJl8f6I\naGWjU5ZyspV0+erqamVwcHLZr69W9ZhvyXN9kDyf2OsWU7YAEC9kcX38dK/Wuo/oYr8feBnR/Op3\nAmngHK31x4wxf16u9AghhJivnL2Arga2GGM+orXeDGwCegCMMf8D/E983JnAdeW6+E+O53jol91c\n8MwtJFP113dYCCEWU85G4B3A5VrrO4HvA+8gWoz79WV8z2PsM4PcfesejhweO5lvK4QQi7r99luW\ndNwnPvFRjhzpKVs6ylkFNAlcuYTjDgDby5WO0nIHzsq6B0KIyuvtPcLPfvYTtm9/+VMe+973/mVZ\n01I1cwEtl4pnwZB1b4QQa8G///u/8Nhjj/DiF1/KFVe8ht7eI3z845/hQx/6RwYHB8hms/zxH7+d\nyy57Me9619v5i7/4f7nttluwtoAxT9LT08173vOXvOAFl604LXUQAKIIICufCSGO9q1b97Dr8YFV\nPeel52/kjS/btuj+3/u93+e73/0WZ511DocOHeAzn/kCo6MjPPe5z+c1r3kdPT3d/MM//DWXXfbi\nea/r6+vjIx/5JL/4xT18//vfkQCwFFICEEKsVRdc8DQAWlvbeOyxR9ix47so5TExMX7Msc9+9rMB\n2LhxI1NTU6vy/nUQAEozoUoEEELM98aXbTvu3Xq5JZNJAH7605uYmJjg05/+AhMTE7z1rb9/zLGJ\nxOzlerVqNCo9FUT5SQlACLGGeJ5HGIbzto2NjbFly1Y8z2PnzlspFk/O9Gg1HwCkDUAIsZacccZZ\nGPM409Oz1Tjbt7+Me+65k/e+9x00NjayceNGrr32v8qeFlUtF8blrgj26ANH2HnTE7z8ygs472mb\nVjtZa5oMl68Pkuf6sIKpIOp3RbCZEoCMAxBCiHnqIABEv6ulpCOEECdLHQSAUhtAhRMihBBrTB0E\ngOi3k26gQggxT80HgDkRQAghxBw1HwCkDUAIIRZWBwFA2gCEEGvLUqeDLnnggfsYHh5e9XTUQQCI\nfksJQAixFpSmgz4RP/rRjrIEgLqZC0iu/0KItaA0HfSXvvR59u3bw+TkJGEY8r73/RXbtp3L1752\nHTt33obneVx22Yu54IILufPO2zl8+AAf+MCH2bx586qlpQ4CQPRbSgBCiKN9d88PuX9g96qe81kb\nn84btr1u0f2l6aA9z+N5z3shV175m+zfv49PfOIjfPzjn+Gb3/waN9xwE77vc8MN3+HSS5/Ptm3n\n8U//dA0dHat38Yc6CADSC0gIsRbt3v0QY2Oj/OQnNwKQz+cA2L795bzvfe/kla98NVdc8eqypqHm\nA4CUAIQQi3nDttcd9269nJLJBH/+53/FRRddPG/7+9//Nxw8eIBbb/0p7373n/D5z3+5bGmog0Zg\naQMQQqwdpemgL7zwIu6443YA9u/fxze/+TWmpqa49tr/4owzzuSP/uhttLauI5OZXnAK6dUgJQAh\nhDiJStNBb9mylf7+Pt75zrdireV973s/LS0tjI2N8ra3/QGNjU1cdNHFtLWt45nPfDbvec97+OAH\n/42zzz5n1dJS89NBH94/wg+vf4jnvuQsLnnhGaudrDVNpsytD5Ln+iDTQS/D7IqQ1RHohBDiZKn5\nAFBaE1Ku/0IIMV/NBwBpAxBCiIXVQQCQEoAQQiykDgJA9FtKAEIIMV/tBwBPSgBCCLGQ2g8AM1VA\nEgGEEGKuOggA0W+5/gshxHx1EAAkAgghxEJqPgCUyPVfCCHmq/kAIG0AQgixsDoIANFvuf4LIcR8\ndRAA4hKArAgjhBDz1EEAiH5LCUAIIeYr23oAWuvtwLeBR+JNu40x756z/23AW4AQeBD4M2PM6l+m\nJQIIIcSCyr0gzE5jzFVHb9RaNwG/C7zYGFPUWt8KvAC4Z7UTINd/IYRYWEVWBDPGZICXw0wwWAf0\nleO9pBeQEEIsrNwB4EKt9Q6gE7jGGPPTuTu11n8NvBf4uDFm3/FO1NHRRCLhn3ACEl70mnQ6SVdX\n6wm/vtpJnuuD5Lk+rHaeyxkAngSuAb4FnA3cprXeZowplA4wxnxYa/0J4Eat9V3GmLsXO9noaGZZ\niZgczwGQzRZkCbk6IHmuD5LnE3vdYsrWC8gY02OMud4Y44wxe4mqeE4B0Fp3aq1fEh+XBX4MXFaO\ndMwuCVmOswshRPUqWwDQWl+ttX5//HgzsAnoiXcngeu01i3x8+cCpiwJkTYAIYRYUDnHAewALtda\n3wl8H3gH8Cat9euNMf3APxJVC/0cGIqPX3WyIIwQQiysbG0AxphJ4Mrj7L8OuK5c718iS0IKIcTC\n6mgksEQAIYSYqw4CgJQAhBBiIXUQAKLfUgIQQoj56iAASAlACCEWUgcBIH4gEUAIIeap+QCAlACE\nEGJBNR8ApA1ACCEWVgcBQEoAQgixkDoIANFvKQEIIcR8dRAApAQghBALqfkAMEMigBBCzFPzAUAp\nBUqu/0IIcbSaDwAQBQEnCwIIIcQ8dRIApAQghBBHq5MAoKQXkBBCHKVOAgCyJKQQQhylTgKAlACE\nEOJoNR8Aug+MUiyEBAVb6aQIIcSaUvMBYKh/EoAgCCucEiGEWFtqPgAoLx4JXOF0CCHEWlPzAcAr\nBQBpAxBCiHlqPgDIXEBCCLGwmg8AUgIQQoiF1XwAmA4yAFikF5AQQsxV8wGge6oHgGxyssIpEUKI\ntaXmA0Aph1b6AQkhxDw1HwC82SXBKpsQIYRYY2o/APhRFqURWAgh5qv9AFAqAFQ2GUIIsebUfgDw\naj6LQgixLLV/dfQU+XUpqQISQoij1HwAGFCNDDyni0JLY6WTIoQQa0rNBwAb9wKyiZrPqhBCnJBE\npRNQbsUwJLRD0ggshBBHqfnb4p7sk0xlvkc+OV7ppAghxJpS8wGgEBYACBOFCqdECCHWlrJVAWmt\ntwPfBh6JN+02xrx7zv6XAh8CQsAAbzXGrPqMbU4pcGDVap9ZCCGqW7nbAHYaY65aZN/ngZcaY7q1\n1t8GXg3cuNoJ8Iiv/BIAhBBinko2Al9ijJmIHw8C68vxJqpUy6WkGVgIIeYqdwC4UGu9A+gErjHG\n/LS0o3Tx11pvAa4A/uF4J+roaCKR8E84AZ7nQwhOObq6Wk/49dVO8lwfJM/1YbXzXM4A8CRwDfAt\n4GzgNq31NmPMTGus1noj8APgncaY4eOdbHQ0s7xUuNkHg4P1tSZAV1er5LkOSJ7rw3LzfLygUbYA\nYIzpAa6Pn+7VWvcBpwD7AbTWbcCPgb8zxtxcrnR4qlQFFM0IWlojWAgh6l3ZuoFqra/WWr8/frwZ\n2AT0zDnko8DHjDE3lSsNAJ6Kqo2cDAUTQoh5ylkFtAP4utb6N4AU8A7gTVrrceAnwB8A52qt3xof\n/3VjzOdXOxGlBWGcclICEEKIOcpZBTQJXHmcQ9Lleu+5ZquAnCwKJmpCppjhwMRhLlyvK50UUeVO\nuApIa53WWp9WjsSUw0wAQFYFE7XhlsN38ukHv8iRqb5KJ0VUuSWVALTWfwNMAV8EfgVMaq1vNsYc\nt+vmWpCIF4RxUgIQNWKyMAXAWH6crS2bK5waUc2WWgK4EvgP4LeBHxhjngdcVrZUrSJflcYOOFkY\nXtSE0vxWmeIyu0YLEVtqACgaYxzwGuCGeNuJj8qqgFIAkBKAqBUFWwRgOshWOCWi2i21EXhMa/0j\n4FRjzM+11q8DVn3itnLw/bmNwBIBRPWTEoBYLUsNAG8CXgncHT/PAW8uS4pWWcKbHQcg139RC0oB\nYDqQACBWZqlVQF3AoDFmUGv9NuD3gObyJWv1pLzZGCclAFELZgKAlADECi01AFwLFLTWzwLeCnwH\n+GTZUrWKEqUAIG0AokbkrVQBidWx1ADgjDG7gNcD/2GMuZEqmWE/OVMFZKUXkKgJhTBuBC5KI7BY\nmaW2AbRorS8FrgIu11qngY7yJWv1lMYBoMDK9V/UgJlGYGkDECu01BLAR4H/Aj5njBkEPgB8vVyJ\nWk1DuUFASgCidsx0A5UqILFCSyoBGGOuB67XWndqrTuAv43HBax53sxFX9oARPWzzhLYAIBMkJWO\nDWJFllQC0FpfprXeCzxOtNDLY1rr55Q1ZatkZhUxGQcgakCp+geiYJALcxVMjah2S60C+hDwG8aY\njcaYDUTdQP+9fMlaPemZRmCpARLVLx83AJdIQ7BYiaUGgNAY83DpiTHmfiAoT5JWV3pmHICVEoCo\nekVbmPdcuoKKlVhqLyCrtf4toLSo+6uBsDxJWl2+imaskLmARC3Ih/MDgIwGFiux1BLAnwJvAw4Q\nren7ZuBPypSmVTWQ740fyWygovqVxgCkvCQgJQCxMsctAWit74SZxXQV8Ej8uA24DnhJ2VK2SryZ\n4WpSAhDVr9QI3NHQTn9mUNoAxIo8VRXQ35+UVJRRyp87HbREAFHdCnEbQGuqhf7MIPkwX+EUiWp2\n3ABgjNl5shJSLunk3EbgiiZFiBUrlQBaky3AsW0CQpyIE14TuNqU6kqjKiCJAKK6lbqBtqZa4+dS\nAhDLV/MBIOlHAcAhF39R/UpVQC2paDb2gpQAxArUfgDwEoAnI4FFTTi2Cqh4vMOFOK6aDwDRegAK\ncFiZDlRUucJMFVBL/FyqgMTy1XwASMYBwGElAIiqN1MCSEkjsFi5OggASRQeUgIQtaA0FXRTohFP\neRIAxIrUfACYVwUU2konR4gVKZUA0n6KtJ+aaRQWYjnqJAB4URVQKCUAUd1Kd/wpP0XKS0kJQKxI\nzQeAlJ9EGoFFrSjd8af8FOlESsYBiBWp+QAQtQEoopHAUgUkqlupCijpJUh7KRkHIFakLgJA1AvI\nYeX6L6pcYEMSysdTHik/TSEsyvgWsWw1HwBSfjwQTBqBRQ0IXYgXr3KX9lM43MzYACFOVO0HAC8F\ncRWQlABEtbPO4qvoa5vyUwDkA2kHEMtT8wGg1AYQVQFJBBDVLXQWX82WAAByEgDEMtV+AJjTCygM\nJACI6mZdiBeXACQAiJWq+QAwOxDMUrBVsYyxEIsKrZ0TANKABACxfEtdFP6Eaa23A99mdhnJ3caY\nd8/Z3wB8DniaMeY55UpHQvnMBIBAAoCobnZOFVAqnuo8Hxbq4FZOlEPZAkBspzHmqkX2/RvwAPC0\ncibA9/yZuYDy0gtIVDnrwniCw9kSQD7IQ6qSqRLVqpL3DX8LfK/cbxIVl6OV4QtBUO63E6KsQjdb\nBZSSNgCxQuUuAVyotd4BdALXGGN+WtphjJnUWq9f6ok6OppIJPxlJiMKACoBXV2tyzxHdaq3/EJt\n59lhSSWTdHW10jW9DoBcUKjpPC9G8rxy5QwATwLXAN8CzgZu01pvM8Ysa+z66GhmxQmayOUYHJxc\n8XmqRVdXa13lF2o/z4ENcRYGByfJZ6I2rVyQr+k8L6TWP+eFLDfPxwsaZQsAxpge4Pr46V6tdR9w\nCrC/XO+5kGJuiLTnCCwUpReQqHLWHdsLSAaCieUqWxuA1vpqrfX748ebgU1AT7nebzHZiSdJxrkM\nZCCYqHLhQiOBZUI4sUzlbATeAVyutb4T+D7wDuBNWuvXA2itvw18M3qob9dav6kciVAqEbcAQNHK\nnCmiejnn5nUDnRkIVpQSgFieclYBTQJXHmf/b5frvedS3pwAgMyaKKqXjaczl5HAYrXU/PARpRIo\nFYUA66QNQFSvowNAU6IJgInCVMXSJKpbzQeAofFivCBMNIxeiGoVxjcwpTaAxkQDaT/FcGa0kskS\nVazmA8DhodxMJgMlJQBRvUolgFIbgFKKjnS7BACxbDUfAHw/SanqP5Q2AFHFwqOqgAA6GtqZKkxL\nTyCxLDUfAA4NWXqPRJNmWRkHIKpYqQpoXgBIR6OBR3NjFUmTqG41HwAGDvTjbJRNKyUAUcVmG4Fn\np0TpaGgHYDQvAUCcuJoPAMnsBM7FvYCUNAKL6jXTBuDNLQHEAUBKAGIZaj4A4BylyeBKdahCVKNw\nphF4fhsASAAQy1PzAaDBZqFUAkACgKheoS21AcypAiq1AeTHK5ImUd1qPgCkVW4mADglbQCielkp\nAYhVVvMBIOnC2QAgjcCiih09EhiiCeFaUs1SAhDLUvMBIKXcnAAgVUCieoVHDQQraU42kg9lPiBx\n4mo/ACRml4SUEoCoZguNAwBI+kmZ6VYsS80HgIZUQtoARE1YqA0AIOknKFpZ71qcuJoPAI2NiZlx\nAHL5F9VsoYFgACkvSSABQCxDzQeApuammRIA0gYgqtjRs4GWJP0k1tmZbqJCLFXNB4CWtnVzGoGF\nqF4zJQDv2AAASDWQOGE1HwCaOjfMrAcQjQoWojot1gso6UcL+0k1kDhRtR8AOjbglWqApAwgqthC\n4wAgagMAWfNanLiaDwCptnZm75ckAIjqNTsVhFQBidVR8wHAb12Hii/8Sj3FwUKsYUevCFYiVUBi\nuWo/AKTS+PGVX8k4AFHFFpoNFGargCQAiBNV8wFAKTWbSVkPQFQxe5yRwCBVQOLE1XwAAPBn6v4l\nAIjqtdCawDA3AEgjsDgxdRUAHI5iUe6SRHWaXRHsqJHA0gYglqkuAsBsFZAjl5EviahOi44E9qQK\nSCxPXQSAUgnA4piazFU4NUIsj7VSBSRWV10EgATRnZH1HJNThQqnRojlWWwkcMqXXkBieeoiAKSC\nqAQQqpDxjCycIarT4r2AojYAqQISJ6ouAkBDHAACL8/EtBSTRXVabCqIpEwFIZapLgJAKowGgoWq\nwFRWSgCiOi06EEyqgMQy1UcAcA4XJLAqx2R2db8kI/kiocwyKk6C2V5AC08FIVVA4kTVRQBIOIcL\nUjjyZPOr9yUZzBb46EMHuHdgfMH9Y/lxJgqTq/Z+or5JFZBYbfURAHC4YgpUnulVXDVpJF/EAX2L\nNCx/+oEv8vmHvrJq7yfq26IBQAaCiWVKVDoBJ4OvHBTToBx5tXptALkw+kJOLDC62DlHf2aQhkR6\n1d5P1LeZKiDv6CqguAQQSgAQJ6YuSgBeaHFBCoBCYvUGguXC6As5WTy2VJENcoQuJFPMzty5CbES\nTzUbqLQBiBNVthKA1no78G3gkXjTbmPMu+fsfwXwz0AI3GiM+adypSXpiKqAgGIiu6TXjOcnSPsp\nGhINix4zUwIoHPvFmypOAdH8Q5lilpZU84kmW4h5nroKqDraAL65txcH/N45WyqdlLpX7iqgncaY\nqxbZ90ngVUAPsFNr/R1jzKPlSMTpzQGu2AJA4D91FVBoQ/727g/SkW7ng5f97aLHlQLAdBASWoc/\nu/YkU8XpeY8lAIiVqpUVwfZMZMiHltC5mbU6RGVUpApIa302MGKMOWyMscCNwMvL9X5bt7Tjh3Gs\nS+QohMdvCB4vTAAwmh9jPD+x6HGlAAAweVQ7wGRhGk+1olQL08XMMlMuxKzFVgSrpgVhnHPxxR9G\nctVRYqll5S4BXKi13gF0AtcYY34ab98MDM45bgA453gn6uhoIpHwj3fIonJnnkXLkUNkAefnyHoe\np3S1Lnr8yNDAzOMnMob/derLFjxO9QzPPPZb0nS1z97lq4mA5qbX4lwev8nSdZz3K6dKvW8l1Wqe\nE6nofm1jV9u8EqVzDoUCv3L/Z0tViC/+ALmUv6L0rvW8lsNq57mcAeBJ4BrgW8DZwG1a623GmIVm\nY3vKcuDo6PLvojde8iLa77yOLBDYPPc+cJh1zzr9mOOmitP8/Mgu1qXbZrbdvvdentl2CbuGJrhk\nQxtpf7bQNDan++ehgQlai7MlggNDo3jeKTjXRM/gEGemTv54gK6uVgYH62scQi3nOZuLvjqjwxmy\nidn/ta6uVhJegkw+v2byPlmYoiXZjDqqimduSXlP/zin+cu7qavlz3kxy83z8YJG2aqAjDE9xpjr\njTHOGLMX6ANOiXcfISoFlJwSbyuL9LoO1gdR42+BPHt29Sx43M7Dd3PD3hvZ2X3PzLYDE4f43CO3\n8MNDg9x8eH4S51YBHd0QPJSLqpmU8hnJz294djJyWCzDYiOBAZJeYs1UAQ1lR/jru/6Rrz/+P8fs\ny8/5zgxkZWbeSitbANBaX621fn/8eDOwiajBF2PMAaBNa32m1joBvA64uVxpAdgYZrH5BoJEHyON\nfUxNRN1Bi9Zi4wtyz1QvAAcnDgPwpxf/IetSreybiKp6HhoZmXfOfDDbljCcm19CGSvM3vmMFqK6\nTuscH9t9gB0HBxHiRC3WCwjWVgDone4D4J7eXeSC+Z0u5t40DeQkAFRaORuBdwCXa63vBL4PvAN4\nk9b69fH+dwDfAO4ErjfGPFHGtHBac0BhzzNxTtG39dd86547Gc8X+fAD+7n1SHRhLwUAFy8gc0bb\nafzZM9/K1pZtAEyHDUzNKcLO/Wd+YGjvvPebDpIzjycKdub4wVwRMz6NECfqeAEg4SXXTC+g/JyL\n/i96fzV/35zvzGC2MHPzJSqjbG0AxphJ4Mrj7L8DeEG53v9oW885h/b7k0wevoDkGY+wm/vw9m8h\nG6bZN5Eht6mFodzsHb6vfFqSzbSlWmlNB4wUc4Dirt6DPL2ziVNatpALLS0JmApgsmgJbTgzSjNv\nG2daNkqrUE7FA8bGCgH50M5rTxDiqYTO4invmHp1iEoAc7seV9JUMFsa/mX/fWw/7bKZ53NvmgLn\nGMsHdDYkEZVRN1egrktexNneAEH/qXjhFgqpYR7u3wfA4ckce8f75h2/LtXGob0jDPVPMpovooj+\ncW/r2cvnHvoyzjlyoaUwlIfCNJ63ge6p6ByhdVhawEVVP9m4C+rc0oPUf4oTFbrwmFHAJWupCigz\np9vzkanemfELMFsCWJ+OLvoDOZmevZLqJgCk1m3l9OYRQNHRvRnwyNkHcC4kVHDtY7dEx3lJEoU0\nhSGPb9x6M5/92XeYKIZ0NYC10yhvE8O5EYZyEzjACxzp0RxKpXhoOGok7s/mQfkk1RAARRv9s0/P\naTPor4J1CQ5Ndq+ZagURVQEtVP0Da6sKKFOMOj2c2rKVog3oz8y2eZVKAKe1RCPs5UaosuomACil\nOP+0ZtpSOQaH1pPOtuLcFGHfj2BoN9nC4wB02A3oB17G6Y8/h3zDFH1b9zMx9Q06U44gPILnNeF5\nHewdOwSAF1ia4//vPePR9A8Hp6Lfzf4UuAIhjcDRAWBt/+P3TPXyL7s+yc8O3l7ppIhYFAAW7jaZ\n9BKELlwT805NxSUA3RG1nXVPzfaeKwWA0yUArAl1EwAAzrj8TVzQ2Es+TLC5vxMcTLf0M57+BaBQ\npBmw/eQapnA+dPWeA87h3BRjucfwXHRHn/C3cmAiCgAqcDSMRv/EQ/kEzjn2TUR9dTtTjoTKg2oi\nCIOZNgCAu/vH2Nk7Mm8xmYHMIA8OPgxUvqtob1ydtXf8QEXTcTyhddzRO8oPDw3WRWNiaBevAkqs\noSmhM3EbgO48F4DDk7PdrvPxKPwtjWl8paQnUIXVVQBINbdzXsMoaT/g0MhZtA2cggp9WsY2sH7s\neTQ0PB/nWfY94z72PX+Yvmd6OBdV1ewZ+TWeHcC5AGsn2dW3iyCcYLD5x0w39ZIoFLBqPff2PcTh\nqQzOhVzYuZm0V0SpJPsnh5k6vFTAAAAYUUlEQVQ+atbQn3QP89jo1Mzz/378f/j87q+wb+wAf7Hz\n77nnyK6T+veZayQ3BkRf3oWCkXOuLEHqsw9dx38+eO2Sjv3G3l5u6h7inv4xeqbXfpXaSllnj9MG\nEFUzFtbAhHCZYgZf+Zyz7gwAuiePLQE0Jny6GpIMZAsVv9mpZ3UVAAAue9M7eVZ7N5kgxRl9LZzx\n5PPJNk+zYV8znX1bSacuxblp8oVfMZK8GZQDfEJXYLI4wMTUdQThIXJhhtzEDygmRjl07n14Iwal\nkuw48CQTQYLQjnDh+m2c2RrNQnpPXw9TcRXQy7d2cv66aCj/A8NRaeHg5Ch7xw4A8KuBBynYIvf2\n/eqY9Ftn+Z8nd/CTA7eW9e80ko8CwFRxmrH8sSuefeL+z/GZh760qu8Z2pDdQ4/y8PBj9E0PHPdY\n69y87rR7J2p/vqXwOFVArcno/2kst/DqdCfTdDFDU7KRhkQDXY3rOTTZMzMfVqkRuMH36GpMUbCO\n8QVm0xUnR90FgFS6ibM6mmlKFnkgOI3L+3fS2nQFQxd3kh7KccZ9mzh938tpnNpANFM1tE8/m5bR\nTSTzDXi2gVTyIiBB6JcuOo7Bxnux4RjOvxDwSKsp2tPruHzLGThn2TdlmS4GKOClWzv5/XO3sLEx\nhRnPkCkGfOHRe2fGHxycOIyyHvvGD5IN5q9f8I3Hv8Nth+9ix76bCG1ILsjxrSdumBnDsFpGc6Mz\nj+cW4QEKYYE9Y/sxI3vm9fBYqeE577mr//7jHjtRCAgdnNMSta/sqYMAcLxG4C3N0cD63un+k5mk\nBU0HGZrjgPT8LZeSC3N89bFvzfScA0j7HhsbopujI4usqCfKr+4CAMArr/oddOcohTDB9zZs5zd/\nsYPNUwMMP2sD/S/Ywvgzziax6VWADw42PdHOmU9egn7wZZz/68tpcheSTl0MCpRNRf39PUdx+jaC\ncJDpzE0ExYf42sPf4+H+R/BcH0W3jvFCQFPCx1OKXw1NxBcxx229o0zk982kr39ihAt+/Uo6jpyB\nGdkzU0Tune7nnt7ZaqHDUz3cdOBWdnbfww/33cw3nzjCP9+3l12D4ysuVg/NuRjf9eBDfO9r98+c\nsz8ziMMRupCh7PBipzhhg3POtavvPqyzjI1ksPbYvAzno6qO6QNjbGlKc2gqR9FWvgG0nEIX4nuL\nBYBNAPRVOABYZ8kUszQnosB8xRnbOa9jG7uHHuWR4cfJhxYFpDzFeXEp+P7h+prTZy2pywAA8Pb/\nfRVndk7TP93Cl72X0TDQg775JrY98Euu2PFFzt79MLbvEuh/OmPndjG5tYmpzY04z7L51xlaMueh\nSIMHKXU+zd4LUY1Pp1DYTRAeZqIwwM8Hfs4PDv2AicxNFIvdjBWKNCd8rHP87OAQudCSUHB33xBB\n0E1TsgNPNZMPc3jOp+vIOfxwz3185tHDTBUD9sVVRNuaz8ELE/yy735uO3wnAI8MP8lD49NMhZbv\nHRjgtt6R4+T+qY1kx1Aq+hIfmuqmr3ucof6ovWLuXWZv5vhVNSdiMBs1sie9BMO5UX5pHuYbn/8l\nN9/wyDEB7Ug8OWB+IMNpqSSBc2uiGsg5x48ODfKrwdWvijleL6DNcQCodAkgF+RwOJqSTUA0avkN\n214HwA/338zBiT48FaKU4tTmNJsbUzw2NnXMdOri5KjbAJBOJvi7t76Wi073yQc+d41tY6f/NH45\nvIGRfDMvvPdWukyR7KFTGMqHDJ7aTHdbksPP2MTk6S2sfzxD2+R5OAoU2U/B68O5IqEdABI0es/C\nU+vA+Vgbksn+mKnMT+md3seHfvV1JuPuekFgCQp9QMAFneezuXkT1g8oJnN4pCnuy3Fg/DG++mQv\ne8YO0TF4Ks13nc85D1/GHQd/QeBCzl53JqniaQA09mdIFSw/6xnhrsNDy/rbZIMsBZvH9zbgexuY\nbB0kn57mwJ7oDn1u/fxCd5yP7+7l0/99A19/+LsMZ0eP2b+YwUyU3t85L5ot5JcPGgD2PzHErjsP\nzDv28FAUjBLZgM7x6OJxV9/Ykt+rXIZyRe7uH+Om7qF5PbxWQ3icRuC2VAvNiaaKB4DpeAxAcxwA\nAE5r3cq29rM4PNmDxSe0OayzKKW4tGsd1sHP+yv/2Z2o/szgTLtdtarbAADgex5/8abLef3l53Dx\nln4uP/sgKqH4YedlfP/iV/G85v1cZA+g9vby9Efv4BXDt/GMgXtoaBii/wWbSDY9ndbJbajQoxjs\nI5e/C+dyNNuLSTU/h9aWN7Ku+c20pF4DKkUQHiSX+TWj2UacszQOZnFegC0con3sQrqHNhEGW2jI\ntHPgWT0cflEr4RkvxuV62T++l30HGzll/8WEgSOdb2ZT93no9m285aKraQyjPtcbJop0/nqQROj4\n6sOHeHzsxKcHKPUA8r2WmaquoS0H+PXuJxjLTsy76B/dWOuc43tP3MijW+7h7oFf8NmHruXunnt5\ndDi6mI/mxvjYff/JL/vuO+Z9B7PDrBveQlPfRi7s0BQHklhl8Rot9//iEGMjs3f4A3Gvn0Q2YHLf\nKNvaGtk3mWXf2DShDRkbn2SfGVyw+qicnogbpjOBZd8ql0isCxdtA1BKsbl5E4PZYYph5XoCTQdR\n/psTTfO2v/L07QAolSa0OQ5NdgPwrA2trEsluKN3lENTS1uudS2wzvKZB7/Ex+//LKO56gteJf4H\nPvCBSqdhSTKZwgeW+9rm5jSZzOL9jc89tYNtZ25jstDEee2Giazi4GQHj7lTGVDtFEhyoLCB8WQr\nWzqzrGeE1r37OW/kUTYczhHu2Yo31UDKL5C0TWwdfxrp/gJe3mLTCVSyjbbJ9eRSE4QMEdhugvw+\n/IkepryHKKhucg2D5P0sU8X9BMkJQoYpBI9j/Rx++nQSYRPpiSb8Akxu3ovn2mie3EA41MnPux8l\n19KOSzQw3tTP+mmf5OEs05sa2T08yVT3EGM9k9xxx366bY57RnZyYHwfe8cOsGPvLTwy5mhMtLG+\nIYlSin1jh7hv4EHaG8/mtMzZDHl7yTYNElif+/cqJkcmCJtHUaqZgDZetPVihvNFUp6iu3eImyZu\nJBGkaRvdxGCyl91Dj/HrgQc5vfUU7j5yLw8OPsLuoUfZ1NTFhsb1M/Mn3Xr3A2zccwGH945hD7eQ\nzjWT3ZChZ9NjtI5sZHRomsbGFA/u6saoEJv0OHMsYKh/iuc+51QensxwX+8It+75Kjcd/Bl9uzyG\n92fZelo76YbjT3s1MZbl7lv20tKWprklvdx/NW7pGWEkbp9IeooL2qOlSDPTBZQCbwXzP9144Ge0\np9dx2dbnztte+v8+MHmYQ5PdXLBe09nQMbO/dLe9FAOZQb7++HfZ0NhJe3rdCaexZ6qPXf33c9GG\nC9jWftbM9o1NXbxwy6XcM5DD2glSqofzO88l4XlsbUpz3/AkD45MEjpH0vNoSUZtZYt5qu90uT00\n9Ch39vwch0MpxQWd55X9PZeb5+bm9DWL7VPV0gd3cHBy2Qk90YUUioHllnsf5MmDvTS5Uc5r7eOu\nntPYM74ed9TaNb5nOW3dBGkvwAEdzXlagwyFDLicBWtZ5zJ0etO0+jmG8Hloc8iB9XZmsri2XBPT\nqYDQK6BcgqR3OulMA9nEXoLk3B4SSZTyUaRAJVEqiXMFrJ1f39+oIFlIExQ6KbSlsG4K5/J4XisK\nH89bhwszFO3+2Xz4W/BoRoUhnkvgXAHwSIdNFFNJQJEoRH3NlU0ARaxfxHkJUB64EKwFZ6PHLsT6\nbmY+JOUSOAXZxBMokjiKoBzK+aSKXfi0Yd00oPBUGkuBYnIcL7mZYnAAKJII15EqthP6AZ7fjkcT\nqSCkqKbJe8P4LhpdmktE03kr1UKDOp/GqRamUrsBR3NwLspP4oXg+RD4OQKVwxUsqphA+SlQIY4c\nzi+AHzX0J71GPOXjSstZpUF57ajAxxFilcLPWTLtHfhhDqsUVimSbgRbHEdNN+PThktnsYk8Sa+D\nhG9xYTYabGiLhGEBRQIVeJBUqESSMEwS2BAIGOcekqxjY+4FkEvgGixhg8X3CyhryahBRt0jgEej\ndwpJtY6sPULRjZBUndHf1qZJhK0kEml8rwEv0UTCD1A2wIYwGWawVuF5inUNHSjr4WwAymGxeCpa\n0SuwXtT3QXk4W0B5ELo8+WCa0Pm0pNpoTDTHn6dHISgwmZ1AtZ2Py49SmLyTZDpBGIQ0h224hi0E\nzeeBl4x6w9kAFUyiXB5cgEJFf5voA8GqIoVwkNCNk1QdNHhNeF4KBTgXAh6e8mdKTNY6bOgICfF8\nD+V5lOb9tU6B83AqgXMhKiygChZbBD/pQ6KI8kGpBJ6yWBTZoEDoLAoPp6Ah0YyPit/b4XuAUijr\nEyiHw+KhojQ5BRYcFqsszgOFwwUWhwfKI0qew4YKTzledc65XPnMZy53QZhFI6kEgCVyztE3kmHf\nkTEmJ8cY6nmUIyOO0Xwj/ZkmlrCo2TyqcQK/bZRwohOXbQVlaW2ZpAlFIUhSCH0SiYDkuiFU0zi2\nYZpCshB1TPWKOM+CCkFBQ3YjniqS8PP4ypH1iuS8OY1qzsNzSaw3v7udsq0kggvxeJJ8amWNxici\n6H42drqBxKZ+vLYB8Bb5bJyKg0QKpdqwDANP/W+gaKDBbSKrDq5uwteAhH82hb3PID+QpfGUZhq6\nmkg0J/DSUSmqGOwll9+Fc7MDDD3VhnWLr20t1r7GQPPlq98nAWA5yr183HSuSOnPODiWZWQ8i0eG\nQn6MzNQYExN5BsYsg5PF6I7BWjI5RS6naEnmSCYtOVJMFNPkwiRpPyDlhRRDj1yYpOgWWTZPhZAo\nQrHh2H1eEA9ic2AT4DzAgmdRjVMQJnC56A4NQKWy0fHKgV8E64NTqHRcL6vszGMFUUnIMfP6uY8V\nRBdvp3DOw8V3WDiFC5KoTBtNDSFT2bhEkZ7GaxnHTreB81DpqO7cTrXjNU9gM60QJiGRx2ucwgVJ\n/I5B/FQO5YcQJmDoFEgE0DiJN92Ky7YReEWS64bwWiZQ0+24YoqwbQiVLESp9KI7S8IkyiZQyuEp\nC8rhgjQu14zyQkjlcMn47wMo5+EcqEQepaL1eKNdHh6KhIvKicopnEugwgaKqSlsooAfJPELDQSp\nLKFnUfF7W+WwgFI2ukNUDvwAp0JU6IP1SWTamezfSraQxleW0M2vTlI4PA88ZfHWDeElCnj5Jvxc\nK84LyBSSJFrHSDZNR3e1iSJ4BVAeqnSu+MZCAS5RjP7HiParKFcz/yYq/rCd8wCFV0zhF9MUU1nw\nLE5ZUBY8F6XNJnAqJJnM4flh/H8Sp9yBsyo+YfQ8VA7fJvCtj1XgVHR3HX8KJMMG0vk2phsmsH6I\nVSGeA+V8Qu/YXkVq7m83+zz+UAGLQ2FR0b0H4FkPnB8f67AzL3ZxWuZ+/qVPgfhvRfxNifJT+sOp\nmYcq+h6pKE++9aPu1crilIu+fwo8p+iYOp1/fc/bVj0AlHtR+LrQPGc+85bGJGdtaTvO0SeuGIRM\nZouMjGVZ16DIj4+SyebI5ItkRscJnSJ0PmGxSCE7RSGfJ18skk81kwnABnnCMJrS2k/7pGyW0Ia4\nlhH8MESFIaHzKFovGtvgz17UM8UUeZUgJIEtthFaj9B5pL2AhBdGz+MLQ/SVUPFP9NhXloTn8JXF\nw9KYynDp1ifY0Jiht9jG4UwH49k008UkZ2zqJRckmCym8ZSjsaWbYtEjbMgQOJ+i9Sg6n4IKKU5s\noGj9eRdBFyYh3xh9P/0CCgjGuiiMbI6+SMqRnGrDOUUYf/kCp7BOEVrv2Oq9BS6yldaQCLjk1F6u\n0AfYO9RO/1QzQ9ONTOeTBM4jtFFeglwboVUE1qNgPWyQYkNjnlyulemJTqxT2DWWN7G4LW2Hy3Je\nCQBVIJnw6Wz16WyN7/S72pf0urW+cLZexXM558gXA7Zsbmd0ZBprHaGNS0AQ3WE6SxgGFIMiYehI\nJT1cWCCfzVHMWfK5PLYYENoQz1k8Bc4GhEHUrmFDSxCGhGFIUwLyYUDWJrG+jwUKxSB6o3jCMxeG\nBGGIchZrQwp4KAUpW8QWClhbRHng4fBdgI/FuagppeAlCFHYMMTPWygUSSUV6xoVDQ0Bhe5mTrF5\ntrg8NEPQ4FDOEYYWZwGrCONGVGUt1vNwyiPOVHRH6hwWFyXXKUi46BgHFrBWEVqw1sMphYcldIqA\nOFhasIHD8+IfBUoBzqKsiwOrwikPqxShH52nkFXYYDbYujmPFOB5DuUBXlSAwMW1gV50J21R4EUl\nD89aVOnvpuIbGAVORXfyNjoQ5aJbfjdzL196SzVTenfRaXGK6CRRYuI2idmSw0LVkM7FeZ+bk7iQ\nXNpsUYTOwzqFK52f0utK5ZooDZ6KbqOiQA0XdpSnpkYCgKgJSikaUkkScS8bz4saMufzgSTE03OX\ntK5uge2kW+uBvhzqMc/lIGVAIYSoUxIAhBCiTkkAEEKIOiUBQAgh6pQEACGEqFMSAIQQok5JABBC\niDolAUAIIepU1cwFJIQQYnVJCUAIIeqUBAAhhKhTEgCEEKJOSQAQQog6JQFACCHqlAQAIYSoUxIA\nhBCiTtX8gjBa648Bzydacue9xphdFU7SqtNabwe+DTwSb9oN/CvwVaJVUHqB3zfG5Bc8QZXRWl8E\nfB/4mDHmP7TWp7FAXrXWVwPvI1rg6vPGmC9WLNErtECerwMuAYbjQ/7NGPOjGsvzvwIvJrpOfQjY\nRe1/zkfn+f+hjJ9zTZcAtNaXA+caY14AvAX4ZIWTVE47jTHb4593A/8IfNoY82JgD/DHlU3e6tBa\nNwOfAm6Zs/mYvMbH/R/gFcB24M+11p0nObmrYpE8A/zNnM/8RzWW55cCF8Xf3VcDH6f2P+eF8gxl\n/JxrOgAALwduADDGPAZ0aK2rfAHAJdsO7Igf/4Don6UW5IH/BRyZs207x+b1ecAuY8y4MSYL3A1c\ndhLTuZoWyvNCainPdwC/HT8eA5qp/c95oTz7Cxy3anmu9SqgzcCv5zwfjLdNVCY5ZXWh1noH0Alc\nAzTPqfIZALZULGWryBgTAIHW85aUXyivm4k+b47aXnUWyTPAu7TWf0GUt3dRW3kOgen46VuAG4FX\n1fjnvFCeQ8r4Odd6CeBoR68SXiueJLro/wbwZuCLzA/utZrvhSyW11r7G3wV+GtjzMuAB4APLHBM\n1edZa/0bRBfDdx21q2Y/56PyXNbPudYDwBGiaFmylajxqKYYY3qMMdcbY5wxZi/QR1Td1RgfcgpP\nXX1QzaYWyOvRn31N/Q2MMbcYYx6In+4Ank6N5Vlr/Srg74DXGGPGqYPP+eg8l/tzrvUAcDNwFYDW\n+tnAEWPMZGWTtPq01ldrrd8fP94MbAKuBX4rPuS3gJsqlLyT4Wccm9d7gUu11u1a6xaiOtI7K5S+\nVae1/o7W+uz46XbgYWooz1rrdcC/Aa8zxozEm2v6c14oz+X+nGt+Omit9YeBlxB1l/ozY8yDFU7S\nqtNatwJfB9qBFFF10P3AV4AG4CDwR8aYYsUSuUq01pcAHwXOBIpAD3A1cB1H5VVrfRXwV0RdgD9l\njPnvSqR5pRbJ86eAvwYywBRRngdqKM9vJ6rueGLO5jcDX6B2P+eF8nwtUVVQWT7nmg8AQgghFlbr\nVUBCCCEWIQFACCHqlAQAIYSoUxIAhBCiTkkAEEKIOiUBQIiTQGv9h1rrr1U6HULMJQFACCHqlIwD\nEGIOrfW7gTcSzaX0ONG6Cj8Efgw8Iz7sd40xPVrr1xJNy5uJf94eb38e0VS+BWAE+AOikatvIJqI\n8EKigUxvMMbIF1BUjJQAhIhprZ8LvB54STwn+xjRlMNnA9fG89DfDvyl1rqJaFTqbxljXkoUID4Y\nn+prwNuMMZcDO4HXxtufBrydaIGPi4Bnn4x8CbGYWp8OWogTsR3YBtwWT73cTDTR1rAxpjSt+N1E\nKzGdB/QbY7rj7bcDf6q13gC0G2MeBjDGfByiNgCiOdwz8fMeoqk7hKgYCQBCzMoDO4wxM1MPa63P\nBO6bc4wimn/l6KqbudsXK1kHC7xGiIqRKiAhZt0NvCaeYRGt9TuJFtro0Fo/Kz7mRcBDRBN2bdRa\nnx5vfwXwC2PMMDCktb40PsdfxucRYs2RACBEzBjzK+DTwO1a67uIqoTGiWbf/EOt9a1EU+9+LF6K\n7y3A9Vrr24mWH/37+FS/D3xCa72TaCZa6f4p1iTpBSTEccRVQHcZY06tdFqEWG1SAhBCiDolJQAh\nhKhTUgIQQog6JQFACCHqlAQAIYSoUxIAhBCiTkkAEEKIOvV/AYni9A6d/zl1AAAAAElFTkSuQmCC\n","text/plain":["<matplotlib.figure.Figure at 0x7fe784f64d68>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"Am2Oq9gk9xWX","colab_type":"text"},"cell_type":"markdown","source":["# **RCAE-MNIST 4_Vs_all**"]},{"metadata":{"id":"1PVJpi9l9kt9","colab_type":"code","outputId":"0d0b5d4a-ea95-471c-bf99-76b942cb2cf6","executionInfo":{"status":"ok","timestamp":1541251368279,"user_tz":-660,"elapsed":4124368,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":99917}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/250\n","5318/5318 [==============================] - 5s 861us/step - loss: 5.0706 - val_loss: 5.4471\n","Epoch 2/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9996 - val_loss: 5.0756\n","Epoch 3/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9845 - val_loss: 5.0292\n","Epoch 4/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9765 - val_loss: 5.0561\n","Epoch 5/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9758 - val_loss: 4.9839\n","Epoch 6/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9699 - val_loss: 4.9754\n","Epoch 7/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9697 - val_loss: 4.9931\n","Epoch 8/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9691 - val_loss: 4.9755\n","Epoch 9/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9656 - val_loss: 4.9713\n","Epoch 10/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9637 - val_loss: 4.9684\n","Epoch 11/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9627 - val_loss: 4.9680\n","Epoch 12/250\n","5318/5318 [==============================] - 2s 288us/step - loss: 4.9617 - val_loss: 4.9667\n","Epoch 13/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9613 - val_loss: 4.9663\n","Epoch 14/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9605 - val_loss: 4.9658\n","Epoch 15/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9597 - val_loss: 4.9650\n","Epoch 16/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9597 - val_loss: 4.9645\n","Epoch 17/250\n","5318/5318 [==============================] - 2s 288us/step - loss: 4.9594 - val_loss: 4.9642\n","Epoch 18/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9603 - val_loss: 4.9687\n","Epoch 19/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9591 - val_loss: 4.9669\n","Epoch 20/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9587 - val_loss: 4.9660\n","Epoch 21/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9586 - val_loss: 4.9650\n","Epoch 22/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9579 - val_loss: 4.9641\n","Epoch 23/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9577 - val_loss: 4.9627\n","Epoch 24/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9574 - val_loss: 4.9638\n","Epoch 25/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9570 - val_loss: 4.9627\n","Epoch 26/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9568 - val_loss: 4.9614\n","Epoch 27/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9566 - val_loss: 4.9613\n","Epoch 28/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9563 - val_loss: 4.9605\n","Epoch 29/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9562 - val_loss: 4.9604\n","Epoch 30/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9563 - val_loss: 4.9597\n","Epoch 31/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9563 - val_loss: 4.9601\n","Epoch 32/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9562 - val_loss: 4.9597\n","Epoch 33/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9557 - val_loss: 4.9591\n","Epoch 34/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9558 - val_loss: 4.9596\n","Epoch 35/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9573 - val_loss: 4.9633\n","Epoch 36/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9560 - val_loss: 4.9608\n","Epoch 37/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9557 - val_loss: 4.9602\n","Epoch 38/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9553 - val_loss: 4.9590\n","Epoch 39/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9552 - val_loss: 4.9589\n","Epoch 40/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9552 - val_loss: 4.9588\n","Epoch 41/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 42/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9549 - val_loss: 4.9587\n","Epoch 43/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9551 - val_loss: 4.9586\n","Epoch 44/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9586\n","Epoch 45/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9548 - val_loss: 4.9587\n","Epoch 46/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 47/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9547 - val_loss: 4.9582\n","Epoch 48/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 49/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9549 - val_loss: 4.9585\n","Epoch 50/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 51/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 52/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 53/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 54/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 55/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 56/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 4.9599\n","Epoch 57/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9559 - val_loss: 4.9637\n","Epoch 58/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9550 - val_loss: 4.9602\n","Epoch 59/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9548 - val_loss: 4.9606\n","Epoch 60/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9544 - val_loss: 4.9584\n","Epoch 61/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 62/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 63/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 64/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 65/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 66/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 67/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 68/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 69/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 70/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 71/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9549 - val_loss: 4.9607\n","Epoch 72/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9544 - val_loss: 4.9595\n","Epoch 73/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 74/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 75/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 76/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 77/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 78/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 79/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 80/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9550 - val_loss: 4.9590\n","Epoch 81/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 82/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 83/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 84/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 85/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 86/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 87/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 88/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 89/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 90/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 91/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 92/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 93/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 94/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 95/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 96/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 97/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 98/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 99/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 100/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 101/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 102/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 103/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 104/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 105/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 106/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 107/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 108/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 109/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 110/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 111/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 112/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 113/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 114/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 115/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 116/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 117/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 118/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 119/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 120/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 121/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 122/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 123/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 124/250\n","5318/5318 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 125/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 126/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 127/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 128/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 129/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 130/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 131/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 132/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 133/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 134/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 135/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 136/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 137/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9642\n","Epoch 138/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 139/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 140/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 141/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 142/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 143/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 144/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 145/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 146/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 147/250\n","5318/5318 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 148/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 149/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 150/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 151/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 152/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 153/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 154/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 155/250\n","5318/5318 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 156/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 157/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 158/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 159/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 160/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 161/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 162/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 163/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 164/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 165/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 166/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 167/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 168/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 169/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 170/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 171/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 172/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 173/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 174/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 175/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 176/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 177/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 178/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 179/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 180/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 181/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 182/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 183/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 184/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 185/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 186/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 187/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 188/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 189/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 190/250\n","5318/5318 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 191/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 192/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 193/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 194/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 195/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 196/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 197/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 198/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 199/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 200/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 201/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 202/250\n","5318/5318 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 203/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9559\n","Epoch 204/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 205/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 206/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 207/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 208/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 209/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 210/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 211/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 212/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 213/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 214/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9561\n","Epoch 215/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 216/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 217/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 218/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 219/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 220/250\n","5318/5318 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 221/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 222/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 223/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 224/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 225/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 226/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 227/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 228/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 229/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9557\n","Epoch 230/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 231/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 232/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 233/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9557\n","Epoch 234/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 235/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 236/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 237/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 238/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 239/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 240/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 241/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 242/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 243/250\n","5318/5318 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 244/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9557\n","Epoch 245/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 246/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9557\n","Epoch 247/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 248/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 249/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 250/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9556\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 34205 0.5\n","The shape of N (5909, 784)\n","The minimum value of N  -0.7487058639526367\n","The max value of N 0.7482286095619202\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9815534037800788\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/250\n","5318/5318 [==============================] - 4s 735us/step - loss: 5.0684 - val_loss: 5.1659\n","Epoch 2/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9960 - val_loss: 5.0123\n","Epoch 3/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9815 - val_loss: 4.9870\n","Epoch 4/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9749 - val_loss: 4.9774\n","Epoch 5/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9701 - val_loss: 4.9742\n","Epoch 6/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9672 - val_loss: 4.9712\n","Epoch 7/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9653 - val_loss: 4.9724\n","Epoch 8/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9640 - val_loss: 4.9687\n","Epoch 9/250\n","5318/5318 [==============================] - 2s 288us/step - loss: 4.9631 - val_loss: 4.9724\n","Epoch 10/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9629 - val_loss: 4.9667\n","Epoch 11/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9611 - val_loss: 4.9653\n","Epoch 12/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9618 - val_loss: 4.9677\n","Epoch 13/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9597 - val_loss: 4.9660\n","Epoch 14/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9593 - val_loss: 4.9636\n","Epoch 15/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9591 - val_loss: 4.9664\n","Epoch 16/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9592 - val_loss: 4.9853\n","Epoch 17/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9599 - val_loss: 4.9765\n","Epoch 18/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9584 - val_loss: 4.9672\n","Epoch 19/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9578 - val_loss: 4.9639\n","Epoch 20/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9574 - val_loss: 4.9629\n","Epoch 21/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9575 - val_loss: 4.9645\n","Epoch 22/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9568 - val_loss: 4.9633\n","Epoch 23/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9600 - val_loss: 4.9750\n","Epoch 24/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9576 - val_loss: 4.9619\n","Epoch 25/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9570 - val_loss: 4.9622\n","Epoch 26/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9564 - val_loss: 4.9622\n","Epoch 27/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9562 - val_loss: 4.9607\n","Epoch 28/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9565 - val_loss: 4.9644\n","Epoch 29/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9560 - val_loss: 4.9609\n","Epoch 30/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9559 - val_loss: 4.9611\n","Epoch 31/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9561 - val_loss: 4.9606\n","Epoch 32/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9558 - val_loss: 4.9599\n","Epoch 33/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9555 - val_loss: 4.9598\n","Epoch 34/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9554 - val_loss: 4.9585\n","Epoch 35/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9604\n","Epoch 36/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9595\n","Epoch 37/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 38/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 39/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 40/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9550 - val_loss: 4.9893\n","Epoch 41/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9559 - val_loss: 4.9716\n","Epoch 42/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9554 - val_loss: 4.9616\n","Epoch 43/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9554 - val_loss: 4.9595\n","Epoch 44/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9550 - val_loss: 4.9587\n","Epoch 45/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9548 - val_loss: 4.9585\n","Epoch 46/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9548 - val_loss: 4.9596\n","Epoch 47/250\n","5318/5318 [==============================] - 2s 288us/step - loss: 4.9549 - val_loss: 4.9586\n","Epoch 48/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 49/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 50/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 51/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 52/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 53/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 54/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 55/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 56/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 57/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 58/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9542 - val_loss: 4.9569\n","Epoch 59/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 60/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 61/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 62/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 63/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 64/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9557 - val_loss: 4.9593\n","Epoch 65/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9550 - val_loss: 4.9597\n","Epoch 66/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9546 - val_loss: 4.9611\n","Epoch 67/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 68/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 69/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 70/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 71/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 72/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 73/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 74/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 75/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 76/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 77/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 78/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 79/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 80/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 81/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 82/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 83/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 84/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 85/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 86/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 87/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 88/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 89/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 90/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 91/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 92/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 93/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 94/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 95/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 96/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 97/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 98/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 99/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 100/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 101/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 102/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 103/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 104/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 105/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 106/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 107/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 108/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 109/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 110/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 111/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 112/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 113/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 114/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 115/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 116/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 117/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 118/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 119/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 120/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 121/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 122/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 123/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 124/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 125/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 126/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 127/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 128/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 129/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 130/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 131/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 132/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 133/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 134/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 135/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 136/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 137/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 138/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 139/250\n","5318/5318 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 140/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 141/250\n","5318/5318 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 142/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 143/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 144/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 145/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 146/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 147/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 148/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 149/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 150/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 151/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 152/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 153/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 154/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 155/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 156/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 157/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 158/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 159/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 160/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 161/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 162/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 163/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 164/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 165/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 166/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 167/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 168/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 169/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 170/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 171/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 172/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 173/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 174/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 175/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 176/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 177/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 178/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 179/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 180/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 181/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 182/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 183/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 184/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 185/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 186/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 187/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 188/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 189/250\n","5318/5318 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 190/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 191/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 192/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 193/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 194/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 195/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 196/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9559\n","Epoch 197/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 198/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 199/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 200/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9561\n","Epoch 201/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 202/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9559\n","Epoch 203/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 204/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 205/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 206/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9559\n","Epoch 207/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9559\n","Epoch 208/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9559\n","Epoch 209/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 210/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 211/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9559\n","Epoch 212/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9558\n","Epoch 213/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9712\n","Epoch 214/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9682\n","Epoch 215/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9603\n","Epoch 216/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 217/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 218/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 219/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 220/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 221/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 222/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 223/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 224/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 225/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 226/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 227/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 228/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 229/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 230/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 231/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 232/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 233/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 234/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 235/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 236/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 237/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 238/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 239/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 240/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 241/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 242/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 243/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 244/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 245/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 246/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 247/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 248/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 249/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 250/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9557\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 36526 0.5\n","The shape of N (5909, 784)\n","The minimum value of N  -0.7499792575836182\n","The max value of N 0.749941885471344\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9905527496036635\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/250\n","5318/5318 [==============================] - 4s 823us/step - loss: 5.0734 - val_loss: 5.2047\n","Epoch 2/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9984 - val_loss: 5.0195\n","Epoch 3/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9832 - val_loss: 4.9943\n","Epoch 4/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9761 - val_loss: 4.9828\n","Epoch 5/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9712 - val_loss: 4.9786\n","Epoch 6/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9679 - val_loss: 4.9740\n","Epoch 7/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9655 - val_loss: 4.9721\n","Epoch 8/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9639 - val_loss: 4.9705\n","Epoch 9/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9626 - val_loss: 4.9694\n","Epoch 10/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9621 - val_loss: 4.9777\n","Epoch 11/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9620 - val_loss: 4.9685\n","Epoch 12/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9606 - val_loss: 4.9759\n","Epoch 13/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9602 - val_loss: 4.9684\n","Epoch 14/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9597 - val_loss: 4.9775\n","Epoch 15/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9591 - val_loss: 4.9678\n","Epoch 16/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9585 - val_loss: 4.9676\n","Epoch 17/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9587 - val_loss: 4.9659\n","Epoch 18/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9583 - val_loss: 4.9648\n","Epoch 19/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9577 - val_loss: 4.9639\n","Epoch 20/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9572 - val_loss: 4.9637\n","Epoch 21/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9571 - val_loss: 4.9644\n","Epoch 22/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9569 - val_loss: 4.9631\n","Epoch 23/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9566 - val_loss: 4.9623\n","Epoch 24/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9570 - val_loss: 4.9846\n","Epoch 25/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9593 - val_loss: 4.9710\n","Epoch 26/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9585 - val_loss: 4.9669\n","Epoch 27/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9570 - val_loss: 4.9642\n","Epoch 28/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9571 - val_loss: 4.9728\n","Epoch 29/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9569 - val_loss: 4.9638\n","Epoch 30/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9564 - val_loss: 4.9626\n","Epoch 31/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9564 - val_loss: 4.9624\n","Epoch 32/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9559 - val_loss: 4.9599\n","Epoch 33/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9557 - val_loss: 4.9618\n","Epoch 34/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9555 - val_loss: 4.9596\n","Epoch 35/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9553 - val_loss: 4.9602\n","Epoch 36/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9560 - val_loss: 4.9627\n","Epoch 37/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9560 - val_loss: 4.9600\n","Epoch 38/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9555 - val_loss: 4.9595\n","Epoch 39/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9551 - val_loss: 4.9590\n","Epoch 40/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9550 - val_loss: 4.9582\n","Epoch 41/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 42/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 43/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9589\n","Epoch 44/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 45/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 46/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 47/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9561 - val_loss: 4.9597\n","Epoch 48/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9554 - val_loss: 4.9586\n","Epoch 49/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 50/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9552 - val_loss: 4.9609\n","Epoch 51/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 52/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9547 - val_loss: 4.9574\n","Epoch 53/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 54/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 55/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 56/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 57/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 58/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 59/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 60/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 61/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 62/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 63/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 64/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 65/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 66/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 67/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 68/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 69/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 70/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 71/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 72/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 73/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 74/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 75/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9859\n","Epoch 76/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9545 - val_loss: 4.9682\n","Epoch 77/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9632\n","Epoch 78/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9593\n","Epoch 79/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 80/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 81/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9545 - val_loss: 4.9589\n","Epoch 82/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 83/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 84/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 85/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 86/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 87/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 88/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 89/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 90/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 91/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 92/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 93/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 94/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 95/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 96/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 97/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 98/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 99/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 100/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 101/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 102/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 103/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 104/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 105/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 106/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 107/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 108/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 109/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 110/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 111/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 112/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 113/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9601\n","Epoch 114/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 115/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 116/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 117/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 118/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 119/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 120/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 121/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 122/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 123/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 124/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 125/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 126/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 127/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 128/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 129/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 130/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 131/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 132/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 133/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 134/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 135/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 136/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 137/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 138/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 139/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 140/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 141/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 142/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 143/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 144/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 145/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 146/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 147/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 148/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 149/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 150/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 151/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 152/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 153/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 154/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 155/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 156/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 157/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 158/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 159/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 160/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 161/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 162/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 163/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 164/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 165/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 166/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 167/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 168/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 169/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 170/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 171/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 172/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 173/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 174/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 175/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 176/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 177/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 178/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 179/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 180/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 181/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 182/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 183/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 184/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 185/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 186/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 187/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 188/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9561\n","Epoch 189/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 190/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 191/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 192/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 193/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 194/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 195/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 196/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 197/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 198/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 199/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 200/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 201/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 202/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 203/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 204/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 205/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 206/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 207/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 208/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 209/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 210/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 211/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 212/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 213/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 214/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 215/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 216/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 217/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 218/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 219/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 220/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 221/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 222/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 223/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 224/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 225/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 226/250\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 227/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 228/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 229/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 230/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 231/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 232/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 233/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 234/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 235/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 236/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 237/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 238/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 239/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 240/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 241/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 242/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 243/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 244/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 245/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 246/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9590\n","Epoch 247/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 248/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 249/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 250/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9563\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 34202 0.5\n","The shape of N (5909, 784)\n","The minimum value of N  -0.7496365308761597\n","The max value of N 0.7499995231628418\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9943923526187683\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/250\n","5318/5318 [==============================] - 5s 950us/step - loss: 5.0789 - val_loss: 5.3048\n","Epoch 2/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 5.0017 - val_loss: 5.0346\n","Epoch 3/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9858 - val_loss: 4.9919\n","Epoch 4/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9782 - val_loss: 4.9827\n","Epoch 5/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9730 - val_loss: 4.9769\n","Epoch 6/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9697 - val_loss: 4.9782\n","Epoch 7/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9668 - val_loss: 4.9770\n","Epoch 8/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9647 - val_loss: 4.9743\n","Epoch 9/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9631 - val_loss: 4.9720\n","Epoch 10/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9620 - val_loss: 4.9693\n","Epoch 11/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9613 - val_loss: 4.9683\n","Epoch 12/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9602 - val_loss: 4.9684\n","Epoch 13/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9601 - val_loss: 4.9722\n","Epoch 14/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9593 - val_loss: 4.9672\n","Epoch 15/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9589 - val_loss: 4.9673\n","Epoch 16/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9581 - val_loss: 4.9656\n","Epoch 17/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9579 - val_loss: 4.9641\n","Epoch 18/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9591 - val_loss: 4.9694\n","Epoch 19/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9580 - val_loss: 4.9647\n","Epoch 20/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9572 - val_loss: 4.9640\n","Epoch 21/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9568 - val_loss: 4.9632\n","Epoch 22/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9566 - val_loss: 4.9645\n","Epoch 23/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9565 - val_loss: 4.9612\n","Epoch 24/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9563 - val_loss: 4.9620\n","Epoch 25/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9560 - val_loss: 4.9634\n","Epoch 26/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9560 - val_loss: 4.9614\n","Epoch 27/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9558 - val_loss: 4.9603\n","Epoch 28/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9556 - val_loss: 4.9602\n","Epoch 29/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9560 - val_loss: 4.9597\n","Epoch 30/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9553 - val_loss: 4.9598\n","Epoch 31/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9552 - val_loss: 4.9594\n","Epoch 32/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9604\n","Epoch 33/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9553 - val_loss: 4.9589\n","Epoch 34/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9553 - val_loss: 4.9590\n","Epoch 35/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9550 - val_loss: 4.9588\n","Epoch 36/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9600\n","Epoch 37/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9549 - val_loss: 4.9587\n","Epoch 38/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9546 - val_loss: 4.9586\n","Epoch 39/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 40/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9583\n","Epoch 41/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 42/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 43/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9551 - val_loss: 4.9588\n","Epoch 44/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9634\n","Epoch 45/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 46/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 47/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9542 - val_loss: 4.9590\n","Epoch 48/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9544 - val_loss: 4.9584\n","Epoch 49/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 50/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 51/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 52/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 53/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 54/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 55/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 56/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 57/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 58/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 59/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 60/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 61/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 62/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 63/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 64/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 65/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 66/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 67/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 68/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 69/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 70/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 71/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 72/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 73/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 74/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 75/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 76/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 77/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 78/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 79/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 80/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 81/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 82/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 83/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 84/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 85/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 86/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 87/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 88/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 89/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 90/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 91/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 92/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 93/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 94/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 95/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 96/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 97/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 98/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 99/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 100/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 101/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 102/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 103/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 104/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 105/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 106/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 107/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 108/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 109/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 110/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 111/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9593\n","Epoch 112/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 113/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 114/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 115/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 116/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 117/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 118/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 119/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 120/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 121/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 122/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 123/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 124/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 125/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 126/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 127/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 128/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 129/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 130/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 131/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 132/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 133/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 134/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9633\n","Epoch 135/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 136/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 137/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 138/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 139/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 140/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 141/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 142/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 143/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 144/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 145/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 146/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 147/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 148/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 149/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 150/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 151/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 152/250\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 153/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 154/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9561\n","Epoch 155/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 156/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 157/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 158/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 159/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 160/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 161/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 162/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9561\n","Epoch 163/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 164/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 165/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 166/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 167/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 168/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9577\n","Epoch 169/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 170/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 171/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 172/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 173/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 174/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 175/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 176/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 177/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 178/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 179/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 180/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 181/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 182/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 183/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 184/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 185/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 186/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 187/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 188/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 189/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9561\n","Epoch 190/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9561\n","Epoch 191/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9521 - val_loss: 4.9561\n","Epoch 192/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9560\n","Epoch 193/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9561\n","Epoch 194/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 195/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 196/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9561\n","Epoch 197/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9561\n","Epoch 198/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9562\n","Epoch 199/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9560\n","Epoch 200/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9520 - val_loss: 4.9562\n","Epoch 201/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9520 - val_loss: 4.9562\n","Epoch 202/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 203/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9561\n","Epoch 204/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 205/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9562\n","Epoch 206/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 207/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 208/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 209/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 210/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 211/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 212/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9562\n","Epoch 213/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 214/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 215/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9562\n","Epoch 216/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9561\n","Epoch 217/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9520 - val_loss: 4.9562\n","Epoch 218/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9562\n","Epoch 219/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9565\n","Epoch 220/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9561\n","Epoch 221/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9562\n","Epoch 222/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9562\n","Epoch 223/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9561\n","Epoch 224/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9562\n","Epoch 225/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9562\n","Epoch 226/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9562\n","Epoch 227/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 228/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9561\n","Epoch 229/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9561\n","Epoch 230/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9562\n","Epoch 231/250\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9564\n","Epoch 232/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9519 - val_loss: 4.9562\n","Epoch 233/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9518 - val_loss: 4.9562\n","Epoch 234/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9562\n","Epoch 235/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9519 - val_loss: 4.9563\n","Epoch 236/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9562\n","Epoch 237/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9563\n","Epoch 238/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 239/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 240/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9563\n","Epoch 241/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9519 - val_loss: 4.9561\n","Epoch 242/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9562\n","Epoch 243/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9562\n","Epoch 244/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9519 - val_loss: 4.9562\n","Epoch 245/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9563\n","Epoch 246/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9562\n","Epoch 247/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9562\n","Epoch 248/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9519 - val_loss: 4.9563\n","Epoch 249/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9562\n","Epoch 250/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9563\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 33812 0.5\n","The shape of N (5909, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7442425489425659\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9926744028430154\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/250\n","5318/5318 [==============================] - 6s 1ms/step - loss: 5.0804 - val_loss: 5.2361\n","Epoch 2/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 5.0010 - val_loss: 5.0359\n","Epoch 3/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9835 - val_loss: 4.9894\n","Epoch 4/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9755 - val_loss: 4.9829\n","Epoch 5/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9703 - val_loss: 4.9781\n","Epoch 6/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9673 - val_loss: 4.9739\n","Epoch 7/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9652 - val_loss: 4.9716\n","Epoch 8/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9641 - val_loss: 4.9735\n","Epoch 9/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9634 - val_loss: 4.9705\n","Epoch 10/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9618 - val_loss: 4.9698\n","Epoch 11/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9609 - val_loss: 4.9691\n","Epoch 12/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9603 - val_loss: 4.9680\n","Epoch 13/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9598 - val_loss: 4.9689\n","Epoch 14/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9591 - val_loss: 4.9676\n","Epoch 15/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9590 - val_loss: 4.9675\n","Epoch 16/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9584 - val_loss: 4.9670\n","Epoch 17/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9583 - val_loss: 4.9681\n","Epoch 18/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9584 - val_loss: 4.9672\n","Epoch 19/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9577 - val_loss: 4.9647\n","Epoch 20/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9573 - val_loss: 4.9650\n","Epoch 21/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9575 - val_loss: 4.9667\n","Epoch 22/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9571 - val_loss: 4.9645\n","Epoch 23/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9566 - val_loss: 4.9645\n","Epoch 24/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9566 - val_loss: 4.9631\n","Epoch 25/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9562 - val_loss: 4.9618\n","Epoch 26/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9562 - val_loss: 4.9621\n","Epoch 27/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9562 - val_loss: 4.9614\n","Epoch 28/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9561 - val_loss: 4.9621\n","Epoch 29/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9559 - val_loss: 4.9607\n","Epoch 30/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9558 - val_loss: 4.9610\n","Epoch 31/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9560 - val_loss: 4.9646\n","Epoch 32/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9559 - val_loss: 4.9637\n","Epoch 33/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9556 - val_loss: 4.9611\n","Epoch 34/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9553 - val_loss: 4.9601\n","Epoch 35/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9552 - val_loss: 4.9598\n","Epoch 36/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9551 - val_loss: 4.9591\n","Epoch 37/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9550 - val_loss: 4.9593\n","Epoch 38/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9550 - val_loss: 4.9590\n","Epoch 39/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9550 - val_loss: 4.9590\n","Epoch 40/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9547 - val_loss: 4.9588\n","Epoch 41/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9588\n","Epoch 42/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9583\n","Epoch 43/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9583\n","Epoch 44/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9547 - val_loss: 4.9585\n","Epoch 45/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9586\n","Epoch 46/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9588\n","Epoch 47/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9584\n","Epoch 48/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9586\n","Epoch 49/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9585\n","Epoch 50/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9586\n","Epoch 51/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 52/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 53/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 54/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 55/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 56/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9585\n","Epoch 57/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 58/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 59/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 60/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 61/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 62/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 63/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 64/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 65/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 66/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 67/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 68/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 69/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 70/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 71/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 72/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 73/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 74/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 75/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 76/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 77/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 78/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 79/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 80/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 81/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 82/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 83/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 84/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 85/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 86/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 87/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 88/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 89/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 90/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 91/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 92/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 93/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 94/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 95/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 96/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 97/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 98/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 99/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 100/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 101/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 102/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 103/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 104/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 105/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 106/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 107/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 108/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 109/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 110/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 111/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 112/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 113/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 114/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 115/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 116/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 117/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 118/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 119/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 120/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 121/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 122/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 123/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 124/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 125/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 126/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 127/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 128/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 129/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 130/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 131/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 132/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 133/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 134/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 135/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 136/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 137/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 138/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 139/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 140/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 141/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 142/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 143/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 144/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 145/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 146/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 147/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 148/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 149/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 150/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 151/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 152/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 153/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 154/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 155/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 156/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 157/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 158/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 159/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 160/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 161/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 162/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 163/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 164/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 165/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 166/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 167/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 168/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 169/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 170/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 171/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 172/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 173/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 174/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 175/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9571\n","Epoch 176/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 177/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 178/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 179/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 180/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9571\n","Epoch 181/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 182/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9587\n","Epoch 183/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 184/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9580\n","Epoch 185/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9583\n","Epoch 186/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9582\n","Epoch 187/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 188/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 189/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 190/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 191/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 192/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 193/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9584\n","Epoch 194/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 195/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 196/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 197/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 198/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 199/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9585\n","Epoch 200/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9582\n","Epoch 201/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9584\n","Epoch 202/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9581\n","Epoch 203/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9586\n","Epoch 204/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9582\n","Epoch 205/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9585\n","Epoch 206/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 207/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 208/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 209/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9586\n","Epoch 210/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9586\n","Epoch 211/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9593\n","Epoch 212/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9585\n","Epoch 213/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9580\n","Epoch 214/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9584\n","Epoch 215/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9587\n","Epoch 216/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9587\n","Epoch 217/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9588\n","Epoch 218/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9589\n","Epoch 219/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9591\n","Epoch 220/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9587\n","Epoch 221/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9586\n","Epoch 222/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9588\n","Epoch 223/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9585\n","Epoch 224/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9586\n","Epoch 225/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9585\n","Epoch 226/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9586\n","Epoch 227/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9592\n","Epoch 228/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9592\n","Epoch 229/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9587\n","Epoch 230/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9589\n","Epoch 231/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9596\n","Epoch 232/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9603\n","Epoch 233/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9592\n","Epoch 234/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9589\n","Epoch 235/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9591\n","Epoch 236/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9591\n","Epoch 237/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9599\n","Epoch 238/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9597\n","Epoch 239/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9587\n","Epoch 240/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9614\n","Epoch 241/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9596\n","Epoch 242/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9598\n","Epoch 243/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9598\n","Epoch 244/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9605\n","Epoch 245/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9588\n","Epoch 246/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9596\n","Epoch 247/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9597\n","Epoch 248/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9603\n","Epoch 249/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9601\n","Epoch 250/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9596\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 35035 0.5\n","The shape of N (5909, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9967556385881576\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/250\n","5318/5318 [==============================] - 6s 1ms/step - loss: 5.0688 - val_loss: 5.2476\n","Epoch 2/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9976 - val_loss: 5.0293\n","Epoch 3/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9828 - val_loss: 4.9911\n","Epoch 4/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9759 - val_loss: 4.9849\n","Epoch 5/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9718 - val_loss: 4.9810\n","Epoch 6/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9682 - val_loss: 4.9781\n","Epoch 7/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9661 - val_loss: 4.9762\n","Epoch 8/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9650 - val_loss: 4.9734\n","Epoch 9/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9629 - val_loss: 4.9709\n","Epoch 10/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9620 - val_loss: 4.9698\n","Epoch 11/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9630 - val_loss: 4.9802\n","Epoch 12/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9616 - val_loss: 4.9751\n","Epoch 13/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9604 - val_loss: 4.9693\n","Epoch 14/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9605 - val_loss: 4.9759\n","Epoch 15/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9597 - val_loss: 4.9688\n","Epoch 16/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9593 - val_loss: 4.9689\n","Epoch 17/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9586 - val_loss: 4.9654\n","Epoch 18/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9636 - val_loss: 4.9904\n","Epoch 19/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9604 - val_loss: 4.9740\n","Epoch 20/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9592 - val_loss: 4.9686\n","Epoch 21/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9586 - val_loss: 4.9666\n","Epoch 22/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9593 - val_loss: 4.9751\n","Epoch 23/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9580 - val_loss: 4.9672\n","Epoch 24/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9573 - val_loss: 4.9635\n","Epoch 25/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9573 - val_loss: 4.9623\n","Epoch 26/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9580 - val_loss: 4.9876\n","Epoch 27/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9589 - val_loss: 4.9830\n","Epoch 28/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9577 - val_loss: 4.9761\n","Epoch 29/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9573 - val_loss: 4.9690\n","Epoch 30/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9569 - val_loss: 4.9670\n","Epoch 31/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9568 - val_loss: 4.9636\n","Epoch 32/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9564 - val_loss: 4.9618\n","Epoch 33/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9564 - val_loss: 4.9643\n","Epoch 34/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9562 - val_loss: 4.9626\n","Epoch 35/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9561 - val_loss: 4.9607\n","Epoch 36/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9558 - val_loss: 4.9595\n","Epoch 37/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9559 - val_loss: 4.9596\n","Epoch 38/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9557 - val_loss: 4.9600\n","Epoch 39/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9556 - val_loss: 4.9590\n","Epoch 40/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 41/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9564 - val_loss: 4.9727\n","Epoch 42/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9563 - val_loss: 4.9638\n","Epoch 43/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9560 - val_loss: 5.0469\n","Epoch 44/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9561 - val_loss: 4.9748\n","Epoch 45/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9558 - val_loss: 4.9601\n","Epoch 46/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9555 - val_loss: 4.9591\n","Epoch 47/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 48/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9552 - val_loss: 4.9584\n","Epoch 49/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9550 - val_loss: 4.9582\n","Epoch 50/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9551 - val_loss: 4.9584\n","Epoch 51/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 52/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 53/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9774\n","Epoch 54/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9555 - val_loss: 4.9665\n","Epoch 55/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9552 - val_loss: 4.9599\n","Epoch 56/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9550 - val_loss: 4.9586\n","Epoch 57/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 58/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 59/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 60/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9583\n","Epoch 61/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 62/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 63/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 64/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 65/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 66/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 67/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 68/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 69/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 70/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 71/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 72/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 73/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 74/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 75/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 76/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 77/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 78/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 79/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 80/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 81/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 82/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 83/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 84/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 85/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 86/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 87/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 88/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 89/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 90/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 91/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 92/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 93/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 94/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 95/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 96/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 97/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 98/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 99/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 100/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 101/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 102/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 103/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 104/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 105/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 106/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 107/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 108/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 109/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 110/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 111/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 112/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 113/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 114/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 115/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 116/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 117/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 118/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 119/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 120/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 121/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 122/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 123/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 124/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 125/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 126/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 127/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 128/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 129/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 130/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 131/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 132/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 133/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 134/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 135/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 136/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 137/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 138/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 139/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 140/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 141/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 142/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 143/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 144/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 145/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 146/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 147/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 148/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 149/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 150/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 151/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 152/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 153/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 154/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 155/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 156/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 157/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 158/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 159/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 160/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 161/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 162/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 163/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 164/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 165/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 166/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 167/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 168/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 169/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 170/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 171/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 172/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 173/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 174/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 175/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 176/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 177/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 178/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 179/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 180/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 181/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 182/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 183/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 184/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 185/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 186/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 187/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 188/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 189/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 190/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 191/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 192/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 193/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 194/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 195/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 196/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 197/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 198/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 199/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 200/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 201/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 202/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 203/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 204/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 205/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 206/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 207/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 208/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 209/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 210/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 211/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9609\n","Epoch 212/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 213/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 214/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 215/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 216/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 217/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 218/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 219/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 220/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 221/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 222/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 223/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 224/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 225/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 226/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 227/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 228/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 229/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 230/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 231/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 232/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 233/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 234/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 235/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 236/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 237/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 238/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 239/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 240/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 241/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 242/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 243/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 244/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 245/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 246/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 247/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 248/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 249/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 250/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9569\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 37558 0.5\n","The shape of N (5909, 784)\n","The minimum value of N  -0.7499970197677612\n","The max value of N 0.7499233484268188\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9927480713582707\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/250\n","5318/5318 [==============================] - 6s 1ms/step - loss: 5.0775 - val_loss: 5.2470\n","Epoch 2/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9988 - val_loss: 5.0242\n","Epoch 3/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9835 - val_loss: 4.9896\n","Epoch 4/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9759 - val_loss: 4.9789\n","Epoch 5/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9717 - val_loss: 4.9778\n","Epoch 6/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9682 - val_loss: 4.9740\n","Epoch 7/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9657 - val_loss: 4.9717\n","Epoch 8/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9641 - val_loss: 4.9700\n","Epoch 9/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9630 - val_loss: 4.9682\n","Epoch 10/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9620 - val_loss: 4.9687\n","Epoch 11/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9612 - val_loss: 4.9667\n","Epoch 12/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9607 - val_loss: 4.9674\n","Epoch 13/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9617 - val_loss: 4.9819\n","Epoch 14/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9619 - val_loss: 4.9734\n","Epoch 15/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9596 - val_loss: 4.9696\n","Epoch 16/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9592 - val_loss: 4.9665\n","Epoch 17/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9585 - val_loss: 4.9648\n","Epoch 18/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9581 - val_loss: 4.9641\n","Epoch 19/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9581 - val_loss: 4.9845\n","Epoch 20/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9616 - val_loss: 5.0258\n","Epoch 21/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9593 - val_loss: 4.9816\n","Epoch 22/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9582 - val_loss: 4.9686\n","Epoch 23/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9581 - val_loss: 4.9659\n","Epoch 24/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9577 - val_loss: 4.9633\n","Epoch 25/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9573 - val_loss: 4.9622\n","Epoch 26/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9571 - val_loss: 4.9615\n","Epoch 27/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9565 - val_loss: 4.9619\n","Epoch 28/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9563 - val_loss: 4.9617\n","Epoch 29/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9564 - val_loss: 4.9608\n","Epoch 30/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9561 - val_loss: 4.9610\n","Epoch 31/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9618\n","Epoch 32/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9558 - val_loss: 4.9611\n","Epoch 33/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9562 - val_loss: 4.9627\n","Epoch 34/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9559 - val_loss: 4.9603\n","Epoch 35/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9555 - val_loss: 4.9611\n","Epoch 36/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 4.9621\n","Epoch 37/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9558 - val_loss: 4.9601\n","Epoch 38/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9553 - val_loss: 4.9593\n","Epoch 39/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9551 - val_loss: 4.9597\n","Epoch 40/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9598\n","Epoch 41/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9589\n","Epoch 42/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9596\n","Epoch 43/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9596\n","Epoch 44/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9592\n","Epoch 45/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9593\n","Epoch 46/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9589\n","Epoch 47/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9545 - val_loss: 4.9591\n","Epoch 48/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9591\n","Epoch 49/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9591\n","Epoch 50/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9593\n","Epoch 51/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9592\n","Epoch 52/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9594\n","Epoch 53/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9594\n","Epoch 54/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9585\n","Epoch 55/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9591\n","Epoch 56/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9586\n","Epoch 57/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9586\n","Epoch 58/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 59/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9586\n","Epoch 60/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 61/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 62/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9589\n","Epoch 63/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 64/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9585\n","Epoch 65/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 66/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9585\n","Epoch 67/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9591\n","Epoch 68/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 69/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9583\n","Epoch 70/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9585\n","Epoch 71/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9588\n","Epoch 72/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9585\n","Epoch 73/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 74/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 75/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 76/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9584\n","Epoch 77/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 78/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 79/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9583\n","Epoch 80/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 81/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 82/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 83/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 84/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 85/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 86/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 87/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 88/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 89/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 90/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 91/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 92/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 93/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 94/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 95/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 96/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 97/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 98/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 99/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 100/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 101/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 102/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 103/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 104/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 105/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 106/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 107/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 108/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 109/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 110/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 111/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 112/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 113/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 114/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 115/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 116/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 117/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 118/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 119/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 120/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 121/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 122/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9657\n","Epoch 123/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9590\n","Epoch 124/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9588\n","Epoch 125/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 126/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 127/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 128/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 129/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 130/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 131/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 132/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 133/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 134/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 135/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 136/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 137/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 138/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 139/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 140/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 141/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 142/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 143/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 144/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 145/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 146/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 147/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 148/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 149/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 150/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9668\n","Epoch 151/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9591\n","Epoch 152/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 153/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 154/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 155/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 156/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 157/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 158/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 159/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 160/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 161/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 162/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 163/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 164/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 165/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 166/250\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 167/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 168/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 169/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 170/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 171/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 172/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 173/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 174/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 175/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 176/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 177/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 178/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 179/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 180/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 181/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 182/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 183/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 184/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 185/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 186/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 187/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 188/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 189/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9579\n","Epoch 190/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9581\n","Epoch 191/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9570\n","Epoch 192/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 193/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 194/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 195/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9566\n","Epoch 196/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 197/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 198/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9567\n","Epoch 199/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 200/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9566\n","Epoch 201/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 202/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 203/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 204/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 205/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9566\n","Epoch 206/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 207/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9567\n","Epoch 208/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 209/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 210/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 211/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 212/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9566\n","Epoch 213/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 214/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 215/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 216/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 217/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 218/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 219/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9566\n","Epoch 220/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 221/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 222/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9566\n","Epoch 223/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 224/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 225/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 226/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 227/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 228/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 229/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 230/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 231/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 232/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9566\n","Epoch 233/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 234/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9643\n","Epoch 235/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9572\n","Epoch 236/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9569\n","Epoch 237/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 238/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 239/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9566\n","Epoch 240/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 241/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 242/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 243/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9566\n","Epoch 244/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 245/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9570\n","Epoch 246/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 247/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9521 - val_loss: 4.9567\n","Epoch 248/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 249/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 250/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9568\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 44209 0.5\n","The shape of N (5909, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7499918937683105\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9953470965764768\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/250\n","5318/5318 [==============================] - 7s 1ms/step - loss: 5.0719 - val_loss: 5.2328\n","Epoch 2/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9982 - val_loss: 5.0109\n","Epoch 3/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9828 - val_loss: 4.9871\n","Epoch 4/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9749 - val_loss: 4.9804\n","Epoch 5/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9708 - val_loss: 4.9762\n","Epoch 6/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9672 - val_loss: 4.9763\n","Epoch 7/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9659 - val_loss: 4.9728\n","Epoch 8/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9657 - val_loss: 4.9707\n","Epoch 9/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9634 - val_loss: 4.9703\n","Epoch 10/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9624 - val_loss: 4.9667\n","Epoch 11/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9613 - val_loss: 4.9667\n","Epoch 12/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9602 - val_loss: 4.9680\n","Epoch 13/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9601 - val_loss: 4.9646\n","Epoch 14/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9594 - val_loss: 4.9646\n","Epoch 15/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9594 - val_loss: 4.9665\n","Epoch 16/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9587 - val_loss: 4.9648\n","Epoch 17/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9580 - val_loss: 4.9638\n","Epoch 18/250\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9576 - val_loss: 4.9623\n","Epoch 19/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9580 - val_loss: 4.9638\n","Epoch 20/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9578 - val_loss: 4.9675\n","Epoch 21/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9575 - val_loss: 4.9748\n","Epoch 22/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9572 - val_loss: 4.9682\n","Epoch 23/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9571 - val_loss: 4.9638\n","Epoch 24/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9572 - val_loss: 4.9652\n","Epoch 25/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9568 - val_loss: 4.9638\n","Epoch 26/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9566 - val_loss: 4.9616\n","Epoch 27/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9567 - val_loss: 4.9639\n","Epoch 28/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9568 - val_loss: 4.9618\n","Epoch 29/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9562 - val_loss: 4.9603\n","Epoch 30/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9560 - val_loss: 4.9599\n","Epoch 31/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9558 - val_loss: 4.9593\n","Epoch 32/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9556 - val_loss: 4.9589\n","Epoch 33/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9556 - val_loss: 4.9688\n","Epoch 34/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9563 - val_loss: 4.9710\n","Epoch 35/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 4.9637\n","Epoch 36/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9553 - val_loss: 4.9615\n","Epoch 37/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9554 - val_loss: 4.9605\n","Epoch 38/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9553 - val_loss: 4.9595\n","Epoch 39/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9552 - val_loss: 4.9597\n","Epoch 40/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9588 - val_loss: 5.1047\n","Epoch 41/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9616 - val_loss: 4.9955\n","Epoch 42/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9584 - val_loss: 4.9875\n","Epoch 43/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9575 - val_loss: 4.9733\n","Epoch 44/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9565 - val_loss: 4.9623\n","Epoch 45/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9560 - val_loss: 4.9607\n","Epoch 46/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9569 - val_loss: 4.9706\n","Epoch 47/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9577 - val_loss: 4.9632\n","Epoch 48/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9562 - val_loss: 4.9603\n","Epoch 49/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9554 - val_loss: 4.9584\n","Epoch 50/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9553 - val_loss: 4.9586\n","Epoch 51/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9577\n","Epoch 52/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 53/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9572\n","Epoch 54/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9571\n","Epoch 55/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9574\n","Epoch 56/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9595\n","Epoch 57/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 58/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 59/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 60/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 61/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 62/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 63/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 64/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 65/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 66/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 67/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 68/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 69/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 70/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 71/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 72/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 73/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 74/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 75/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 76/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 77/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9590\n","Epoch 78/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 79/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 80/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 81/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 82/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 83/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 84/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 85/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 86/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 87/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 88/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 89/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 90/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9658\n","Epoch 91/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9638\n","Epoch 92/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 93/250\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 94/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 95/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 96/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 97/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 98/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 99/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 100/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 101/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 102/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 103/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 104/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 105/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 106/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 107/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 108/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 109/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 110/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 111/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 112/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 113/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 114/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 115/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 116/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 117/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 118/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 119/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9595\n","Epoch 120/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 121/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 122/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 123/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 124/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 125/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 126/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 127/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 128/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 129/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 130/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 131/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 132/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 133/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 134/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 135/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 136/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 137/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 138/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 139/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 140/250\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 141/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 142/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 143/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 144/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 145/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 146/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 147/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 148/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 149/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 150/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 151/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 152/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 153/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9742\n","Epoch 154/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9696\n","Epoch 155/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9638\n","Epoch 156/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9601\n","Epoch 157/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 158/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 159/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 160/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 161/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 162/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 163/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 164/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 165/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 166/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 167/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 168/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 169/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 170/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 171/250\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 172/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 173/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 174/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 175/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 176/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 177/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 178/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 179/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 180/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 181/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 182/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 183/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 184/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 185/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 186/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 187/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 188/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 189/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 190/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 191/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 192/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 193/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 194/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 195/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 196/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 197/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 198/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 199/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 200/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 201/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 202/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 203/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 204/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 205/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 206/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 207/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 208/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 209/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 210/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 211/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 212/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 213/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 214/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 215/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 216/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 217/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 218/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 219/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 220/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 221/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 222/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 223/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 224/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9561\n","Epoch 225/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 226/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 227/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 228/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 229/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 230/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9561\n","Epoch 231/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9587\n","Epoch 232/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 233/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 234/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9561\n","Epoch 235/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 236/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9561\n","Epoch 237/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 238/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 239/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9561\n","Epoch 240/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 241/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 242/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 243/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 244/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 245/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 246/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 247/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 248/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9561\n","Epoch 249/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 250/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9560\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 38525 0.5\n","The shape of N (5909, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7499334812164307\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9886933562786202\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/250\n","5318/5318 [==============================] - 8s 1ms/step - loss: 5.0734 - val_loss: 5.5517\n","Epoch 2/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9990 - val_loss: 5.1374\n","Epoch 3/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9865 - val_loss: 5.0134\n","Epoch 4/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9799 - val_loss: 4.9932\n","Epoch 5/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9722 - val_loss: 4.9777\n","Epoch 6/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9693 - val_loss: 4.9740\n","Epoch 7/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9669 - val_loss: 4.9713\n","Epoch 8/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9660 - val_loss: 4.9719\n","Epoch 9/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9664 - val_loss: 4.9725\n","Epoch 10/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9647 - val_loss: 4.9711\n","Epoch 11/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9630 - val_loss: 4.9681\n","Epoch 12/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9625 - val_loss: 4.9682\n","Epoch 13/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9620 - val_loss: 4.9681\n","Epoch 14/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9616 - val_loss: 4.9664\n","Epoch 15/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9608 - val_loss: 4.9647\n","Epoch 16/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9599 - val_loss: 4.9643\n","Epoch 17/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9595 - val_loss: 4.9635\n","Epoch 18/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9594 - val_loss: 4.9635\n","Epoch 19/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9601 - val_loss: 4.9642\n","Epoch 20/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9595 - val_loss: 4.9641\n","Epoch 21/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9587 - val_loss: 4.9635\n","Epoch 22/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9585 - val_loss: 4.9638\n","Epoch 23/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9583 - val_loss: 4.9642\n","Epoch 24/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9590 - val_loss: 4.9632\n","Epoch 25/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9583 - val_loss: 4.9641\n","Epoch 26/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9580 - val_loss: 4.9623\n","Epoch 27/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9573 - val_loss: 4.9622\n","Epoch 28/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9573 - val_loss: 4.9617\n","Epoch 29/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9571 - val_loss: 4.9617\n","Epoch 30/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9567 - val_loss: 4.9609\n","Epoch 31/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9567 - val_loss: 4.9608\n","Epoch 32/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9565 - val_loss: 4.9604\n","Epoch 33/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9566 - val_loss: 4.9606\n","Epoch 34/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9563 - val_loss: 4.9607\n","Epoch 35/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9561 - val_loss: 4.9602\n","Epoch 36/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9559 - val_loss: 4.9596\n","Epoch 37/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9559 - val_loss: 4.9601\n","Epoch 38/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9560 - val_loss: 4.9611\n","Epoch 39/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9590 - val_loss: 4.9689\n","Epoch 40/250\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9568 - val_loss: 4.9632\n","Epoch 41/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9561 - val_loss: 4.9608\n","Epoch 42/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9561 - val_loss: 4.9607\n","Epoch 43/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9560 - val_loss: 4.9599\n","Epoch 44/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9558 - val_loss: 4.9604\n","Epoch 45/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9555 - val_loss: 4.9595\n","Epoch 46/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9554 - val_loss: 4.9593\n","Epoch 47/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9586\n","Epoch 48/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9584\n","Epoch 49/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9550 - val_loss: 4.9593\n","Epoch 50/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9588\n","Epoch 51/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9554 - val_loss: 4.9589\n","Epoch 52/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9553 - val_loss: 4.9588\n","Epoch 53/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 54/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 55/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 56/250\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 57/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 58/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9589\n","Epoch 59/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9546 - val_loss: 4.9587\n","Epoch 60/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 61/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 62/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 63/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 64/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 65/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 66/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9577 - val_loss: 4.9802\n","Epoch 67/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9567 - val_loss: 4.9656\n","Epoch 68/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9555 - val_loss: 4.9617\n","Epoch 69/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9593\n","Epoch 70/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9627\n","Epoch 71/250\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9547 - val_loss: 4.9590\n","Epoch 72/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 73/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 74/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 75/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 76/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 77/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 78/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 79/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 80/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 81/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 82/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 83/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 84/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 85/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 86/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 87/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 88/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 89/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 90/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 91/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 92/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 93/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 94/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 95/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 96/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 97/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 98/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 99/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 100/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 101/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 102/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 103/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 104/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 105/250\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 106/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 107/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 108/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 109/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 110/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 111/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 112/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 113/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 114/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 115/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 116/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 117/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 118/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 119/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 120/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 121/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 122/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 123/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 124/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 125/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 126/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 127/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 128/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 129/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 130/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 131/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 132/250\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 133/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 134/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 135/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 136/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 137/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 138/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 139/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 140/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 141/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 142/250\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 143/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 144/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 145/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 146/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 147/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 148/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 149/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 150/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 151/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 152/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 153/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 154/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 155/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 156/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 157/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 158/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 159/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 160/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 161/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 162/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 163/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 164/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 165/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 166/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 167/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 168/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 169/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 170/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 171/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 172/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 173/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9598\n","Epoch 174/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 175/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 176/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 177/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 178/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 179/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 180/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 181/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 182/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 183/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 184/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 185/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 186/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 187/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 188/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9559\n","Epoch 189/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 190/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 191/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 192/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 193/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 194/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 195/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 196/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 197/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 198/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 199/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 200/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 201/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 202/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 203/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 204/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9558\n","Epoch 205/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9558\n","Epoch 206/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 207/250\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 208/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 209/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 210/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 211/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9559\n","Epoch 212/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9558\n","Epoch 213/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9557\n","Epoch 214/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 215/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9521 - val_loss: 4.9559\n","Epoch 216/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9557\n","Epoch 217/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9557\n","Epoch 218/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9558\n","Epoch 219/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9558\n","Epoch 220/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9558\n","Epoch 221/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9559\n","Epoch 222/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9559\n","Epoch 223/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9520 - val_loss: 4.9558\n","Epoch 224/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9558\n","Epoch 225/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9599\n","Epoch 226/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 227/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 228/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 229/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 230/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 231/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 232/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9559\n","Epoch 233/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9559\n","Epoch 234/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9558\n","Epoch 235/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9559\n","Epoch 236/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9559\n","Epoch 237/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9521 - val_loss: 4.9559\n","Epoch 238/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9560\n","Epoch 239/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9558\n","Epoch 240/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9520 - val_loss: 4.9558\n","Epoch 241/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9520 - val_loss: 4.9558\n","Epoch 242/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9521 - val_loss: 4.9560\n","Epoch 243/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9559\n","Epoch 244/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9520 - val_loss: 4.9558\n","Epoch 245/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9559\n","Epoch 246/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9557\n","Epoch 247/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9521 - val_loss: 4.9569\n","Epoch 248/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 249/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9559\n","Epoch 250/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9559\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 36624 0.5\n","The shape of N (5909, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7492981553077698\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9935436913230276\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/250\n","5318/5318 [==============================] - 8s 2ms/step - loss: 5.0724 - val_loss: 5.2906\n","Epoch 2/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9988 - val_loss: 5.0194\n","Epoch 3/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9816 - val_loss: 4.9855\n","Epoch 4/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9743 - val_loss: 4.9807\n","Epoch 5/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9711 - val_loss: 4.9803\n","Epoch 6/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9675 - val_loss: 4.9833\n","Epoch 7/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9661 - val_loss: 4.9726\n","Epoch 8/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9647 - val_loss: 4.9738\n","Epoch 9/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9637 - val_loss: 4.9706\n","Epoch 10/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9623 - val_loss: 4.9701\n","Epoch 11/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9615 - val_loss: 4.9692\n","Epoch 12/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9622 - val_loss: 4.9672\n","Epoch 13/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9606 - val_loss: 4.9664\n","Epoch 14/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9603 - val_loss: 4.9661\n","Epoch 15/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9597 - val_loss: 4.9653\n","Epoch 16/250\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9591 - val_loss: 4.9645\n","Epoch 17/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9590 - val_loss: 4.9642\n","Epoch 18/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9586 - val_loss: 4.9639\n","Epoch 19/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9622 - val_loss: 5.0018\n","Epoch 20/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9602 - val_loss: 4.9884\n","Epoch 21/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9590 - val_loss: 4.9792\n","Epoch 22/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9580 - val_loss: 4.9792\n","Epoch 23/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9578 - val_loss: 4.9708\n","Epoch 24/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9578 - val_loss: 4.9710\n","Epoch 25/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9587 - val_loss: 4.9846\n","Epoch 26/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9578 - val_loss: 4.9787\n","Epoch 27/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9575 - val_loss: 4.9721\n","Epoch 28/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9571 - val_loss: 4.9690\n","Epoch 29/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9569 - val_loss: 4.9672\n","Epoch 30/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9565 - val_loss: 4.9641\n","Epoch 31/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9564 - val_loss: 4.9642\n","Epoch 32/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9562 - val_loss: 4.9626\n","Epoch 33/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9564 - val_loss: 4.9624\n","Epoch 34/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9560 - val_loss: 4.9617\n","Epoch 35/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9559 - val_loss: 4.9618\n","Epoch 36/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 4.9610\n","Epoch 37/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9558 - val_loss: 4.9606\n","Epoch 38/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9555 - val_loss: 4.9598\n","Epoch 39/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9556 - val_loss: 4.9606\n","Epoch 40/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9553 - val_loss: 4.9601\n","Epoch 41/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9552 - val_loss: 4.9597\n","Epoch 42/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9593\n","Epoch 43/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9551 - val_loss: 4.9594\n","Epoch 44/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9588\n","Epoch 45/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9640\n","Epoch 46/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9600\n","Epoch 47/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9589\n","Epoch 48/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9585\n","Epoch 49/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9553 - val_loss: 4.9589\n","Epoch 50/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9597\n","Epoch 51/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9555 - val_loss: 4.9643\n","Epoch 52/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9549 - val_loss: 4.9601\n","Epoch 53/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9592\n","Epoch 54/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9585\n","Epoch 55/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9546 - val_loss: 4.9596\n","Epoch 56/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9585\n","Epoch 57/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 58/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 59/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 60/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 61/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 62/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 63/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 64/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 65/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 66/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 67/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 68/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 69/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 70/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 71/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 72/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 73/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 74/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 75/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 76/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 77/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 78/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 79/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 80/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 81/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 82/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 83/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 84/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 85/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 86/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 87/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 88/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 89/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 90/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 91/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 92/250\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 93/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 94/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 95/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 96/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 97/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 98/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 99/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 100/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 101/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 102/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9596\n","Epoch 103/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 104/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 105/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 106/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9640\n","Epoch 107/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9611\n","Epoch 108/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9590\n","Epoch 109/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 110/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 111/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 112/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 113/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 114/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 115/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 116/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 117/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 118/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 119/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 120/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 121/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 122/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 123/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 124/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 125/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 126/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 127/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 128/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 129/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 130/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 131/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 132/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 133/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 134/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 135/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 136/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 137/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 138/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 139/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 140/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 141/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 142/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 143/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 144/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 145/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 146/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 147/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 148/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 149/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 150/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 151/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 152/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 153/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 154/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 155/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 156/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 157/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 158/250\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 159/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 160/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 161/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 162/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 163/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 164/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 165/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 166/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 167/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 168/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9566\n","Epoch 169/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 170/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 171/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 172/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 173/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 174/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 175/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 176/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 177/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 178/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 179/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 180/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9566\n","Epoch 181/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 182/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9566\n","Epoch 183/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 184/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9566\n","Epoch 185/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9566\n","Epoch 186/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9566\n","Epoch 187/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9566\n","Epoch 188/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 189/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 190/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 191/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 192/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 193/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 194/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 195/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 196/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 197/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 198/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 199/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 200/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9567\n","Epoch 201/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 202/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 203/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 204/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 205/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 206/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 207/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 208/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 209/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 210/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 211/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 212/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 213/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 214/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 215/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 216/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 217/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 218/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 219/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 220/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9568\n","Epoch 221/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 222/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 223/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 224/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 225/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 226/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 227/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 228/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 229/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 230/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 231/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 232/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 233/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 234/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 235/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9570\n","Epoch 236/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9695\n","Epoch 237/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9611\n","Epoch 238/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9583\n","Epoch 239/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 240/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 241/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 242/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 243/250\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 244/250\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9524 - val_loss: 4.9571\n","Epoch 245/250\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 246/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9566\n","Epoch 247/250\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9567\n","Epoch 248/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 249/250\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 250/250\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9564\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 37510 0.5\n","The shape of N (5909, 784)\n","The minimum value of N  -0.7490600943565369\n","The max value of N 0.749739408493042\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9951850258429151\n","=======================\n","Saved model to disk....\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.9815534037800788, 0.9905527496036635, 0.9943923526187683, 0.9926744028430154, 0.9967556385881576, 0.9927480713582707, 0.9953470965764768, 0.9886933562786202, 0.9935436913230276, 0.9951850258429151]\n","AUROC ===== 0.9921445788812994 +/- 0.004183719348393898\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noJQAREQalBKAiEiDUgIQEWlQ/x/Ez2j5/rHotAAAAABJRU5ErkJggg==\n","text/plain":["<matplotlib.figure.Figure at 0x7f4507989b00>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"AmmHmYSaWE-T","colab_type":"text"},"cell_type":"markdown","source":["# **RCAE-MNIST 3_Vs_all**"]},{"metadata":{"id":"h38EeKY1WJKu","colab_type":"code","outputId":"1e6db877-9605-4ed7-fd0a-c5ed5fafb581","executionInfo":{"status":"ok","timestamp":1541256128103,"user_tz":-660,"elapsed":4235920,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":99917}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/250\n","5562/5562 [==============================] - 5s 823us/step - loss: 5.0724 - val_loss: 5.1477\n","Epoch 2/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9997 - val_loss: 5.0066\n","Epoch 3/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9839 - val_loss: 4.9858\n","Epoch 4/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9756 - val_loss: 4.9813\n","Epoch 5/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9705 - val_loss: 4.9742\n","Epoch 6/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9675 - val_loss: 4.9722\n","Epoch 7/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9656 - val_loss: 4.9722\n","Epoch 8/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9637 - val_loss: 4.9690\n","Epoch 9/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9641 - val_loss: 4.9798\n","Epoch 10/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9617 - val_loss: 4.9682\n","Epoch 11/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9603 - val_loss: 4.9663\n","Epoch 12/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9595 - val_loss: 4.9652\n","Epoch 13/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9592 - val_loss: 4.9659\n","Epoch 14/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9585 - val_loss: 4.9638\n","Epoch 15/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9580 - val_loss: 4.9722\n","Epoch 16/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9576 - val_loss: 4.9647\n","Epoch 17/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9570 - val_loss: 4.9633\n","Epoch 18/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9569 - val_loss: 4.9624\n","Epoch 19/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9564 - val_loss: 4.9617\n","Epoch 20/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9563 - val_loss: 4.9605\n","Epoch 21/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9573 - val_loss: 4.9681\n","Epoch 22/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9591 - val_loss: 4.9646\n","Epoch 23/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9565 - val_loss: 4.9618\n","Epoch 24/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9560 - val_loss: 4.9597\n","Epoch 25/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9558 - val_loss: 4.9598\n","Epoch 26/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9560 - val_loss: 4.9801\n","Epoch 27/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9574 - val_loss: 4.9761\n","Epoch 28/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9568 - val_loss: 4.9731\n","Epoch 29/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9562 - val_loss: 4.9650\n","Epoch 30/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9561 - val_loss: 4.9629\n","Epoch 31/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9555 - val_loss: 4.9599\n","Epoch 32/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9552 - val_loss: 4.9589\n","Epoch 33/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9552 - val_loss: 4.9577\n","Epoch 34/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 35/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9551 - val_loss: 4.9575\n","Epoch 36/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9554 - val_loss: 4.9599\n","Epoch 37/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9550 - val_loss: 4.9579\n","Epoch 38/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9548 - val_loss: 4.9568\n","Epoch 39/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 40/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 41/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9545 - val_loss: 4.9567\n","Epoch 42/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9544 - val_loss: 4.9567\n","Epoch 43/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9544 - val_loss: 4.9568\n","Epoch 44/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9563\n","Epoch 45/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 46/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9566\n","Epoch 47/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9565\n","Epoch 48/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9552 - val_loss: 4.9636\n","Epoch 49/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 50/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 51/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 52/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9588\n","Epoch 53/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 54/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 55/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9666\n","Epoch 56/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 57/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 58/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9563\n","Epoch 59/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 60/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 61/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 62/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 63/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9557\n","Epoch 64/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 65/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 66/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 67/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 68/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 69/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 70/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 71/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 72/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9555\n","Epoch 73/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 74/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9557\n","Epoch 75/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9557\n","Epoch 76/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 77/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 78/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 79/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 80/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 81/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 82/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 83/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 84/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 85/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 86/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 87/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 88/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 89/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 90/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 91/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 92/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 93/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 94/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 95/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 96/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 97/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 98/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 99/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 100/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 101/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 102/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 103/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 104/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 105/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 106/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 107/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 108/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 109/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 110/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 111/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 112/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 113/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 114/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 115/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 116/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 117/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 118/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 119/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 120/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 121/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 122/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 123/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 124/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 125/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 126/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 127/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 128/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9912\n","Epoch 129/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9659\n","Epoch 130/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9590\n","Epoch 131/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 132/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 133/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 134/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 135/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 136/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 137/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 138/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 139/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 140/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 141/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 142/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 143/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 144/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 145/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 146/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 147/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 148/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 149/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 150/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 151/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 152/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 153/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 154/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 155/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 156/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9609\n","Epoch 157/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 158/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 159/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 160/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 161/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 162/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 163/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 164/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 165/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 166/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9591\n","Epoch 167/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 168/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 169/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 170/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 171/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 172/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 173/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 174/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 175/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 176/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 177/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 178/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 179/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 180/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 181/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 182/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 183/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 184/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 185/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 186/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 187/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 188/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 189/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 190/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 191/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 192/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 193/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 194/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 195/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 196/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 197/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 198/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 199/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 200/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 201/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 202/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 203/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 204/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 205/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 206/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 207/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 208/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 209/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 210/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 211/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 212/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 213/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 214/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 215/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 216/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 217/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 218/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 219/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 220/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 221/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 222/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 223/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 224/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 225/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 226/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 227/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 228/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 229/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 230/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 231/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 232/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 233/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 234/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 235/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 236/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 237/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 238/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 239/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 240/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9557\n","Epoch 241/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 242/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 243/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 244/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 245/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 246/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 247/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 248/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 249/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 250/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9554\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 43874 0.5\n","The shape of N (6181, 784)\n","The minimum value of N  -0.7470724582672119\n","The max value of N 0.7496131658554077\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9934453016179149\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/250\n","5562/5562 [==============================] - 4s 713us/step - loss: 5.0650 - val_loss: 5.0738\n","Epoch 2/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9971 - val_loss: 4.9969\n","Epoch 3/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9830 - val_loss: 4.9833\n","Epoch 4/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9759 - val_loss: 4.9774\n","Epoch 5/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9714 - val_loss: 4.9721\n","Epoch 6/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9682 - val_loss: 4.9721\n","Epoch 7/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9658 - val_loss: 4.9690\n","Epoch 8/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9645 - val_loss: 4.9690\n","Epoch 9/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9634 - val_loss: 4.9679\n","Epoch 10/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9654 - val_loss: 4.9782\n","Epoch 11/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9620 - val_loss: 4.9686\n","Epoch 12/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9607 - val_loss: 4.9682\n","Epoch 13/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9601 - val_loss: 4.9679\n","Epoch 14/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9596 - val_loss: 4.9642\n","Epoch 15/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9586 - val_loss: 4.9642\n","Epoch 16/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9584 - val_loss: 4.9646\n","Epoch 17/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9578 - val_loss: 4.9636\n","Epoch 18/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9577 - val_loss: 4.9639\n","Epoch 19/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9573 - val_loss: 4.9630\n","Epoch 20/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9578 - val_loss: 4.9655\n","Epoch 21/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9572 - val_loss: 4.9634\n","Epoch 22/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9571 - val_loss: 4.9837\n","Epoch 23/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9574 - val_loss: 4.9665\n","Epoch 24/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9569 - val_loss: 4.9768\n","Epoch 25/250\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9569 - val_loss: 4.9644\n","Epoch 26/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9567 - val_loss: 4.9609\n","Epoch 27/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9564 - val_loss: 4.9596\n","Epoch 28/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9562 - val_loss: 4.9590\n","Epoch 29/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9560 - val_loss: 4.9592\n","Epoch 30/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9605 - val_loss: 5.0075\n","Epoch 31/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9604 - val_loss: 4.9898\n","Epoch 32/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9583 - val_loss: 4.9741\n","Epoch 33/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9571 - val_loss: 4.9640\n","Epoch 34/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9565 - val_loss: 4.9599\n","Epoch 35/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9563 - val_loss: 4.9613\n","Epoch 36/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9560 - val_loss: 4.9587\n","Epoch 37/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9559 - val_loss: 4.9599\n","Epoch 38/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9556 - val_loss: 4.9585\n","Epoch 39/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9554 - val_loss: 4.9582\n","Epoch 40/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9554 - val_loss: 4.9574\n","Epoch 41/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9554 - val_loss: 4.9570\n","Epoch 42/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9551 - val_loss: 4.9573\n","Epoch 43/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9550 - val_loss: 4.9571\n","Epoch 44/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9551 - val_loss: 4.9570\n","Epoch 45/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9548 - val_loss: 4.9566\n","Epoch 46/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9552 - val_loss: 4.9571\n","Epoch 47/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9550 - val_loss: 4.9566\n","Epoch 48/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9547 - val_loss: 4.9570\n","Epoch 49/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 50/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9566\n","Epoch 51/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 52/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9548 - val_loss: 4.9566\n","Epoch 53/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9546 - val_loss: 4.9563\n","Epoch 54/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9545 - val_loss: 4.9568\n","Epoch 55/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9595\n","Epoch 56/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 57/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9567\n","Epoch 58/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9565\n","Epoch 59/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 60/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9595\n","Epoch 61/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 62/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9564\n","Epoch 63/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9562\n","Epoch 64/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9561\n","Epoch 65/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9559\n","Epoch 66/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9559\n","Epoch 67/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9561\n","Epoch 68/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9559\n","Epoch 69/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9558\n","Epoch 70/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9562\n","Epoch 71/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9558\n","Epoch 72/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 73/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9559\n","Epoch 74/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9559\n","Epoch 75/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 76/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 77/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 78/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9561\n","Epoch 79/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 80/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 81/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 82/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 83/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 84/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 85/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 86/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 87/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 88/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 89/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9557\n","Epoch 90/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 91/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9558\n","Epoch 92/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9561\n","Epoch 93/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9632\n","Epoch 94/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9632\n","Epoch 95/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 96/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 97/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 98/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 99/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 100/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9557\n","Epoch 101/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9557\n","Epoch 102/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 103/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 104/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 105/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 106/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 107/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9557\n","Epoch 108/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 109/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 110/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 111/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 112/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 113/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 114/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 115/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 116/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 117/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 118/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 119/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 120/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 121/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 122/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 123/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 124/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 125/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 126/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 127/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 128/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 129/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 130/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 131/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 132/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 133/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 134/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 135/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 136/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 137/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 138/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9584\n","Epoch 139/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9562\n","Epoch 140/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 141/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 142/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 143/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 144/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 145/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 146/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 147/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 148/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9789\n","Epoch 149/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9622\n","Epoch 150/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 151/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 152/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 153/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 154/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 155/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 156/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 157/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 158/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 159/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 160/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 161/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 162/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 163/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 164/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 165/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 166/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 167/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 168/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 169/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 170/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 171/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 172/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 173/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 174/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 175/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 176/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 177/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 178/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 179/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 180/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 181/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 182/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 183/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 184/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 185/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 186/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 187/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 188/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 189/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 190/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 191/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 192/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 193/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 194/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 195/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 196/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 197/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 198/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 199/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 200/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 201/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 202/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 203/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 204/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 205/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 206/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 207/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 208/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 209/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 210/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 211/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 212/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 213/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 214/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 215/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 216/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 217/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 218/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 219/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 220/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 221/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 222/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 223/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 224/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 225/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 226/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 227/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 228/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 229/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 230/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 231/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 232/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 233/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 234/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 235/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 236/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 237/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9603\n","Epoch 238/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 239/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 240/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 241/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 242/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 243/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 244/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 245/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 246/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 247/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 248/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 249/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 250/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9556\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 41537 0.5\n","The shape of N (6181, 784)\n","The minimum value of N  -0.7499268054962158\n","The max value of N 0.749839186668396\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9922800814314796\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/250\n","5562/5562 [==============================] - 4s 799us/step - loss: 5.0779 - val_loss: 5.1339\n","Epoch 2/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9999 - val_loss: 5.0080\n","Epoch 3/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9840 - val_loss: 4.9870\n","Epoch 4/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9757 - val_loss: 4.9806\n","Epoch 5/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9713 - val_loss: 4.9781\n","Epoch 6/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9678 - val_loss: 4.9745\n","Epoch 7/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9652 - val_loss: 4.9719\n","Epoch 8/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9634 - val_loss: 5.0127\n","Epoch 9/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9621 - val_loss: 4.9740\n","Epoch 10/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9613 - val_loss: 4.9697\n","Epoch 11/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9612 - val_loss: 4.9714\n","Epoch 12/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9600 - val_loss: 4.9692\n","Epoch 13/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9592 - val_loss: 4.9672\n","Epoch 14/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9590 - val_loss: 4.9669\n","Epoch 15/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9586 - val_loss: 4.9734\n","Epoch 16/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9580 - val_loss: 4.9700\n","Epoch 17/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9578 - val_loss: 4.9675\n","Epoch 18/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9574 - val_loss: 4.9653\n","Epoch 19/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9568 - val_loss: 4.9650\n","Epoch 20/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9565 - val_loss: 4.9635\n","Epoch 21/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9567 - val_loss: 4.9691\n","Epoch 22/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9566 - val_loss: 4.9625\n","Epoch 23/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9561 - val_loss: 4.9617\n","Epoch 24/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9557 - val_loss: 4.9611\n","Epoch 25/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9559 - val_loss: 4.9645\n","Epoch 26/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9556 - val_loss: 4.9608\n","Epoch 27/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9554 - val_loss: 4.9599\n","Epoch 28/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9552 - val_loss: 4.9596\n","Epoch 29/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9555 - val_loss: 4.9594\n","Epoch 30/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9549 - val_loss: 4.9586\n","Epoch 31/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9550 - val_loss: 4.9588\n","Epoch 32/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9547 - val_loss: 4.9582\n","Epoch 33/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9546 - val_loss: 4.9593\n","Epoch 34/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 35/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 36/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9545 - val_loss: 4.9589\n","Epoch 37/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 38/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 39/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 40/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 4.9584\n","Epoch 41/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 42/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 43/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 44/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 45/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 46/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 47/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 48/250\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 49/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 50/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 51/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 52/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 53/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 54/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 55/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 56/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 57/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 58/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9560\n","Epoch 59/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 60/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 61/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 62/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 63/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 64/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 65/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 66/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 67/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9596\n","Epoch 68/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 69/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 70/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 71/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 72/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 73/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 74/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 75/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 76/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 77/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9603\n","Epoch 78/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 79/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 80/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 81/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 82/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 83/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 84/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 85/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 86/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 87/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 88/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 89/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 90/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 91/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 92/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 93/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 94/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 95/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 96/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 97/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 98/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 99/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 100/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9579\n","Epoch 101/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9632\n","Epoch 102/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 103/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 104/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 105/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 106/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 107/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9681\n","Epoch 108/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9613\n","Epoch 109/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9597\n","Epoch 110/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 111/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 112/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 113/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 114/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 115/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 116/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 117/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 118/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 119/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 120/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 121/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 122/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 123/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 124/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 125/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 126/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 127/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 128/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 129/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 130/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 131/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 132/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 133/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9567\n","Epoch 134/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 135/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 136/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 137/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 138/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 139/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9551\n","Epoch 140/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 141/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 142/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 143/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 144/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 145/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 146/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 147/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 148/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 149/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 150/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 151/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 152/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 153/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 154/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 155/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 156/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 157/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 158/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 159/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 160/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 161/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 162/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 163/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 164/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 165/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9558\n","Epoch 166/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 167/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 168/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 169/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 170/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 171/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 172/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 173/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 174/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 175/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 176/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 177/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 178/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 179/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 180/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 181/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 182/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 183/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9557\n","Epoch 184/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 185/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 186/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 187/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 188/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 189/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 190/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 191/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 192/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 193/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 194/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 195/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9553\n","Epoch 196/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 197/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 198/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 199/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 200/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 201/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 202/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 203/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 204/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 205/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9559\n","Epoch 206/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 207/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 208/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 209/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 210/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 211/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9553\n","Epoch 212/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 213/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 214/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9553\n","Epoch 215/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 216/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9553\n","Epoch 217/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 218/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9553\n","Epoch 219/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 220/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9912\n","Epoch 221/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9655\n","Epoch 222/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9608\n","Epoch 223/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9589\n","Epoch 224/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9570\n","Epoch 225/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9561\n","Epoch 226/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9559\n","Epoch 227/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 228/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9628\n","Epoch 229/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9579\n","Epoch 230/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 231/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9566\n","Epoch 232/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 233/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9557\n","Epoch 234/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9557\n","Epoch 235/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 236/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9557\n","Epoch 237/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 238/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 239/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9556\n","Epoch 240/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 241/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 242/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 243/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 244/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9556\n","Epoch 245/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9556\n","Epoch 246/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 247/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 248/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 249/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9556\n","Epoch 250/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9518 - val_loss: 4.9557\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 40558 0.5\n","The shape of N (6181, 784)\n","The minimum value of N  -0.7499849796295166\n","The max value of N 0.7427303791046143\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9933140469302475\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/250\n","5562/5562 [==============================] - 5s 899us/step - loss: 5.0632 - val_loss: 5.1827\n","Epoch 2/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9971 - val_loss: 5.0162\n","Epoch 3/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9835 - val_loss: 4.9858\n","Epoch 4/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9769 - val_loss: 4.9827\n","Epoch 5/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9717 - val_loss: 4.9770\n","Epoch 6/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9680 - val_loss: 4.9799\n","Epoch 7/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9665 - val_loss: 4.9757\n","Epoch 8/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9648 - val_loss: 4.9774\n","Epoch 9/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9638 - val_loss: 4.9782\n","Epoch 10/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9623 - val_loss: 4.9728\n","Epoch 11/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9621 - val_loss: 4.9696\n","Epoch 12/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9609 - val_loss: 4.9733\n","Epoch 13/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9601 - val_loss: 4.9709\n","Epoch 14/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9596 - val_loss: 4.9683\n","Epoch 15/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9593 - val_loss: 4.9669\n","Epoch 16/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9599 - val_loss: 4.9663\n","Epoch 17/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9590 - val_loss: 4.9648\n","Epoch 18/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9583 - val_loss: 4.9656\n","Epoch 19/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9582 - val_loss: 4.9671\n","Epoch 20/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9581 - val_loss: 4.9672\n","Epoch 21/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9579 - val_loss: 4.9673\n","Epoch 22/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9573 - val_loss: 4.9641\n","Epoch 23/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9572 - val_loss: 4.9627\n","Epoch 24/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9630 - val_loss: 4.9741\n","Epoch 25/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9587 - val_loss: 4.9654\n","Epoch 26/250\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9576 - val_loss: 4.9631\n","Epoch 27/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9570 - val_loss: 4.9625\n","Epoch 28/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9569 - val_loss: 4.9612\n","Epoch 29/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9566 - val_loss: 4.9596\n","Epoch 30/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9570 - val_loss: 4.9672\n","Epoch 31/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9567 - val_loss: 4.9608\n","Epoch 32/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9564 - val_loss: 4.9602\n","Epoch 33/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9562 - val_loss: 4.9601\n","Epoch 34/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9560 - val_loss: 4.9591\n","Epoch 35/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9559 - val_loss: 4.9585\n","Epoch 36/250\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9559 - val_loss: 4.9597\n","Epoch 37/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9558 - val_loss: 4.9591\n","Epoch 38/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9556 - val_loss: 4.9589\n","Epoch 39/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9555 - val_loss: 4.9580\n","Epoch 40/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9555 - val_loss: 4.9579\n","Epoch 41/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9556 - val_loss: 4.9598\n","Epoch 42/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9552 - val_loss: 4.9586\n","Epoch 43/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 44/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9551 - val_loss: 4.9572\n","Epoch 45/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9578\n","Epoch 46/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9551 - val_loss: 4.9573\n","Epoch 47/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9550 - val_loss: 4.9607\n","Epoch 48/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9552 - val_loss: 4.9607\n","Epoch 49/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9550 - val_loss: 4.9579\n","Epoch 50/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9549 - val_loss: 4.9575\n","Epoch 51/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 52/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9548 - val_loss: 4.9570\n","Epoch 53/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9549 - val_loss: 4.9571\n","Epoch 54/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9550 - val_loss: 4.9570\n","Epoch 55/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9546 - val_loss: 4.9570\n","Epoch 56/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9546 - val_loss: 4.9569\n","Epoch 57/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9548 - val_loss: 4.9609\n","Epoch 58/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9549 - val_loss: 4.9587\n","Epoch 59/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9546 - val_loss: 4.9607\n","Epoch 60/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 61/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 62/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9592 - val_loss: 4.9876\n","Epoch 63/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9568 - val_loss: 4.9668\n","Epoch 64/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9557 - val_loss: 4.9613\n","Epoch 65/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9587\n","Epoch 66/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9550 - val_loss: 4.9574\n","Epoch 67/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9547 - val_loss: 4.9570\n","Epoch 68/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9547 - val_loss: 4.9564\n","Epoch 69/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9565\n","Epoch 70/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9561\n","Epoch 71/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9562\n","Epoch 72/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9544 - val_loss: 4.9563\n","Epoch 73/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9545 - val_loss: 4.9683\n","Epoch 74/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9560 - val_loss: 4.9976\n","Epoch 75/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9553 - val_loss: 4.9680\n","Epoch 76/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9551 - val_loss: 4.9614\n","Epoch 77/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9550 - val_loss: 4.9609\n","Epoch 78/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9556 - val_loss: 4.9591\n","Epoch 79/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 80/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 81/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 82/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9544 - val_loss: 4.9568\n","Epoch 83/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 84/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9543 - val_loss: 4.9567\n","Epoch 85/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 86/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9566\n","Epoch 87/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9564\n","Epoch 88/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9563\n","Epoch 89/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9563\n","Epoch 90/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9561\n","Epoch 91/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9565\n","Epoch 92/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9561\n","Epoch 93/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9565\n","Epoch 94/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9563\n","Epoch 95/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9562\n","Epoch 96/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 97/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9563\n","Epoch 98/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9563\n","Epoch 99/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9560\n","Epoch 100/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9562\n","Epoch 101/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 102/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9544 - val_loss: 4.9602\n","Epoch 103/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 104/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9564\n","Epoch 105/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 106/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 107/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 108/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9560\n","Epoch 109/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 110/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 111/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9565\n","Epoch 112/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 113/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9559\n","Epoch 114/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 115/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9559\n","Epoch 116/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 117/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 118/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 119/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 120/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 121/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 122/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 123/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 124/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9558\n","Epoch 125/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 126/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 127/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 128/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 129/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 130/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 131/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 132/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 133/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9561\n","Epoch 134/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 135/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 136/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9560\n","Epoch 137/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 138/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 139/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 140/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 141/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 142/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 143/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 144/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 145/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9935\n","Epoch 146/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9568 - val_loss: 4.9758\n","Epoch 147/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9656\n","Epoch 148/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9626\n","Epoch 149/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9548 - val_loss: 4.9591\n","Epoch 150/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9593\n","Epoch 151/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 152/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 153/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9566\n","Epoch 154/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 155/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 156/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 157/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9559\n","Epoch 158/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9563\n","Epoch 159/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 160/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 161/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 162/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 163/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 164/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 165/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 166/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 167/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 168/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 169/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 170/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 171/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 172/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 173/250\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 174/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 175/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 176/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 177/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 178/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 179/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 180/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 181/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 182/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 183/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 184/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 185/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 186/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 187/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 188/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 189/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 190/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 191/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 192/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 193/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 194/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 195/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 196/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 197/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 198/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 199/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 200/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 201/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 202/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 203/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 204/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 205/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 206/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 207/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 208/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 209/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 210/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 211/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 212/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 213/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 214/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 215/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 216/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 217/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 218/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 219/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 220/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 221/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 222/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 223/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 224/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 225/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 226/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 227/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 228/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 229/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 230/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 231/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 232/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 233/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 234/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 235/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 236/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 237/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 238/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 239/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 240/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 241/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 242/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 243/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 244/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 245/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 246/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 247/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 248/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 249/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 250/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9564\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 45124 0.5\n","The shape of N (6181, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.993351548269581\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/250\n","5562/5562 [==============================] - 5s 986us/step - loss: 5.0735 - val_loss: 5.1110\n","Epoch 2/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9986 - val_loss: 5.0132\n","Epoch 3/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9839 - val_loss: 4.9853\n","Epoch 4/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9762 - val_loss: 4.9807\n","Epoch 5/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9710 - val_loss: 4.9758\n","Epoch 6/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9681 - val_loss: 4.9717\n","Epoch 7/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9662 - val_loss: 4.9814\n","Epoch 8/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9652 - val_loss: 4.9757\n","Epoch 9/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9632 - val_loss: 4.9712\n","Epoch 10/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9623 - val_loss: 4.9708\n","Epoch 11/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9618 - val_loss: 4.9652\n","Epoch 12/250\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9610 - val_loss: 4.9668\n","Epoch 13/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9602 - val_loss: 4.9643\n","Epoch 14/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9591 - val_loss: 4.9645\n","Epoch 15/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9589 - val_loss: 4.9821\n","Epoch 16/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9585 - val_loss: 4.9673\n","Epoch 17/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9605 - val_loss: 4.9886\n","Epoch 18/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9605 - val_loss: 4.9821\n","Epoch 19/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9597 - val_loss: 4.9799\n","Epoch 20/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9580 - val_loss: 4.9719\n","Epoch 21/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9575 - val_loss: 4.9689\n","Epoch 22/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9574 - val_loss: 4.9639\n","Epoch 23/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9573 - val_loss: 4.9652\n","Epoch 24/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9569 - val_loss: 4.9616\n","Epoch 25/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9565 - val_loss: 4.9610\n","Epoch 26/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9561 - val_loss: 4.9610\n","Epoch 27/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9561 - val_loss: 4.9603\n","Epoch 28/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9559 - val_loss: 4.9587\n","Epoch 29/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9557 - val_loss: 4.9584\n","Epoch 30/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9592 - val_loss: 4.9670\n","Epoch 31/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9569 - val_loss: 4.9609\n","Epoch 32/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9562 - val_loss: 4.9595\n","Epoch 33/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9558 - val_loss: 4.9589\n","Epoch 34/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9555 - val_loss: 4.9587\n","Epoch 35/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9554 - val_loss: 4.9583\n","Epoch 36/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9554 - val_loss: 4.9587\n","Epoch 37/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9552 - val_loss: 4.9589\n","Epoch 38/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9551 - val_loss: 4.9594\n","Epoch 39/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 40/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 41/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 42/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9547 - val_loss: 4.9575\n","Epoch 43/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9573\n","Epoch 44/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9545 - val_loss: 4.9568\n","Epoch 45/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9564\n","Epoch 46/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9543 - val_loss: 4.9566\n","Epoch 47/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 48/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9544 - val_loss: 4.9568\n","Epoch 49/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 50/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9569\n","Epoch 51/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 52/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 53/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9564\n","Epoch 54/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9566\n","Epoch 55/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9562\n","Epoch 56/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 57/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9560\n","Epoch 58/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 59/250\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 5.0193\n","Epoch 60/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9565 - val_loss: 4.9859\n","Epoch 61/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9553 - val_loss: 4.9662\n","Epoch 62/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9550 - val_loss: 4.9624\n","Epoch 63/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9554 - val_loss: 4.9608\n","Epoch 64/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9547 - val_loss: 4.9585\n","Epoch 65/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 66/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 67/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 68/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 69/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9550 - val_loss: 4.9609\n","Epoch 70/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9549 - val_loss: 4.9573\n","Epoch 71/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9542 - val_loss: 4.9564\n","Epoch 72/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 73/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 74/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9561\n","Epoch 75/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9590\n","Epoch 76/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9542 - val_loss: 4.9601\n","Epoch 77/250\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 78/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 79/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 80/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 81/250\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 82/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 83/250\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 84/250\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 85/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 86/250\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 87/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9587\n","Epoch 88/250\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9557\n","Epoch 89/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 90/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 91/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 92/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9560\n","Epoch 93/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 94/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9557\n","Epoch 95/250\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 96/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 97/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 98/250\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 99/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 100/250\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 101/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9555\n","Epoch 102/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 103/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 104/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 105/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 106/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 107/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 108/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 109/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9583\n","Epoch 110/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 111/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 112/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 113/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 114/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 115/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 116/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9589\n","Epoch 117/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 118/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 119/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 120/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 121/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9554\n","Epoch 122/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 123/250\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9554\n","Epoch 124/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9553\n","Epoch 125/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 126/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 127/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9553\n","Epoch 128/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 129/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 130/250\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 131/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 132/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 133/250\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 134/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 135/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 136/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 137/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 138/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 139/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 140/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 141/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 142/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 143/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 144/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 145/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 146/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 147/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 148/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 149/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 150/250\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 151/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 152/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 153/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 154/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 155/250\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 156/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 157/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 158/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 159/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 160/250\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 161/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 162/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 163/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 164/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 165/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 166/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 167/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 168/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9723\n","Epoch 169/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 170/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 171/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 172/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 173/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 174/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 175/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 176/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 177/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 178/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 179/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 180/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 181/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 182/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 183/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 184/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 185/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 186/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 187/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 188/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 189/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 190/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 191/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 192/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 193/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 194/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 195/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 196/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 197/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 198/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 199/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 200/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 201/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 202/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 203/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 204/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 205/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 206/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 207/250\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 208/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 209/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 210/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 211/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 212/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 213/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 214/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 215/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 216/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 217/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 218/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 219/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 220/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 221/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 222/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 223/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 224/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 225/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 226/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 227/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 228/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 229/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 230/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 231/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 232/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 233/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 234/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 235/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 236/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 237/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 238/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 239/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 240/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 241/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 242/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 243/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 244/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 245/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 246/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9556\n","Epoch 247/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 248/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 249/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 250/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9555\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 42919 0.5\n","The shape of N (6181, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7499405741691589\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9928934961962927\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/250\n","5562/5562 [==============================] - 6s 1ms/step - loss: 5.0573 - val_loss: 5.2236\n","Epoch 2/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9962 - val_loss: 5.0140\n","Epoch 3/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9835 - val_loss: 4.9874\n","Epoch 4/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9760 - val_loss: 4.9777\n","Epoch 5/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9725 - val_loss: 4.9732\n","Epoch 6/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9686 - val_loss: 4.9705\n","Epoch 7/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9668 - val_loss: 4.9698\n","Epoch 8/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9650 - val_loss: 4.9795\n","Epoch 9/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9642 - val_loss: 4.9712\n","Epoch 10/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9630 - val_loss: 4.9739\n","Epoch 11/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9617 - val_loss: 4.9708\n","Epoch 12/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9608 - val_loss: 4.9692\n","Epoch 13/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9611 - val_loss: 4.9672\n","Epoch 14/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9601 - val_loss: 4.9693\n","Epoch 15/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9602 - val_loss: 4.9771\n","Epoch 16/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9596 - val_loss: 4.9671\n","Epoch 17/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9585 - val_loss: 4.9666\n","Epoch 18/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9583 - val_loss: 4.9663\n","Epoch 19/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9581 - val_loss: 4.9653\n","Epoch 20/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9575 - val_loss: 4.9640\n","Epoch 21/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9569 - val_loss: 4.9627\n","Epoch 22/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9605 - val_loss: 5.0119\n","Epoch 23/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9600 - val_loss: 4.9913\n","Epoch 24/250\n","5562/5562 [==============================] - 2s 283us/step - loss: 4.9621 - val_loss: 4.9766\n","Epoch 25/250\n","5562/5562 [==============================] - 2s 283us/step - loss: 4.9594 - val_loss: 4.9674\n","Epoch 26/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9578 - val_loss: 4.9634\n","Epoch 27/250\n","5562/5562 [==============================] - 2s 282us/step - loss: 4.9570 - val_loss: 4.9611\n","Epoch 28/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9570 - val_loss: 4.9618\n","Epoch 29/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9569 - val_loss: 4.9593\n","Epoch 30/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9566 - val_loss: 4.9591\n","Epoch 31/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9561 - val_loss: 4.9584\n","Epoch 32/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9560 - val_loss: 4.9598\n","Epoch 33/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9561 - val_loss: 4.9587\n","Epoch 34/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9560 - val_loss: 4.9620\n","Epoch 35/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9563 - val_loss: 4.9596\n","Epoch 36/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9558 - val_loss: 4.9584\n","Epoch 37/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9557 - val_loss: 4.9579\n","Epoch 38/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9556 - val_loss: 4.9580\n","Epoch 39/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 40/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9553 - val_loss: 4.9576\n","Epoch 41/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9553 - val_loss: 4.9577\n","Epoch 42/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9550 - val_loss: 4.9573\n","Epoch 43/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9547 - val_loss: 4.9568\n","Epoch 44/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9548 - val_loss: 4.9566\n","Epoch 45/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9549 - val_loss: 4.9567\n","Epoch 46/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9547 - val_loss: 4.9567\n","Epoch 47/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9547 - val_loss: 4.9569\n","Epoch 48/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9547 - val_loss: 4.9569\n","Epoch 49/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9546 - val_loss: 4.9568\n","Epoch 50/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9566\n","Epoch 51/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9564\n","Epoch 52/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9544 - val_loss: 4.9565\n","Epoch 53/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9545 - val_loss: 4.9565\n","Epoch 54/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 55/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9542 - val_loss: 4.9566\n","Epoch 56/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9564\n","Epoch 57/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9542 - val_loss: 4.9565\n","Epoch 58/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9541 - val_loss: 4.9562\n","Epoch 59/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9563\n","Epoch 60/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9561\n","Epoch 61/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 62/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9546 - val_loss: 4.9590\n","Epoch 63/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9569 - val_loss: 4.9940\n","Epoch 64/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9561 - val_loss: 4.9646\n","Epoch 65/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9550 - val_loss: 4.9595\n","Epoch 66/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 67/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 68/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 69/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9563\n","Epoch 70/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9563\n","Epoch 71/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 72/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9562\n","Epoch 73/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9562\n","Epoch 74/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9560 - val_loss: 4.9616\n","Epoch 75/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9547 - val_loss: 4.9574\n","Epoch 76/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9544 - val_loss: 4.9564\n","Epoch 77/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9542 - val_loss: 4.9563\n","Epoch 78/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9560\n","Epoch 79/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9558\n","Epoch 80/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9559\n","Epoch 81/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 82/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 83/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9590\n","Epoch 84/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 85/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 86/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9564\n","Epoch 87/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 88/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9556\n","Epoch 89/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9560\n","Epoch 90/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 91/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 92/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 93/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 94/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 95/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 96/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 97/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9557\n","Epoch 98/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9558\n","Epoch 99/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 100/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 101/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 102/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 103/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 104/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 105/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 106/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 107/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9541 - val_loss: 4.9565\n","Epoch 108/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9588\n","Epoch 109/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 110/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 111/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 112/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9561\n","Epoch 113/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9556\n","Epoch 114/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9556\n","Epoch 115/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9555\n","Epoch 116/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9554\n","Epoch 117/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9555\n","Epoch 118/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9555\n","Epoch 119/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 120/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 121/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 122/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 123/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9554\n","Epoch 124/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9555\n","Epoch 125/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9554\n","Epoch 126/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 127/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 128/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 129/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9647\n","Epoch 130/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9543 - val_loss: 4.9589\n","Epoch 131/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 132/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 133/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 134/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9558\n","Epoch 135/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 136/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 137/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 138/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 139/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9555\n","Epoch 140/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9554\n","Epoch 141/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 142/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 143/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 144/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 145/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 146/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 147/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9554\n","Epoch 148/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9605\n","Epoch 149/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 150/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 151/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 152/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 153/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9554\n","Epoch 154/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 155/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 156/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 157/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9554\n","Epoch 158/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9554\n","Epoch 159/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 160/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 161/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 162/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 163/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 164/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 165/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 166/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 167/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 168/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 169/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 170/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9588\n","Epoch 171/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 172/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 173/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 174/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 175/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 176/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 177/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 178/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 179/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 180/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 181/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 182/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 183/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 184/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 185/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 186/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 187/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 188/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 189/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 190/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 191/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 192/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 193/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 194/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 195/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 196/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 197/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 198/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 199/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 200/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 201/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 202/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 203/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 204/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 205/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 206/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 207/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 208/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 209/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 210/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 211/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 212/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 213/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 214/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 215/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 216/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 217/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 218/250\n","5562/5562 [==============================] - 2s 283us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 219/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 220/250\n","5562/5562 [==============================] - 2s 283us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 221/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 222/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 223/250\n","5562/5562 [==============================] - 2s 283us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 224/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 225/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 226/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 227/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 228/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 229/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 230/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 231/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 232/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 233/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 234/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 235/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 236/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 237/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 238/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 239/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 240/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 241/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 242/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 243/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 244/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 245/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 246/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 247/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 248/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 249/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 250/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9555\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 45004 0.5\n","The shape of N (6181, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9897165970213222\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/250\n","5562/5562 [==============================] - 6s 1ms/step - loss: 5.0614 - val_loss: 5.1054\n","Epoch 2/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9983 - val_loss: 5.0017\n","Epoch 3/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9843 - val_loss: 4.9856\n","Epoch 4/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9769 - val_loss: 4.9779\n","Epoch 5/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9722 - val_loss: 4.9761\n","Epoch 6/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9686 - val_loss: 4.9723\n","Epoch 7/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9665 - val_loss: 4.9717\n","Epoch 8/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9641 - val_loss: 4.9696\n","Epoch 9/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9634 - val_loss: 4.9716\n","Epoch 10/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9622 - val_loss: 4.9688\n","Epoch 11/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9609 - val_loss: 4.9664\n","Epoch 12/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9601 - val_loss: 4.9680\n","Epoch 13/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9596 - val_loss: 4.9679\n","Epoch 14/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9598 - val_loss: 4.9710\n","Epoch 15/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9597 - val_loss: 4.9690\n","Epoch 16/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9586 - val_loss: 4.9905\n","Epoch 17/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9587 - val_loss: 4.9740\n","Epoch 18/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9580 - val_loss: 4.9687\n","Epoch 19/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9574 - val_loss: 4.9678\n","Epoch 20/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9578 - val_loss: 4.9649\n","Epoch 21/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9573 - val_loss: 4.9648\n","Epoch 22/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9567 - val_loss: 4.9635\n","Epoch 23/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9564 - val_loss: 4.9653\n","Epoch 24/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9564 - val_loss: 4.9618\n","Epoch 25/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9563 - val_loss: 4.9646\n","Epoch 26/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9561 - val_loss: 4.9601\n","Epoch 27/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9562 - val_loss: 4.9600\n","Epoch 28/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9558 - val_loss: 4.9589\n","Epoch 29/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9556 - val_loss: 4.9588\n","Epoch 30/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9554 - val_loss: 4.9579\n","Epoch 31/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9551 - val_loss: 4.9602\n","Epoch 32/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 33/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 34/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 35/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9548 - val_loss: 4.9619\n","Epoch 36/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9550 - val_loss: 4.9597\n","Epoch 37/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 38/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9547 - val_loss: 4.9574\n","Epoch 39/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 40/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9546 - val_loss: 4.9573\n","Epoch 41/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9546 - val_loss: 4.9570\n","Epoch 42/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 43/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9549 - val_loss: 4.9591\n","Epoch 44/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 45/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 46/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9567\n","Epoch 47/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 48/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 49/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9610\n","Epoch 50/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 51/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 52/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 53/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 54/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 55/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 56/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 57/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9546 - val_loss: 4.9745\n","Epoch 58/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9544 - val_loss: 4.9623\n","Epoch 59/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9550 - val_loss: 4.9687\n","Epoch 60/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9547 - val_loss: 4.9613\n","Epoch 61/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9556 - val_loss: 4.9626\n","Epoch 62/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9544 - val_loss: 4.9585\n","Epoch 63/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 64/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 65/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 66/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 67/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 68/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9538 - val_loss: 4.9616\n","Epoch 69/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9588\n","Epoch 70/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 71/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 72/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9557\n","Epoch 73/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9557\n","Epoch 74/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 75/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9555\n","Epoch 76/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 77/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 78/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9555\n","Epoch 79/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9554\n","Epoch 80/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 81/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 82/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 83/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 84/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9623\n","Epoch 85/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 86/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 87/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 88/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 89/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 90/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 91/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 92/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 93/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 94/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 95/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 96/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 97/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 98/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 99/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 100/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 101/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 102/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9695\n","Epoch 103/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9536 - val_loss: 4.9610\n","Epoch 104/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 105/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 106/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 107/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9554\n","Epoch 108/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9552\n","Epoch 109/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 110/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9553\n","Epoch 111/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9552\n","Epoch 112/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 113/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 114/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 115/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 116/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9552\n","Epoch 117/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 118/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 119/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 120/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 121/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 122/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9552\n","Epoch 123/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 124/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9550\n","Epoch 125/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9551\n","Epoch 126/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 127/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 128/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9554 - val_loss: 4.9920\n","Epoch 129/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9642\n","Epoch 130/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9598\n","Epoch 131/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 132/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 133/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 134/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9551\n","Epoch 135/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9551\n","Epoch 136/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9550\n","Epoch 137/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9550\n","Epoch 138/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9550\n","Epoch 139/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9551\n","Epoch 140/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9550\n","Epoch 141/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 142/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 143/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 144/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9552\n","Epoch 145/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9552\n","Epoch 146/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 147/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9552\n","Epoch 148/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 149/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 150/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 151/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9551\n","Epoch 152/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9524 - val_loss: 4.9550\n","Epoch 153/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 154/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9550\n","Epoch 155/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9551\n","Epoch 156/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 157/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9548\n","Epoch 158/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 159/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9550\n","Epoch 160/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 161/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 162/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 163/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9550\n","Epoch 164/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 165/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9548\n","Epoch 166/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9549\n","Epoch 167/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9549\n","Epoch 168/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9549\n","Epoch 169/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 170/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 171/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9551\n","Epoch 172/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9756\n","Epoch 173/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9546 - val_loss: 4.9765\n","Epoch 174/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9592\n","Epoch 175/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 176/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 177/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 178/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9551\n","Epoch 179/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9551\n","Epoch 180/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9549\n","Epoch 181/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9550\n","Epoch 182/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9551\n","Epoch 183/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9550\n","Epoch 184/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9549\n","Epoch 185/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9551\n","Epoch 186/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9550\n","Epoch 187/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9551\n","Epoch 188/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9550\n","Epoch 189/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9550\n","Epoch 190/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 191/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 192/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 193/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9551\n","Epoch 194/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 195/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9550\n","Epoch 196/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9550\n","Epoch 197/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9549\n","Epoch 198/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9550\n","Epoch 199/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9551\n","Epoch 200/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9551\n","Epoch 201/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9551\n","Epoch 202/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9520 - val_loss: 4.9549\n","Epoch 203/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 204/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9550\n","Epoch 205/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9550\n","Epoch 206/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9549\n","Epoch 207/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 208/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 209/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9624\n","Epoch 210/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9630\n","Epoch 211/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 212/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 213/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 214/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9521 - val_loss: 4.9550\n","Epoch 215/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9549\n","Epoch 216/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9522 - val_loss: 4.9549\n","Epoch 217/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9521 - val_loss: 4.9549\n","Epoch 218/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9520 - val_loss: 4.9548\n","Epoch 219/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 220/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9550\n","Epoch 221/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9520 - val_loss: 4.9549\n","Epoch 222/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 223/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 224/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 225/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9521 - val_loss: 4.9549\n","Epoch 226/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 227/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9519 - val_loss: 4.9549\n","Epoch 228/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 229/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9520 - val_loss: 4.9549\n","Epoch 230/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9519 - val_loss: 4.9548\n","Epoch 231/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 232/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9519 - val_loss: 4.9549\n","Epoch 233/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 234/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 235/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 236/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9548\n","Epoch 237/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 238/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9519 - val_loss: 4.9549\n","Epoch 239/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 240/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 241/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 242/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 243/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9519 - val_loss: 4.9549\n","Epoch 244/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 245/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9549\n","Epoch 246/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9518 - val_loss: 4.9549\n","Epoch 247/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9518 - val_loss: 4.9549\n","Epoch 248/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 249/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 250/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9549\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 34806 0.5\n","The shape of N (6181, 784)\n","The minimum value of N  -0.7455748319625854\n","The max value of N 0.7499339580535889\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9948007071681131\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/250\n","5562/5562 [==============================] - 7s 1ms/step - loss: 5.0785 - val_loss: 5.1356\n","Epoch 2/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 5.0019 - val_loss: 5.0071\n","Epoch 3/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9845 - val_loss: 4.9862\n","Epoch 4/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9754 - val_loss: 4.9808\n","Epoch 5/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9700 - val_loss: 4.9768\n","Epoch 6/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9669 - val_loss: 4.9704\n","Epoch 7/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9653 - val_loss: 4.9708\n","Epoch 8/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9634 - val_loss: 4.9684\n","Epoch 9/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9625 - val_loss: 4.9755\n","Epoch 10/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9617 - val_loss: 4.9673\n","Epoch 11/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9609 - val_loss: 4.9669\n","Epoch 12/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9602 - val_loss: 4.9645\n","Epoch 13/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9596 - val_loss: 4.9661\n","Epoch 14/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9591 - val_loss: 4.9655\n","Epoch 15/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9585 - val_loss: 4.9639\n","Epoch 16/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9584 - val_loss: 4.9639\n","Epoch 17/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9579 - val_loss: 4.9644\n","Epoch 18/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9574 - val_loss: 4.9626\n","Epoch 19/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9572 - val_loss: 4.9619\n","Epoch 20/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9570 - val_loss: 4.9617\n","Epoch 21/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9566 - val_loss: 4.9615\n","Epoch 22/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9566 - val_loss: 4.9636\n","Epoch 23/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9565 - val_loss: 4.9621\n","Epoch 24/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9561 - val_loss: 4.9608\n","Epoch 25/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9561 - val_loss: 4.9619\n","Epoch 26/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9563 - val_loss: 4.9600\n","Epoch 27/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9558 - val_loss: 4.9596\n","Epoch 28/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9557 - val_loss: 4.9604\n","Epoch 29/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9555 - val_loss: 4.9591\n","Epoch 30/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9557 - val_loss: 4.9593\n","Epoch 31/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9553 - val_loss: 4.9589\n","Epoch 32/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9552 - val_loss: 4.9584\n","Epoch 33/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9550 - val_loss: 4.9583\n","Epoch 34/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9550 - val_loss: 4.9584\n","Epoch 35/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9549 - val_loss: 4.9577\n","Epoch 36/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9547 - val_loss: 4.9606\n","Epoch 37/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9550 - val_loss: 4.9574\n","Epoch 38/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9548 - val_loss: 4.9571\n","Epoch 39/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 40/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 41/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9548 - val_loss: 4.9569\n","Epoch 42/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9546 - val_loss: 4.9587\n","Epoch 43/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9546 - val_loss: 4.9569\n","Epoch 44/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 45/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9543 - val_loss: 4.9565\n","Epoch 46/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 47/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 48/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 49/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 50/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 51/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 52/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9561\n","Epoch 53/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9539 - val_loss: 4.9563\n","Epoch 54/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 55/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 56/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 57/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9671\n","Epoch 58/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 59/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 60/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 61/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9564\n","Epoch 62/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 63/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 64/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 65/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9559\n","Epoch 66/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 67/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 68/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 69/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 70/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 71/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 72/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 73/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 74/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9560\n","Epoch 75/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9558\n","Epoch 76/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 77/250\n","5562/5562 [==============================] - 2s 283us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 78/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 79/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 80/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 81/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 82/250\n","5562/5562 [==============================] - 2s 283us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 83/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 84/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 85/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 86/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 87/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 88/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 89/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 90/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 91/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 92/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 93/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 94/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 95/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 96/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 97/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 98/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 99/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 100/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 101/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 102/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 103/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 104/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 105/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 106/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 107/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 108/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 109/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 110/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 111/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 112/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 113/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 114/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 115/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 116/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 117/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 118/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 119/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 120/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 121/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 122/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 123/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 124/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 125/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 126/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 127/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 128/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 129/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 130/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 131/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 132/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 133/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 134/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 135/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 136/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 137/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 138/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 139/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 140/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 141/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 142/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 143/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 144/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 145/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 146/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 147/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 148/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 149/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 150/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 151/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 152/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 153/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 154/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 155/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 156/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 157/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 158/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 159/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 160/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 161/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 162/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 163/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 164/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 165/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 166/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 167/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 168/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 169/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 170/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 171/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 172/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 173/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 174/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 175/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 176/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 177/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 178/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 179/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 180/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 181/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 182/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 183/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 184/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 185/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 186/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 187/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 188/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 189/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 190/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 191/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 192/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 193/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 194/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 195/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 196/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 197/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 198/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 199/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 200/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 201/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 202/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 203/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 204/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 205/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 206/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 207/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 208/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 209/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 210/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 211/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 212/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 213/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 214/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 215/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9557\n","Epoch 216/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9561\n","Epoch 217/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 218/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 219/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 220/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 221/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 222/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 223/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 224/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 225/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 226/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 227/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 228/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 229/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 230/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9521 - val_loss: 4.9557\n","Epoch 231/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 232/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 233/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9559\n","Epoch 234/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 235/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 236/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 237/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 238/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9561\n","Epoch 239/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 240/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 241/250\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 242/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 243/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 244/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 245/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 246/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 247/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 248/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9561\n","Epoch 249/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 250/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9554\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 35003 0.5\n","The shape of N (6181, 784)\n","The minimum value of N  -0.7396372556686401\n","The max value of N 0.7478069067001343\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9916907746705239\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/250\n","5562/5562 [==============================] - 7s 1ms/step - loss: 5.0671 - val_loss: 5.2086\n","Epoch 2/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9976 - val_loss: 5.0167\n","Epoch 3/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9829 - val_loss: 4.9919\n","Epoch 4/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9760 - val_loss: 4.9829\n","Epoch 5/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9712 - val_loss: 4.9911\n","Epoch 6/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9679 - val_loss: 4.9738\n","Epoch 7/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9652 - val_loss: 4.9720\n","Epoch 8/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9632 - val_loss: 4.9695\n","Epoch 9/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9650 - val_loss: 4.9735\n","Epoch 10/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9618 - val_loss: 4.9691\n","Epoch 11/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9600 - val_loss: 4.9673\n","Epoch 12/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9602 - val_loss: 4.9737\n","Epoch 13/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9598 - val_loss: 4.9667\n","Epoch 14/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9590 - val_loss: 4.9675\n","Epoch 15/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9585 - val_loss: 4.9663\n","Epoch 16/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9580 - val_loss: 4.9682\n","Epoch 17/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9578 - val_loss: 4.9652\n","Epoch 18/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9583 - val_loss: 4.9649\n","Epoch 19/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9571 - val_loss: 4.9638\n","Epoch 20/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9571 - val_loss: 4.9622\n","Epoch 21/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9563 - val_loss: 4.9610\n","Epoch 22/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9561 - val_loss: 4.9635\n","Epoch 23/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9558 - val_loss: 4.9632\n","Epoch 24/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9560 - val_loss: 4.9616\n","Epoch 25/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9556 - val_loss: 4.9598\n","Epoch 26/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9555 - val_loss: 4.9594\n","Epoch 27/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9553 - val_loss: 4.9593\n","Epoch 28/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9550 - val_loss: 4.9582\n","Epoch 29/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 30/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9563 - val_loss: 4.9685\n","Epoch 31/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9566 - val_loss: 4.9633\n","Epoch 32/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9555 - val_loss: 4.9615\n","Epoch 33/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9561 - val_loss: 4.9599\n","Epoch 34/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 35/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9549 - val_loss: 4.9591\n","Epoch 36/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9546 - val_loss: 4.9573\n","Epoch 37/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9567\n","Epoch 38/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9565\n","Epoch 39/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 40/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9592\n","Epoch 41/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 42/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 43/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 44/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9561\n","Epoch 45/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 46/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9598 - val_loss: 5.0045\n","Epoch 47/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9579 - val_loss: 4.9749\n","Epoch 48/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9562 - val_loss: 4.9696\n","Epoch 49/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9555 - val_loss: 4.9605\n","Epoch 50/250\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9550 - val_loss: 4.9588\n","Epoch 51/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9546 - val_loss: 4.9599\n","Epoch 52/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 53/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 54/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 55/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 56/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 57/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9562\n","Epoch 58/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 59/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9546 - val_loss: 4.9570\n","Epoch 60/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9542 - val_loss: 4.9562\n","Epoch 61/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9564\n","Epoch 62/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 63/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 64/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 65/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 66/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 67/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9535 - val_loss: 4.9558\n","Epoch 68/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9583\n","Epoch 69/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 70/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 71/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9592\n","Epoch 72/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 73/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9596\n","Epoch 74/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 75/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 76/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 77/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 78/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 79/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 80/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 81/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 82/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 83/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9551\n","Epoch 84/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 85/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9552\n","Epoch 86/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 87/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9553\n","Epoch 88/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 89/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 90/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 91/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 92/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 93/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 94/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 95/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 96/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 97/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9549\n","Epoch 98/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9553\n","Epoch 99/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 100/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 101/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 102/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 103/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 104/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 105/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9551\n","Epoch 106/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9552\n","Epoch 107/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 108/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9551\n","Epoch 109/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9549\n","Epoch 110/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9551\n","Epoch 111/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 112/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 113/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9550\n","Epoch 114/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9551\n","Epoch 115/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9552\n","Epoch 116/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 117/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9550\n","Epoch 118/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 119/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 120/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 121/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 122/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 123/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 124/250\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9550\n","Epoch 125/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 126/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 127/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 128/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 129/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 130/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 131/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9551\n","Epoch 132/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9551\n","Epoch 133/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9551\n","Epoch 134/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 135/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9524 - val_loss: 4.9582\n","Epoch 136/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 137/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 138/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 139/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 140/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9550\n","Epoch 141/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9551\n","Epoch 142/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9549\n","Epoch 143/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9522 - val_loss: 4.9550\n","Epoch 144/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9549\n","Epoch 145/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9549\n","Epoch 146/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 147/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 148/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 149/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9550\n","Epoch 150/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 151/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 152/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9522 - val_loss: 4.9551\n","Epoch 153/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 154/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 155/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 156/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 157/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 158/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 159/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 160/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9520 - val_loss: 4.9549\n","Epoch 161/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 162/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 163/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 164/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 165/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 166/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 167/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9519 - val_loss: 4.9549\n","Epoch 168/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9519 - val_loss: 4.9556\n","Epoch 169/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 170/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 171/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 172/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 173/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 174/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 175/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 176/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9518 - val_loss: 4.9549\n","Epoch 177/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9552\n","Epoch 178/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 179/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 180/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 181/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 182/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 183/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 184/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 185/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 186/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9549\n","Epoch 187/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 188/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9518 - val_loss: 4.9552\n","Epoch 189/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 190/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9518 - val_loss: 4.9558\n","Epoch 191/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9518 - val_loss: 4.9553\n","Epoch 192/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9518 - val_loss: 4.9553\n","Epoch 193/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9519 - val_loss: 4.9556\n","Epoch 194/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 195/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 196/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9567\n","Epoch 197/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 198/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9519 - val_loss: 4.9556\n","Epoch 199/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 200/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9518 - val_loss: 4.9553\n","Epoch 201/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9518 - val_loss: 4.9551\n","Epoch 202/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 203/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9518 - val_loss: 4.9549\n","Epoch 204/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9517 - val_loss: 4.9551\n","Epoch 205/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 206/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 207/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9517 - val_loss: 4.9551\n","Epoch 208/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9550\n","Epoch 209/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9550\n","Epoch 210/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9516 - val_loss: 4.9548\n","Epoch 211/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 212/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9549\n","Epoch 213/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9552\n","Epoch 214/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 215/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9516 - val_loss: 4.9549\n","Epoch 216/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 217/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9517 - val_loss: 4.9551\n","Epoch 218/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9516 - val_loss: 4.9549\n","Epoch 219/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9549\n","Epoch 220/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9515 - val_loss: 4.9551\n","Epoch 221/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9516 - val_loss: 4.9551\n","Epoch 222/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9516 - val_loss: 4.9552\n","Epoch 223/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9516 - val_loss: 4.9552\n","Epoch 224/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 225/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9517 - val_loss: 4.9550\n","Epoch 226/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 227/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9516 - val_loss: 4.9552\n","Epoch 228/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9516 - val_loss: 4.9549\n","Epoch 229/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9516 - val_loss: 4.9551\n","Epoch 230/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 231/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 232/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9516 - val_loss: 4.9549\n","Epoch 233/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9551\n","Epoch 234/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9551\n","Epoch 235/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 236/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9516 - val_loss: 4.9552\n","Epoch 237/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9515 - val_loss: 4.9550\n","Epoch 238/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9515 - val_loss: 4.9550\n","Epoch 239/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9515 - val_loss: 4.9550\n","Epoch 240/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 241/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9515 - val_loss: 4.9555\n","Epoch 242/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9517 - val_loss: 4.9560\n","Epoch 243/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9515 - val_loss: 4.9552\n","Epoch 244/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9552\n","Epoch 245/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9515 - val_loss: 4.9550\n","Epoch 246/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9516 - val_loss: 4.9550\n","Epoch 247/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9515 - val_loss: 4.9552\n","Epoch 248/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9515 - val_loss: 4.9550\n","Epoch 249/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9514 - val_loss: 4.9550\n","Epoch 250/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9515 - val_loss: 4.9550\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 45188 0.5\n","The shape of N (6181, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.994471231115397\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/250\n","5562/5562 [==============================] - 8s 1ms/step - loss: 5.0612 - val_loss: 5.1202\n","Epoch 2/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9963 - val_loss: 5.0008\n","Epoch 3/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9831 - val_loss: 4.9877\n","Epoch 4/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9756 - val_loss: 4.9800\n","Epoch 5/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9718 - val_loss: 4.9761\n","Epoch 6/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9681 - val_loss: 4.9724\n","Epoch 7/250\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9682 - val_loss: 4.9911\n","Epoch 8/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9674 - val_loss: 4.9747\n","Epoch 9/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9635 - val_loss: 4.9682\n","Epoch 10/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9638 - val_loss: 5.0007\n","Epoch 11/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9626 - val_loss: 4.9691\n","Epoch 12/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9610 - val_loss: 4.9690\n","Epoch 13/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9607 - val_loss: 4.9695\n","Epoch 14/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9597 - val_loss: 4.9674\n","Epoch 15/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9590 - val_loss: 4.9651\n","Epoch 16/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9584 - val_loss: 4.9658\n","Epoch 17/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9588 - val_loss: 4.9806\n","Epoch 18/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9604 - val_loss: 4.9655\n","Epoch 19/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9586 - val_loss: 4.9638\n","Epoch 20/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9580 - val_loss: 4.9620\n","Epoch 21/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9574 - val_loss: 4.9624\n","Epoch 22/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9570 - val_loss: 4.9625\n","Epoch 23/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9571 - val_loss: 4.9634\n","Epoch 24/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9568 - val_loss: 4.9609\n","Epoch 25/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9571 - val_loss: 4.9613\n","Epoch 26/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9565 - val_loss: 4.9604\n","Epoch 27/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9563 - val_loss: 4.9596\n","Epoch 28/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9560 - val_loss: 4.9603\n","Epoch 29/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9559 - val_loss: 4.9597\n","Epoch 30/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9559 - val_loss: 4.9600\n","Epoch 31/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9558 - val_loss: 4.9591\n","Epoch 32/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9557 - val_loss: 4.9582\n","Epoch 33/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9557 - val_loss: 4.9580\n","Epoch 34/250\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 35/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9554 - val_loss: 4.9581\n","Epoch 36/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9611\n","Epoch 37/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 38/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9581 - val_loss: 4.9831\n","Epoch 39/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9581 - val_loss: 4.9641\n","Epoch 40/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9565 - val_loss: 4.9596\n","Epoch 41/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9563 - val_loss: 4.9641\n","Epoch 42/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9566 - val_loss: 4.9731\n","Epoch 43/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9557 - val_loss: 4.9648\n","Epoch 44/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9555 - val_loss: 4.9609\n","Epoch 45/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9553 - val_loss: 4.9590\n","Epoch 46/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9553 - val_loss: 4.9598\n","Epoch 47/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 48/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9550 - val_loss: 4.9623\n","Epoch 49/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9558 - val_loss: 4.9756\n","Epoch 50/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9552 - val_loss: 4.9587\n","Epoch 51/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9549 - val_loss: 4.9584\n","Epoch 52/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9548 - val_loss: 4.9571\n","Epoch 53/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9548 - val_loss: 4.9571\n","Epoch 54/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9552 - val_loss: 4.9606\n","Epoch 55/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 56/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9547 - val_loss: 4.9566\n","Epoch 57/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9597 - val_loss: 4.9990\n","Epoch 58/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9564 - val_loss: 4.9647\n","Epoch 59/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9555 - val_loss: 4.9593\n","Epoch 60/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 61/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9549 - val_loss: 4.9573\n","Epoch 62/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9548 - val_loss: 4.9570\n","Epoch 63/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9544 - val_loss: 4.9565\n","Epoch 64/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9546 - val_loss: 4.9565\n","Epoch 65/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9567\n","Epoch 66/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9566\n","Epoch 67/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9542 - val_loss: 4.9562\n","Epoch 68/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9542 - val_loss: 4.9612\n","Epoch 69/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9546 - val_loss: 4.9597\n","Epoch 70/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 71/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9544 - val_loss: 4.9567\n","Epoch 72/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9564\n","Epoch 73/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 4.9559\n","Epoch 74/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9561\n","Epoch 75/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9560\n","Epoch 76/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9560\n","Epoch 77/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 78/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9558\n","Epoch 79/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9559\n","Epoch 80/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 81/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9558\n","Epoch 82/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9557\n","Epoch 83/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9557\n","Epoch 84/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9558\n","Epoch 85/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9556\n","Epoch 86/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9558\n","Epoch 87/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9557\n","Epoch 88/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9558\n","Epoch 89/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9559\n","Epoch 90/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9558\n","Epoch 91/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9559\n","Epoch 92/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9562\n","Epoch 93/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9558\n","Epoch 94/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9536 - val_loss: 4.9556\n","Epoch 95/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9557\n","Epoch 96/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9558\n","Epoch 97/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9557\n","Epoch 98/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 99/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 100/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9546 - val_loss: 4.9610\n","Epoch 101/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 102/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 103/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9555\n","Epoch 104/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9536 - val_loss: 4.9554\n","Epoch 105/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 106/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9649\n","Epoch 107/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9540 - val_loss: 4.9609\n","Epoch 108/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 109/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 110/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 111/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9556\n","Epoch 112/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9558\n","Epoch 113/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 114/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9534 - val_loss: 4.9555\n","Epoch 115/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9636\n","Epoch 116/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9543 - val_loss: 4.9586\n","Epoch 117/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 118/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 119/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9564\n","Epoch 120/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 121/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9557\n","Epoch 122/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9556\n","Epoch 123/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9556\n","Epoch 124/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9535 - val_loss: 4.9555\n","Epoch 125/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9556\n","Epoch 126/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9535 - val_loss: 4.9555\n","Epoch 127/250\n","5562/5562 [==============================] - 2s 284us/step - loss: 4.9534 - val_loss: 4.9556\n","Epoch 128/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9555\n","Epoch 129/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9555\n","Epoch 130/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9554\n","Epoch 131/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9540 - val_loss: 4.9633\n","Epoch 132/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9587\n","Epoch 133/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 134/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 135/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 136/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 137/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9560\n","Epoch 138/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 139/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 140/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 141/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 142/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 143/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 144/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 145/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 146/250\n","5562/5562 [==============================] - 2s 285us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 147/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 148/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 149/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 150/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 151/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 152/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 153/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 154/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 155/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 156/250\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 157/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 158/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 159/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 160/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 161/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 162/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 163/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 164/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 165/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 166/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 167/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 168/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 169/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 170/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 171/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 172/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 173/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 174/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 175/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 176/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 177/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 178/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 179/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 180/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 181/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 182/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 183/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 184/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 185/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 186/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 187/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 188/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 189/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 190/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 191/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 192/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 193/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 194/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 195/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 196/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 197/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 198/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 199/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 200/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 201/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 202/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 203/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 204/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 205/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 206/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 207/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 208/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 209/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 210/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 211/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 212/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 213/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 214/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 215/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 216/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 217/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 218/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 219/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 220/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 221/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 222/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 223/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 224/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 225/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 226/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 227/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 228/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 229/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 230/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 231/250\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 232/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 233/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 234/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 235/250\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 236/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 237/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 238/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 239/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 240/250\n","5562/5562 [==============================] - 2s 286us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 241/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 242/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 243/250\n","5562/5562 [==============================] - 2s 287us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 244/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 245/250\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 246/250\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 247/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 248/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 249/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 250/250\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9559\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 54629 0.5\n","The shape of N (6181, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9897058823529412\n","=======================\n","Saved model to disk....\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.9934453016179149, 0.9922800814314796, 0.9933140469302475, 0.993351548269581, 0.9928934961962927, 0.9897165970213222, 0.9948007071681131, 0.9916907746705239, 0.994471231115397, 0.9897058823529412]\n","AUROC ===== 0.9925669666773814 +/- 0.0016681737579846468\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3XmcHGd94P/PU1V9zakZaaSRfMvH\nYxvs2GACjjlMOJJs4JcQIPsL7EIOSEISlmyWZHOQDf6RheSVEJKQkAAJbLIkYByMD26DbdlgsI18\nSrZL9zkazT19d13P74+q7umRZqSZ0bQO1/f9eknTU9VV/VRPd33r+1yljDEIIYQQANaZLoAQQoiz\nhwQFIYQQLRIUhBBCtEhQEEII0SJBQQghRIsEBSGEEC0SFIQ4BVrrf9Jaf/Akz/lFrfW3l7pciDNJ\ngoIQQogW50wXQIjTRWt9MfB94GPArwAKeAfwx8B1wDdd1/3l5LlvBf6E+DsyArzbdd3dWuu1wOeB\ny4FngCpwKNnmauAfgI1AA/gl13V/uMSyDQL/CPwIEAL/4rrunyfr/hR4a1LeQ8B/cV13ZLHlK31/\nhADJFET6rANGXdfVwFPArcA7gWuBt2mtL9VaXwh8GvhZ13WvBL4KfDLZ/n8C467rXgL8JvATAFpr\nC7gD+FfXda8Afh24U2u91AuvDwPTSbleDvyG1vrlWusXAD8PvDDZ75eB1y62fOVvixAxCQoibRzg\ntuTx08CjrutOuK47CRwBNgGvA+5zXXdX8rx/Al6dnOBfCXwRwHXdfcCW5DlXAuuBzyTrvgeMAz+2\nxHL9NPCJZNsp4Hbg9cAMMAS8XWs94Lrux13X/dcTLBfilEhQEGkTuq5baz4Gyu3rAJv4ZDvdXOi6\n7ixxFc06YBCYbdum+bw1QBfwrNb6Oa31c8RBYu0SyzXvNZPH613XPQz8HHE10QGt9Ve11hcstnyJ\nryXEoqRNQYjjHQVubP6itR4AImCC+GTd3/bcIWAPcbtDMalumkdr/YtLfM21wIHk97XJMlzXvQ+4\nT2vdDfwl8GfA2xdbvuSjFGIBkikIcbx7gFdqrTcnv/868C3XdQPihuo3AWitLyWu/wfYDxzSWr8l\nWbdOa/355IS9FF8BfrW5LXEW8FWt9eu11n+vtbZc160ATwJmseWneuBCSFAQ4hiu6x4C3kXcUPwc\ncTvCryWrPwJcpLXeC3ycuO4f13UN8P8Cv5Vs8wDwneSEvRQfAAbatv0z13UfSR53ATu01tuB/wz8\nrxMsF+KUKLmfghBCiCbJFIQQQrRIUBBCCNEiQUEIIUSLBAUhhBAt5/w4hfHx0opbygcGupierq5m\ncc56cszpkMZjhnQe90qPeWioVy20PNWZguPYZ7oIp50cczqk8Zghnce92sec6qAghBBiPgkKQggh\nWiQoCCGEaJGgIIQQokWCghBCiBYJCkIIIVokKAghhGg55wevrdS2qTK5hsflueyZLooQQpw1Upsp\n3DsyyX88d/hMF0MIIQC4//7vLOl5f/M3H2VkpHPnrtQGBQNEci8JIcRZ4MiREb797W8u6bnve9//\nYNOm8zpWltRWHylAYoIQ4mzwV3/15zz77HZe8YqX8PrX/xRHjozw13/9CT7ykf+P8fExarUav/zL\nv8pNN72C3/qtX+V3fuf3uO++71CplBkdPczevfv4b//tf3DjjTedclnSGxSUwsgtbYUQx/jivbt4\n9LmxVd3nS65cz8//+GWLrv+FX/iv3H77F7nkkks5cGAfn/jEPzE9PcWP/ujL+KmfegOHDx/ij//4\n97npplfM225s7Cif/vSnufvub3LnnV+SoHAqFBBJTBBCnGWuuuoFAPT29vHss9u5667bUcqiWJw9\n7rnXXnsdAOvXr6dcLq/K66c6KEhMEEIc6+d//LITXtV3WiaTAeCee75BsVjk7//+nygWi7zrXf/1\nuOfa9twMqWaV6sNT29Cs1Oq9iUIIcSosyyIMw3nLZmZm2LhxE5ZlsWXLvfi+f3rKclpe5SykUJIp\nCCHOChdddAmu+xyVylwV0M03/zgPPfQg73vfeygUCqxfv57PfvbTHS+LOtevlld657V/fPYghyp1\n/vSGy1e7SGe1oaFexsdLZ7oYp5Ucc3qk8bhXesxy57VjSJdUIYQ4XmqDQiUIMUi7ghBCtEttUJj1\nAkB6IAkhRLvUBoVmZZoEBSGEmJPaoNAktUdCCDGnY4PXtNY3A7cB25NFT7uu+9629a8GPgKEgAu8\ny3XdSGv9MeBlxBfx73Nd99FOlG8uUzBtvwkhRLp1OlPY4rruzcm/9x6z7lPAW1zXvQnoBX5Sa/0q\n4HLXdW8EfgX4246VTMWBQDIFIcTZYKlTZzc98cRjTE9PrXo5zmT10Ytd1z2UPB4H1gKvAe4AcF33\nWWBAa93XiReXNgUhxNliOVNnN331q3d1JCh0eu6jq7XWdwGDwC2u697TXOG6bhFAa70ReD3wx8TV\nSVvbth8HhoHiYi8wMNCF49iLrV6UlWQKa9f2UMgsf/tz2dBQ75kuwmknx5we5+Jx/9EffZSnnnqK\nW2/9F3bs2MHs7CxhGPKBD3yAK6+8kk996lPcc889WJbFq1/9aq655hq++90tHDy4j49//ONs2rRp\n1crSyaCwE7gF+CKwGbhPa32Z67pe8wla6/XA3cBvuK47qbU+dh8nreyfnq6uqHDN8QnjEyUKKwgq\n5yoZ8ZkOaTxmWJ3jvn3XV3h87OlVKlHs+vXX8HOXvWHR9W9+8y+glE2t5nPddS/hjW/8Wfbu3cOH\nPvS/+eu//gT//M//zB13fAPbtrnjji9xxRXXcumll/M7v/N7bNq0aaUjmhdc3rGg4LruYeDW5Nfd\nWutR4DxgL0BSLfR14I9c1/1W8rwR4sygaRNwpFNlBKk+EkKcPZ5++ilmZqb55je/BkCjUQfg5ptf\nw2//9m/wutf9JK9//U92tAyd7H30dmCj67p/qbUeBjYA7TcW/SjwMdd1v9G27FvE2cUntdYvAkZc\n1+3I5U5Se0QkN1UQQrT5ucvecMKr+k7KZBz++3//XV74wmvnLX//+/+A/fv3ce+99/De9/4an/rU\nv3SsDJ1saL4LeJXW+kHgTuA9wNu01m/SWncB7wDepbW+P/n3q67rPgRs1Vo/RNzz6Dc7VTiV1EzJ\n3deEEGdac+rsq69+IQ88cD8Ae/fu4Qtf+BzlcpnPfvbTXHTRxfzSL72b3t5+qtXKgtNtr4ZOVh+V\ngDee4Cm5Rbb7/c6UaGHR6XwxIYRYQHPq7I0bN3H06Ci/8RvvIooifvu3309PTw8zM9O8+93voFDo\n4oUvvJa+vn6uu+5FfOAD/5NPfvIfWbNm+OQvskSpnTr7w4/voRyE/O41FzOQz6x2sc5aaWyAlGNO\njzQet0ydvUqabQqh5ApCCNGS3qCQ/JR2ZiGEmJPaoNAkQUEIIeakNiiopP4oOsfbVIQQYjWlNygk\nP0MJCkII0ZLaoFALaoDMkiqEEO1SGxS8MJ6CSfoeCSHEnNQGheasR9KmIIQQc1IcFGISFIQQYk5q\ng0JrQrwzWwwhhDirpDYoNMksqUIIMSfFQcG0/S+EEAJSHBTmprmQsCCEEE2pDQpNUnskhBBzUhsU\n5kY0S1OzEEI0pTYoyDgFIYQ4XmqDQjNTCCQoCCFES2qDQpNkCkIIMSe1QaF15zUJCkII0ZLaoNAU\nSUOzEEK0pDYoqKShOZQ+qUII0ZLaoNAUyphmIYRoSW1QkBHNQghxPAkKEhSEEKIltUGhGRUkKAgh\nxJzUBgXJFIQQ4njpDQqtTEG6pAohRJPTqR1rrW8GbgO2J4uedl33vW3r88AngRe4rnvDUrZZXUmX\nVEkUhBCipWNBIbHFdd23LLLuL4AngBcsY5tV00yRjHRJFUKIljNZffSHwJfP3MvH9UfSpiCEEHM6\nnSlcrbW+CxgEbnFd957mCtd1S1rrtcvZZiEDA104jr3sghkVApDNZRga6l329ueytB0vyDGnSRqP\nezWPuZNBYSdwC/BFYDNwn9b6Mtd1vdXcZnq6uqLCFRslLHuQar3B+HhpRfs4Fw0N9abqeEGOOU3S\neNwrPebFAknHgoLruoeBW5Nfd2utR4HzgL2ruc2KJb2OZOojIYSY07E2Ba3127XW708eDwMbgMOr\nvc2KyeA1IYQ4Tierj+4C/l1r/TNAFngP8Dat9azrul/WWt8GXABorfX9wKcW2uYk1U0rlwQD6X0k\nhBBzOll9VALeeIL1b11k1aLbrKa5wWun49WEEOLckN4RzUmGYKT6SAghWlIbFJpkRLMQQsxJcVCI\no4E0NAshxJzUBoVsMQdI9ZEQQrRLbVAoTOYBuR2nEEK0S21QMMlABckUhBBiTmqDQpPcTUEIIeak\nNygkCUIkAxWEEKIlvUEBGdEshBDHSm9QkBHNQghxnPQGhYTco1kIIeakPihIoiCEEHNSHBSSNgWp\nPxJCiJYUB4WYxAQhhJiT2qBQz4zjB/ul+kgIIdqkNijM9G6jVn9Qqo+EEKJNaoMCymBMKCOahRCi\nTXqDglGAQaY+EkKIOakNCgoLiGRCPCGEaJPaoBAPaTYyUEEIIdqkPihIoiCEEHNSGxSs0AKM3I5T\nCCHapDYoqKTbkbQpCCHEnNQGhdahS1AQQoiW1AYF1bwdp4xUEEKIltQGhYgQkOojIYRol9qgYFQS\nFKRPqhBCtDid2rHW+mbgNmB7suhp13Xf27Y+D3wSeIHruje0Lf8Y8DLiEQTvc1330U6Ur1l9hNxk\nRwghWjoWFBJbXNd9yyLr/gJ4AnhBc4HW+lXA5a7r3qi1vgr4DHBjJwsomYIQQsw5k9VHfwh8+Zhl\nrwHuAHBd91lgQGvd15FXN6r5oCO7F0KIc1GnM4WrtdZ3AYPALa7r3tNc4bpuSWu99pjnDwNb234f\nT5YVF3uBgYEuHMdedsFU2+Ohod5lb38uS9vxghxzmqTxuFfzmDsZFHYCtwBfBDYD92mtL3Nd11vG\nPtTJnjA9XV1h8ZIuqSZifLy0wn2ce4aGelN1vCDHnCZpPO6VHvNigWTZQUFrnQPWu6578ETPc133\nMHBr8uturfUocB6w9wSbjRBnBk2bgCPLLeNySJdUIYSYs6Q2Ba31H2it36u17gIeB/5Da/2hk2zz\ndq31+5PHw8AG4PBJXupbwFuSbV4EjLiu25mwb5qD1yQoCCFE01Ibmt8I/B3wVuBu13VfCtx0km3u\nAl6ltX4QuBN4D/A2rfWbALTWtwFfiB/q+7XWb3Nd9yFgq9b6IeBvgd9c9hEt0Vy9VNiplxBCiHPO\nUquPfNd1jdb6p4C/SZadsHU3ucJ/4wnWv3WR5b+/xDKdGkkQhBDiOEsNCjNa668C57uu+32t9Rvg\n3J40SAavCSHE8ZYaFN4GvA74XvJ7HXhnR0p0miglbQpCCHGspbYpDAHjruuOa63fDfwC0N25Yp0G\n5rgHQgiReksNCp8FPK319cC7gC8RNwSfs1rVRxIUhBCiZalBwSQT070J+DvXdb/GEgaWnc3mQoK0\nKQghRNNS2xR6tNYvIR5D8KpkANtA54p1OjTjoWQKQgjRtNRM4aPAp4FPuq47DnwQ+PdOFep0mMsU\nJCgIIUTTkjIF13VvBW7VWg9qrQeAP3Rd95w+m0qbghBCHG+p01zcpLXeDTxHPNHds1rrG06y2VnN\nCZttCdKmIIQQTUutPvoI8DOu6653XXcdcZfUv+pcsTpveHQ6fiAT4gkhRMtSg0Louu625i+u6z4O\nBJ0p0unhBHGGIG0KQggxZ6m9jyKt9ZuB5k1yfpJzfCY51epQK0FBCCGalpop/DrwbmAf8f0Q3gn8\nWofKdFpYURIMlAQFIYRoOmGmkEx73TxrKmB78rgP+D/AKztWsg5T0tAshBDHOVn10QdOSynOABmn\nIIQQxzthUHBdd8vpKsjpppIEQUlQEEKIlqW2KTzvzE3cJNVHQgjRlNqgsGZzfOhSfSSEEHNSGxQK\n/TIhnhBCHCu1QUFusiOEEMdLbVBQraYECQpCCNGU2qCwbza+HYTcZEcIIeakNig8O7UheSSZghBC\nNKU2KChpUxBCiOOkNihYSTCQLqlCCDEnvUGhNXpN2hSEEKIptUHBbmUIkikIIUTTUu+nsGxa65uB\n25ibWfVp13Xf27b+tcCHie/L8DXXdT90sm1WU2vompFMQQghmjoWFBJbXNd9yyLr/hb4CeAwsEVr\n/aUlbLNqWm0KcjtOIYRoOSPVR1rrzcCU67oHXdeNgK8BrzmdZZg7cMkUhBCiqdOZwtVa67uAQeAW\n13Wbt/McBsbbnjcGXAo8fYJtFjQw0IXj2MsumNXWJXVoqHfZ25/L0na8IMecJmk87tU85k4GhZ3A\nLcAXgc3AfVrry1zX9RZ4rlrBNgBMT1dXVDhLNauPIsbHSyvax7loaKg3VccLcsxpksbjXukxLxZI\nOhYUXNc9DNya/Lpbaz0KnEd8j+cR4myh6Txg5CTbrCpbxikIIcRxOtamoLV+u9b6/cnjYWADcaMy\nruvuA/q01hdrrR3gDcC3TrTNamuNaJaGZiGEaOlkQ/NdwKu01g8CdwLvAd6mtX5Tsv49wOeBB4Fb\nXdfdsdA2J6o6OhXS0CyEEMfrZPVRCXjjCdY/ANy4nG1WU7NpWrqkCiHEnNSOaLZkRLMQQhwnxUEh\nJvdTEEKIOakPCmCkCkkIIRKpDQq2UknHI0MkQUEIIYBUBwXAKDCRVCAJIUQitUHBAjAWBkMYSqYg\nhBCQ4qBg281OqRGBCc9oWYQQ4myR2qBgWRYYCzCEck8FIYQAUhwUbNtKhigYgkiqj4QQAlIcFDKO\nk2QKkWQKQgiRSG9QyDgYowBDIF1ShRACSHNQyOeTR4YwkkxBCCEgxUEhn8/PNTTLSAUhhABSHBS6\nu7rAgFERQShBQQghIMVBobenq5UpNHz/TBdHCCHOCqkNCt09PcS3hjZ4oQxea1cuNajXJFAKkUap\nDQq9vT3x3EfK4DWCM12cs8odn3ucb355+5kuhhDiDOjYndfOdj09hTgoYGhIpjBPteKh1JkuhRDi\nTEhtppDLJl1SVUTDl6DQZIwhDCJ8T94TIdIotUFBWXYrU6hIQ3NLlEz5IUFBiHRKb1BQClAoBcUF\ngkJoDFsnilSDdJ0co6R7bhBERDKoT4jUSW1QAFAmrjiveMcHhX2lGl/ae5St48UV7z+IDP++6wg7\nZysr3sfp1n5vCckWhEifVAeFuEsqVBdoU6glGUL1FBqhdxWrbJsu89kdIyvex+kWtg3kk6AgRPqk\nOig0O9h40fEnPz+pW/dO4a5sjnXudeEJAwkKQqRZyoNCfNL2F7ifgpfUp/unUK++1De3Uff5wf17\nzorxEu3VR54EBSFSJ91BITn/LZQMtDKFUwgKS52S+3vf3sXjPzjAlm/sWPFrrZZIqo+ESLVUB4Xm\nwS9047VmMPBO4a5sC2UgC6nX4gxhenJ1GqQPluvsLlZXtO38NoUzn7kIIU6vjo1o1lrfDNwGNOdL\neNp13fe2rX8t8GEgBL7muu6HkuUfA15GfLPM97mu+2inytjMFKIF6v6bbQneKcyg2l715IURWXvh\nGJzLx38Gr746J+E7949R9AL+8PrNy95Wqo+ESLdOT3OxxXXdtyyy7m+BnwAOA1u01l8ChoDLXde9\nUWt9FfAZ4MZOFa4ZCswC7cF+q01h5ZlC+72fK0G4aFDI5mwAGo3VOQnXwpBqEGKMScZjLJ00NAuR\nbmek+khrvRmYcl33oOu6EfA14DXJvzsAXNd9FhjQWvd1qhzN0+VCuUCz2qgRRTw08ijfOfDAsvfv\nHxMUFuNk4qCwWg3NQWSIWFlAky6pQqRbp4PC1Vrru7TW39Vav65t+TAw3vb7GLBxgeXjybKOsKP4\n8CN1/OC1VqYQRtxz4D6+vu/by95/e6ZwopHR7Vfnq6EZDOrLqPqqBXXuPfggdb/eWuZJm4IQqdPJ\n6qOdwC3AF4HNwH1a68tc1/UWeO5idRwnrfsYGOjCcewVFdAJsgCEVBhY241jzcVItX8MgADwIo96\n2GDdup5lVcdkZ8qtx1Yhw9BQ78LlaCv/cl9jIc1eT139BYZ68setX6gc9+/dxpd23s2bhn9mrly2\nvWiZl2KiOsXOyb3ceMGLV7yP1XIqx3GuSuMxQzqPezWPuWNBwXXdw8Ctya+7tdajwHnAXmCE+RnA\necky75jlm4AjJ3qd6emV9bIBcPw4KERRiQOjs/Rm5t6Ocj2OXfUgpOrVMMZw+OgUOTu75P3PlOeu\nukenKoxnF9623Pa8w4emyeUzyzqOdpExrQzlyHgJ55ib5QwN9TI+Xjpuu7HpGQCmykWgAEBxprbg\nc5fqVvcrPHD4+/S/bC1DXWtXvJ9TtdgxP5+l8Zghnce90mNeLJB0rPpIa/12rfX7k8fDwAbiRmVc\n190H9GmtL9ZaO8AbgG8l/96SbPMiYMR13Y79hTN+fPKNohK1Y6pw5nofhXhRfGKtB3WWY15D8wmm\n5w78udeulhdKpFb2mvVlTNHhh8kxho25Zac4pXjJizOlsn/uzP0kRNp1sk3hLuBVWusHgTuB9wBv\n01q/KVn/HuDzwIPAra7r7nBd9yFgq9b6IeLeSb/ZwfLRa+JqmzAqUfTmn4znupPO1asvNygstaG5\nvU2hWjm1oODPCwpLb1PwIm/eT5jrkmpMPLHfD8dnl1WWWvJ+Lfd9Eyd3oHiIndO7z3QxxPNQJ6uP\nSsAbT7D+ARbobuq67u93qkzHWpPJYPwMWCXGqjUu6+9prWv2PjJm7iTZfhW9FEHbOIUTBYWgLShU\nTjFTaB8b0VhOUEgyBS/yWg05zcFrtTBi23SZehhyw1D/kvdZC+vzfqbVTGOWfbMHuG79Nau2z8+7\ntzNRm+QvXnnLqu1TCEj5iObz16/FNApgVTg8W+K7o9M8MVlkpDzK0eK3MSbAmLk6+XpwfFAIooAj\nlaML7t9favVRW8ConWKm0D61xvIyhbmg0OQn4yaawWU5+4O59yvtmcIXnvsyn972fzlSGVu1fVb8\nKtWgtuBnUqyu0Bi+tPcou1Y4S8C5JtVB4YaXXEfU6AIVsbc4xdcPTnDvyBQ/GP0hVW8HQXgYQ1tQ\nWOCK97sjD/OnD3+Ug6Xjp8f2kxN0zrIo+ot37wz91cwUVhgUwsWrj1YeFGrJz3QHhZHKKAD7iwdX\nbZ+NJGsteiu/34dYmsm6z9aJIo+dwr1VziWpDgqXXXEpyou7bE6GFQxQ8kJKjbiB1EQ1OEmmMFad\nAOD2u7/Hwb1T89Y1G33X5TMU/YBokQnygnltCqd25dfe0Ly86qM4GPhtx9scvFb0asveH8y1KdRS\nHhT8JAtbzRN48+9V9MoneaY4Vc2pbmqncG+Vc0mqg4JlWeSjuFklNHEnp0YUMdOIH0emctI2hUrS\ns6bolfjKrU/NW+dHEQoYzGWIDJQWqUIKgoju3ri7aqV4akGhvU1hRdVHyfEqFbcpjFaO8vdP/suy\n9xdGbb22ltkW83wTRHGWWFqlE3hkotZ7W/TS1f3yTGgk36ljeyg+X6U6KAD0+DkAAn+uvnfGawaF\n6rzqo8YCmULZi4OCn42vhk1bNhBEhoyl6M/GgWd2gdt+AoRBSDbrUOjOrGr10YoyBeKf+UKGMDSM\nlMdAZVr7Dpc682tbIEh7phAkN3FqflZOVbNTAMBsIx1VGmdSc8bk2ilMjnkuSX1Q6PcsTGQRhHNB\noew1q48q8xqaF+pF0+yD72fik+D0xFxjlB8ZLKAnmQhvdpFpI4IgwnYsenpzlEuNeYFluVbeJTU+\nziDpgtvVk2QulRqqrZNaY4n3l2gPBGkPCqGJg0IlWJ2GyvZ2H8kUOq85Zmk5437OZakPCr2WR1Tp\nw5hpjPExxlAL4hO9MdUT9j4KgrB19RckmcLh/dOt9Y0gxK/6TOyOl80sEBSMMYRBhJOx6O7JEQYR\njUWm0H74yFb+/NG/oREunk0EZmVdUpuD15qZQk9vnEFVyh5KzY2wri8xhW4PBGlvaI6Sv0nVr63K\n/rxQgsLp1PweSfVRSqzL20TlNfHNFap7MaaOoTlGoQrt1UfH1I0/8K0dzNbjL6WXibd77unR1mA0\nPzKoCIKZ+KS4UKYQRQZj4vmPuvuSE3Fp4Tr4Z6ZcDpQOc/QEXRv9FY5obp5omplCdxIUatUG0BYU\nlrjP9kCQ9nEKzc/TQr3XVqL9oqDYkKDQac3qo8CYU7o977ki9UHhx26+OQ4KgJocwZi5q7nI1DBm\n8SvesbFZjJV8YHI1ci8sM3G0zEP37oqXRREqNETF+Es86/nUgjqf2fZvHC7HUzo1p7iwnThTACgv\nEhSaV5onujpcaZtCI6mSiFSIIaKnL+6VVa8G8zOFJe6z/QSY5r707VWBC7VJrURDMoXTqv17lIZs\nIfVBYdOFF7PGz2GMop4fnRcUAMJwsvW4dkymUKy39SZRkNvcoG9NnmeeOEIYRASAigxesYGjFDON\ngKfGt7N17Ek+/MjH4v0nA9ecpE0BFs8UmnXSpRPMJRSssPeR39Z4GdlhqyyN2vygsNRAM7/6aHWq\nTc5F7e9Ds93mVLVnrEWvROCH3Plvj7Pr2dUbHCfmtN+SNw3dUlMfFACGrTrh1AYahRJeEM8no5Ku\nqlE0N99Po+0LHkWG6dz8uv2pxhQXXDJIFBkmxkpExEGhVvbozdhM1/1W90SAo9Xx1hgFx7FaVTaL\nBYVakimUlpApKOIPc7iERmtjzLwTVmgH9CRVWV49mtfQvNRAM6+hOcVdUtuv5P1VCgrtbQolr8zk\nRJmRg7PsccdPsFVnRCZ63meC7bfkXWqb2rlMggKwwSoTjF4MgO/vAMAxAwCYpOHVChyqbTegqVc9\nZi5MTpZRPFvQdH2W9Rvj6WiPHIlPBio0RJEhmK5TjSKmqnM9UO4/+L1WULAzdisoLFp9FDSDwuL9\n3ZtBoTu5R4MXRjx8ZCuTtelFtwlN2GoMBTB2QFdSlRXWTatLKiyj+qj9Cjn0CKPn/xXWQtrr/INV\neg/aq48MhulKfOFyqt2Zl2vP7H7+10N/xh9870OrNgbjbOS1Zd+SKaTE1RdtxFTW4NR6IGkUtOzB\nuSdECmVsqn69VX0yW2rg5+PYBTrOAAAgAElEQVSrfiuZbbXkl1i/Mb576NHRJCgkJ+lgNv7SjM/M\nZQrfG3mY8XI8CjquPkq6gS7w5TbGLCso9Ca3+Nw7O8K/Pnsr39p/76LbtPd7ByATUeiKA0FYV6dU\nfdSbiScZTOsAtvZpw9sD76loBoXebPzejlfiz1C1fHrf4y/uuIPpxgxe6DG6ivM6nW2kTSGFXvQT\nb+Dirimqhy9tLbPz57ceW2Qx2W5qYZ1bHtvNQ6P7OTBbxZB8CZM7pdWCOj0DWZyMxdh4fOJuBoVK\nfi8A07X4uW++7A2EJmTL/oeAOChksg7ZnENp9vheKo2w0TqptAeF7ZPP8Y1932k1aDa7pJ6f3HHt\n2em4QXu6sfi01+393gFMJiSXd7AshWlYKJafKTR7HA3k41lV09ottdIWFAzmlMagNDWrjy7qjT+j\nR2vxCbla9lZl/0s1U5/7TD2f52Ca36YgQSEVlGVxHfsIpzagkhHOtj2MMvFj7CyWyRIRYEzE1/c/\nx97yLJFJxjOouQ9KyS+zfriX6eTE3gwKDSueFK2UtFW8/LwbOa9nI3umDwBxUAAYWNtFcbpGeMyH\nr9LWx729nvore77F3Xu+yXQjvnNaM1O4djCuxto1G58wTjTy9bhMwYlQSpHvyoBnn1L10UAu7tmV\n1gFsxw5Yq61Co3uzofnCvgsAGG/E828FQYTX6Mx9tR+6dzef+4cftD6XkYnmHdvs87gXlDcvU5Dq\no9S4+oqrWNdVo7HrWgYmrseyurAzmwCITB0rat5HOaAR9fC94r/jefFcR0bNfVD2zu7jgs2DGCvO\nCFRoiKwQzx7FGINv58nZWbJ2hsvWXAJh/Dw7aQMYWNdFFBlmp+efPKptJ5OSH2cKYRS2ZuA8UDwE\nzAWFDYUsw4UsY7W4amH2BFdyXnh8pgBQ6Mpg+c4xmcLyxikM5Nck26Wz+qhyzIC1ZvA+Fc0g3swU\npoK5HnKdalcYPTRLabZOOZmbqxrUiExET6YbeH6Pl/BW2KPvXCVBIbH5Na/nRzO7CUtrKYz1kDtU\nxrE3JWt9TCZpBK7eRaV2FxHHnuTik/sT49u55sXnk0umiVC2wstVgIDIFImyvXQn9ewX9V6AimwM\nMG4bImMYXBd/yaYn5nc7rbVdlZW9CpGJGKtNtHoz7S/FQaHZJTVjWVw10EMQlVrbLNbYe2xXyVq+\nyMHSCIWuLFbooFSGKIpffzltCgpFf64v+f3MdUvdMbGH9933B+ya2Xvcuk5Xtxx73NP15d29biHN\nNoWB/Br6sr3MmLlOBJ1qV6gk+y0X42DfHMl/Xs9G4MQXHee69qldJFNIESuT4TIrQ3fGY3e5m/3u\nNI3tc10xHec8wCaKZlBkgYgMm8k6L4jXJ72VHh9/io8//UlmhuMvSWRDIx9/gaJwEmVl6c7EN7G/\nqO98rMiiMpznPtPgkbFpBtZ1AfPnUIL5UyQYDBW/yqG2ezgcmylkLMUlvQWiJCgYTCvDOFYzU1Bh\n/HE4smYXn9n+OXKFOHuxVDYZv2GW1SU17+QoOPnjyn+6PTO+k8CEbJt4dt7yibrHn2zdzZOTnbvK\nbVabWSqZ/2oVJrBrVh/l7BwbuzdQtcqEVnxxUCmtfqZgjGndO7zZM67ZgL6pZxh4nmcKoWl13JA2\nhZS59p2/wk8VnuSFw+MMZKtUpwr0HLmGS/a8lC7/Uvq7f4nu4EV0d/8/QAafA80EASvbzCpsds/u\nYyT3GACq3sArxF+gMIzrfi0rfu76riGyZKmuja8+tuw7MpcpTM7PFJrVRzk7zkBKXplD5TgoKBQH\nSocwxhBEBluBpRTD+WwrKMDCJ6TPu7dz9+5vAtBTXAeAsQxj1QnCQiPpi+VgjI8iXFKmYIxhsj7F\nYH6A4a71ABwsHz7pdp0ykfTOOViaX4b95TqBMTw11bkTWrMaLWvFf7fVGIHcDOI5O8tw94Z4WSEO\n+Kd6j++F1Gs+UXKx0aw+ak4aOZhbQ8HJP68zBS+K6Ms4KCQopI6Vz3PDDS/n5y7exntfuZUuq8HU\noY28Tu1mYNs2hh8e46LH17PmgM26qRuAAM/fDoBjx2m0pXoAi5Ai1dq9+Gac2cEj5Mt9FGpTNLzn\nKAbDGGOwlMVgdpAwH19Nz6oMquCQydpMHZspJEFhQ3KSLXql1lQZVw1eQTWoMVGbwjcGx4r/rEp5\ntM/ddGxQMMaw9egT7Csljd1+dt76YtcExgKUImNBFNWX1E+75JdphB5DhbVc3HchjuWw4wzeZH68\nGgeFZuAEGK2Mce/+uzHGY1+ptugNkE5V86q+KxNngOO1U29TaE5JkrOzbEyCQrk3PsbFBj6eimpb\nO0Wz+qg5qr4n20Nftu95O91GZAx+ZMjZFjnborTITMfPJxIUjrH+R2/Csq9EKXjVFYcIjcU/z17L\ndNnC8upYRrFmd5HhXQOsP3QFXcVBhg9cR1dlmIxzBZGZxbbi6qGwfhi/t5uoa4hGr0VZTVNvPEg9\nHOPRsfiqdUN+mCgJCijFtukSA2u7mJms4rfdlKfix0GiGRQma1McKB1iMD/ANeuuAuDxsafwo4hM\n0kV23+yBZLdxe8ixPUSKXmler6Bso2ve+krfJH7S6Nyf7SI0VUpecNJsYbwaN3wOFdaRsTNc0nch\nI+XR1jGsBt8LeOz7+3nkgb3MTC2+31kv4OBsPNK3GtSYqsf171sOfY99s0/S8J6hFkYcrS1+hb2/\neJB/ePIzKxqg1az/78rE1Sy7Zks8NvYUz0y6y95Xa5+Bh0KRsTJcP3QNdugwsWk3oe13pKG5fZ+t\n6qOkTaEn001/tpeKX503Wv9sVvGrPDe1c0njRpo9j7K2xcW9BSYbPkdriwfe8gluu3uukKCwgAtf\n8XaC4hDXnTfGmlwNL7Q4xCBPBoaLXryWV/+0JnNJlQuDy7hu9GaGq8Osf2yMSx+9jFx4IWE0jqVy\nGDtiOvsNPPZgzCy2Ws/g6EUE9V18ae8RPvb43YTd/US5DE41gMjwtQMT9G0ewMta3PbUPr617wFm\nG8VWpnDdUNyGcfeeb1Lxq1y9VvOS4evJ2lkeOPx9/DAkk/R8+v6RRwGwrfhq8thM4dgBR9l6N9la\nd3P8HlsnnmDXNd8mDKfI1woEwUEiFM/NVObt49jR0uO1uJpsTW6QkUqdKwYuxWDYObPnhO/7aGWM\nj279BKOVoyf9Gz331CgPb9nL1of28/CW4xuQm76yf4yJ6txtUg8kVUj7inHA9PxnMSZib2nxNo/v\njTzCtsnn+MGRH560XMdqNuL3568AYLo+yr888wU+u/3f8Vd4EvUij4ydQSlFd6aLoZFLCR2fiU27\nWw3Cq6m98bpVfZS0T/Vku+nLxd2fz4Vs4Ynxbfzegx/k4098mh8c2XrS5zeSarOspbhubXycTyzS\nBrVjtsKHn9jLtg5WR54OEhQWcemrf4O13S/llye/xv988YO85UeeQwG3bz3KrQ9+n439dd76s8O8\n+e2beMc7X8qPbz5MEE6w6bEryXvnEZkSysrjMER/8RLA0LD2MDW8n0yxTOSPMTma5+n6KAYPa/Yo\n3XsO4Rv4XiFg/Jp+ttaf4s7dX+FP7v9Lvnv4BwBc3H8hl/Rd2Go0vnH4BgpOgZcOv5jpxgzF+k4c\nS7FjagdPTTwDQMa5CIiDQvPKxxjDlsPxwLnmNB1WZNM3NQwKupxC/DwrpOE/TXm0Qd94fBJ7ciru\nQeMFHv/7kb/ig9//c+64+weteueJWpwp7K2s5e+fOchAfjPZWjefffoL/N9nv7zoFdrDo1vZM7uP\new9+96R/n5GDcRkcx+Lw/unWa7e7c/c32Dr6VcDHTsZaHCgdwgt9DiVVb8aUCcJD/HB8dtGeJQeS\nnl2PjT214PoTac531DAD2PYmIjNNEAVUgxrbJ59b9v4grpJqti15jZCB0Yvpsl/A1PAko+OTNOoL\nz7E0Vh3nbx775JKCbrt5mUJxfkNzb6aH/mzcw2z2HGhsfuDQQ63Hz03tOOnzm9+XnG1x1ZpucpbF\nE5OlBafQfja5WPrhxMnbV6LIsH/X5HHjkc4GEhROYPDq13L1B/+SMH8lVw2O8/YbnmGwu87+4hq+\n9MQa/uifdvMnn36MO77yOb62O+TpaJA9gc9LHqoyWL6OS/I/y3/pzfFLwzNsLF2EpQawrAGqfdNU\n/S1M5+5h1rmXYvlfGe/5GmMD9+HNPoQXgd+bJwgOkY024mfmqni+8qUnuab/GgBsleXzz9zOtpEd\ndDkFMlaGmeoWDkz+G5976jPJHb8UThIUnpgY4Xe/8yTbJkdwp3fx5Pg2rNCmUI2/1PsiRWYmziqq\nQY11/kbAwvd3Y1ke63echwmKuDMVxmoN/vyxrxOZiIiI7+TuYtszh7h3ZJKRSol87mXsLsdJx9Yd\nAVc8/So27r6WHxz5Pl/ccedx3WNrQcjTEzuBuBosiAK8RtCqw25njOHIoRm6e7JcdvV6GvWAiaOl\nY/ZX59sHttDw432GxsdSFk+Ob+dg6RCRibDtuEonZ3YwWvP47I7Dx00g6Id+q+3mQOkQo8XFJ53z\nvfC4K/UgjAPpZEORzeh56x4ZfWzRfZ2oq6wX+uSShutG3cfvL5Dp+jHyXS9n7+at3Pr4V1uv2+7B\nwz9gx8xuvrX//kX3fWwZHj6ylcmZ+CTX1ZPFawTsmipTTKrSejLtmcLZ3dgcRiF7iwcY7t5Ab7aH\nnTN7TtoluTlGIWtZZCyL69f1MusF3Lbn6HHtUPuSbHPXbJXKIvdjb3r2yRG+9h9P88gDi2e5Z4r9\nwQ9+8EyX4ZRUq94HV7ptd3eOavXEdbDKyjB4wbWYrsuZHB/hKrOTlw4dpJbNc7jcRyXMs3tmLeOs\noSfToGpy7MlswppQrKvt5+I1k+zc20t5coDBzAb6+6+m4ZxPZKrY9noc+3yUypAvd9EozBKoccLK\nDhpmF5GpEKopwAHiD+eBXZfgHjDYa+vks9fTtfdidu732VE7RFbVaKgaoWlQSz6wtoHI9wmZxY9m\nqDSeZftklqfH76cRVTGWIcjGJ7LG2IXUzCCZwSNElkfVLmNbmzBmlop9hFwtR77eT32wi0cPjTJZ\n+y6GBpbqI7Jq7CxOs9ffQDHow7bPb9ZCUVaGnkMVCpVunKCX0q6Ih/Zu5fHaD3Fsi+Gu9Xzq2d3s\nndkCgB8FnN+ziUfvPsIjW/Zy+dXryeXjK/1qucGBkTG27n6WS8/bxMWXDbHHHae3P8+mC9a0/m5P\njm/j8WOu7C1lUfLLFJwu9hb3k8+9hIJVZ7ZxiAKXYT9VxAzkuGhNN5GJR3UfKB3meyMPk1cFAgJ2\nHTjMjRdfj23Z8/ZtjOGrtz3FD+7fw6VXDpEvxOX9+r7vEJqQXO4GNnStYbq6DYjIWllGq0dZVxik\nJ9PLbXsn2DZdotuxKR8ucsfnHqe3P8/gum7CIKJe88lk7dY+e7M9vOK8G5mdrvHIdAmvP4tl9VK1\nn+GQv5NnxndS9euszQ+2sorbdtxJJagyVp3glef9GBk7w4lsn3yOf97+OTjYg13Nc95FaxhxDA9l\nQmarT2KpgP90yeuo+jV+ePQJvMjnxet/BJW0aTV5oYdS6rjlizHG4IU+o5WjrYCzVCf6Th8qjfDA\n4Yf4kaEX0JftZV/xIDcMX98agLeQibrHY5MlLuvv4tK+Ljb3FdhfrrNjtkotCLmivwulFNUg5OsH\nJ1DEF0J9GZsLewqt/RypNngsySDW5DJ899u7qJQajB0pcdlV61ufl5VYynlske1uWWi5Op1zpXTC\n+HhpxQcwNNTL+PjyUt4oMhw6cpj9D3+HQmkPO+zzGPHXsL6nwg3nHWHX1CA/2L+J0VLPvO0sE+IQ\n4eGwXhXp7gqorl9Ho2cAOwuX+vfTl4nYHhUp2Q0iy4BVQEUR+cLL8bxdNCareDuvB0BZiq4LeoiC\nCIWi66JeeqoBlYEGm9QMGxvfZZQGL/PW8+STV3FkQ4lq9xFKvYfju8wRT+URhlOQzARbf/omTK2X\n/hd2Q9cP8djfKr8d9cTjGIxPZAcokyFy/PgL0PM2yuXbiShhWYM49gZsexiLAgYPFShsPyJUVawo\nQ/dEAcur4VkThF02Qc7BqSsylSLVgRAaFZwgi0U3TqObTN4hGlLUanUytUGmex8lyJRxyLKhcD7V\nfTnyzgDdPVnya0pM1ScYN0fxqRN3n4qwzBrshk2QnQbbYDD0+C+C3i7K9e9iWQNk2Miaoxdg5RpM\n5R5jaM0AG7oG2Tr5OBccfiGTfYep9k7TrboZyGykGkLeDghDRa/pprLbplBbw9C6PJe+dA17q/v4\n/tGHscIs/fm3c2l/hScm7yIIGxTKa6n1TQPJ/TTsC8hkNqPI0DNdI2oYcuUGV1w+yIHdU1T9Kus2\n9pLZaHhs8lE25C/lSv/HeXL7UdS16wgzCpQhWx2hXNlKvRBnNA4O3U43NhZT3jS2sVGBzfCa9Wzq\nHqbWaDBemeSiwoVsHFhHVz5PIZen7FX42s57qdd8hkY2M7tuFK+3SNZ+EarrQur176PwuLx3E7Ww\nyljtEJWgQk/Yj+1nsQsRV6zbTKXcYOfMHrJBgXX5tVy8fiM5VWB/ZRzPKpPJhPTn++jP9uFHPsVq\nmWeO7sYPfVRos26gj4v7LsBYERP1KSrRII51GWsKHkPZCo7ysJVF1s5Q8itYSjE6UsRSFt3dOWp+\nSH93lgjDnpl9HPIOcWnvZmxbsWNmN5ucTfRl+3CCHI1yiAptgkZEX3+BvvMyHK5McbDicF5kM1Dr\nIsqFeP1l9tfiyTEHnBxDhTzlRoMj1TJre7qZrFcwRPTZXRToIUM/s94QQV5hOVkuyOxl/+49WLah\nYdXI2A7D6wcwVkjWKtCVW4fvF5nxJnFsh8H8GmYbRRzlkC31oGwwfT692W7efNkb2DQ8uOzzWHL+\nWzBKS1BYwZvZZIxhZLLKtl0jeHseoXdsLwNDDQr9hlquwM7qenZNrMEYRdnL4AU2WTvkSOnkVz8Z\nOyRnBWRVSOhkqJHDq0b0XNpHjzXL0Z0Gc0ztn8pY2Hmb3qyHVamxrqdGNhOyfXQdhUzAupyHypYp\nrj2Kb9uY8csxPT7RwHaiqI459Gq8yeT+Dt0OzkAZtWYElZvBWBPQmuMpPtECZNXl9M3cQLmvjG89\nShSOte5Gd6ZZ9OFwJR6PxAuMQuFglE8u2ES/dzMmEzJlf53QWqTqw8Qn27WNN4LTRy36IVVnZ9t7\ncfyrKmMlU5/MvU/goJSDMTWy4TBr66/Bc2apZFx8Z5YwOoVZRg2t8TKd1bwOPgUGwDr+/VvWMTTL\noeKBpCbEqKS6zFioZEfHl3QpZTeti6YTP00t7XkrZmOZHJgIo0IM4YKfuZu6/xPve8MbJSi0O5NB\nYSEz5QZjU1XymYB1+SKN0mGmJ/dQn5lE+XWcfIiHQynIMusVmK3nKDUylI9COB0QDBSomwwNk6Gm\nstR9h3pgA4q1XTV+5can6HJ8qp7DWLFAxvOYrHTx5MQw414vFc8hjOYHi4FslVqUoR6cOEV97VWH\n2WiN8dDoxRyY6cMP26pIrBCsIB6iHTngeFhdRaj2EgW5ueepCNVVxOqZJpPxsLExVjzFk+XbYDcw\nWR9FhsjrRjlVVMYDxyTfMxtjN5KbHMUnVRU6ENhghxilUGEBY3KQnQKnijIKwgxGKXDCeNbaSOEd\n3Eg4vR67t0F2wzh0j4JdIyqvo7FTg7GwsnEgtQsK1X8E8kdQqgqqEB+zamC8Ao2d1xE1IpwuB7s3\nxLJmIQTVFULXEbAboEJQIcY2KGODiZJjUKCCZH1ENHk14fQmrIwV/7Mh5x3GZhbjGMKCwjgNgq46\nWF0Q9kOYAaue/B0aGFUF1WxQNvHrKAuLbpQ/iKlbqC4fY1WSk4mFUhkUBUIzhlIZLNWFMRYYk9w3\nxJt/4jGgrAIQYbOWTGMdvtpHaM9i1xRYFmHGEFkRyoriY2euLUO1LloUzRO5wQcTolQOpQqgLIzx\nMKaGIoMyGXLhMMayadgHwRhU2AwAQOhhggoWOcIMRE4AWFiqEJ84Tcj8k/9C570TRR+FZXVjjI8h\nQKlsfGImjG82pRTGePExWEkXbpO0H6j4b25ZvYDCmAYQJcvDuWXKwrbWYIwft/eFOYwpE6kyyi9j\neR6N7AQRHkrZgJ28to3CQqkuLKsbyHBT16X8+k+86twJClrrArAN+JDruv+nbfnPAB8AGsAXXNf9\nO631zcBtwPbkaU+7rvvek73G2RYUlsoYQ9CYIvRLWHaOMKhTqvrMTo5TKxapFCt4tSJZ6vRZFaqW\nQqmQrN/AjgKwFHafjZ0D2wbLMpgIlAPT23xmt9bxlEPFKXBBdZRsN8xsWMtsVz+B45DLRQx1VSir\nArNhgZ6egCvOL2IaEcFoA9OIqPZ0M+n0M1HrYrJaQGGwLUM9cKj7NvXAoeY7dGd9BrvqVL0MfmgR\nRBZ+ZNHwbWqeQz10sBRYymApQxBZ1AMHS0XYlpkffFZRzg64sH+WfTNr8KO511AYNnRXcFRIyctS\n8nJEJ+lzUbB91mRrjNe7CUxnyvt8oJacSTSfd+x5aeHtM4QoBSEWjhURGYhMHGwUBgsDGCwrLkNk\nFJGJ19kmQimDUorQxH9noxROFGCURVwBG2ERYSwLy0QYA5Gy4osQAw5xsDHE+zWo+OJDxa+tjMEY\nRZTszxiDAiwFtoqwCQmMjUHFz1cRCqhGOXwTT6fjEFJQDUIsAmNhKUNGBTgqistigOS1I8BRIe/5\n2eu56ceuXtWg4Cy0cBV9AJhqX6C1toC/A14ETAJf11rfkaze4rruWzpcprOCUopMfi2Z/NrWskIf\nrB++YsX7NCYiChpceL1CvcOBZL6d5NNEEEb0dIdMHNmHbdt4ocLzfOr1OpkooOB4hLZDY2NEZEIK\nGNbU61xcrxLWKxgVEQYeYbVI1Aioh2uwHYXjVFE0CCNDGBjCAMIAqiWbegB9a+vYWUOkMti2wcGH\nMMBSBmVBiMI3NkoZMlEItkUQKaJIxeeMZFR1fDHc/AnYijBUeJGNMoasFRIqGy+08QKLjT0lerI+\nkVIUvTx+aOEbm/5Cg76817wNBpFSlLwsVT8bbxvF+2gE8cm/P+9xyZppnAxEBqpehtAoLGUoN7IE\nkUUYKcLIIjSKIAmMWSckihT1wKER2K3nXTw4ywVritQDh6qXoepnqHoZvCr4viLMZeKfysYPLXJO\nSHfWj8sfWfih3Qq+kVHxCcwojIkPKOcErO2uM13L4Yc2URSfyKLkBArgJYE4a4c4VkRoLILQIjJg\nkhOPMcl5DxP/VAZjwI9scnYYn4YNZJ2Ium9T8zMs0DN4FSjqvg0KHCsiiKzkAoPWsbf/CyNF1oqS\n4BH/HhoLY+ZyF4MhMk584rXiTCSMHMJIYSwbSxnspHooMopaGGfDjhVhK4NtJYHDxH9zAFsZHMtg\nqQir9V5Z+GGGWpQl74Sti6DmZ2FNocG67iIKw0wtT9nLkLEiuuyAILKo+TnKQfzdaF5UqeQCy3EM\nhx75NvzY1av6bncsKGitrwSuBr56zKp1wIzrxjeU1Vp/B3gtsK9TZUkLpSzsTGGBFfGPjAV9/b00\nvPg5Xcc/83lpuRmhSYIoxhCZEIVCWQ4mCjCEmCiupjBRQBCGGBNgmZBQOfhhhjAKKFg1lNVW7UFy\nDWwiIMJEIYYIopAoihtVbacXZTtgg7JtTBQShTVCv0IU1FAGLLuAiQKiyMOE8f09iEKiMMAoB5wM\nkcqQzeeZnpyipxBh2xCFIfWGj1f34y99FBGiMCqDbSt8P6LhRZgoigOHY4NjxxV4nocKQ2wLLCLA\nx/MNdhSRzxgsy8TVYioivpaPIDIEniEK46tro2wiZcdX0cpg2wZDUq2SDOIzYXwToiip1ndsK654\nit84Ql/RqDk0Gja27WNnQywrQqkISwVYJiLEwcNC2RGOFWI5cdZgojiAZKyIvB2XLzLxujAJrO1/\n/zgTiC8+lGVQVhxUotDCRKbVVKRMhFFzWQUGbKK5GrPmRQ1JFtWMpEkbisIknwviMkVxWS0nWa/A\ncgyEcSZCszxA4NtsvunXlvNVWJJOZgofBX4LeOcxy8eBXq315cSB4NXA/cnjq7XWdwGDwC2u695z\nshcZGOjCcVaezg8NLa/L2/OBHLMQzy+r+fnuSFDQWr8D+L7runu1nj9gx3Vdo7V+J/AZYBbYSxxP\ndwK3AF8ENgP3aa0vc133hB1wp6dXPp/OmWxTOFPkmNMhjccM6TzulR7zYoGkU5nCTwObtdZvAM4H\nGlrrQ67rfhvAdd0twCsAtNYfAfa5rnsYuDXZfrfWehQ4jzhoCCGEOA06EhRc1/3Pzcda6w8Sn/S/\n3bbs68TVShXgjcBHtdZvBza6rvuXWuthYANw5ibhF0KIFDptcx9prX9Ra/2m5NdPA98Cvgt8xHXd\nCeAu4FVa6weBO4H3nKzqSAghxOrqdJdUXNf94ALLbgduP2ZZiThrEEIIcYbILKlCCCFaJCgIIYRo\nkaAghBCi5ZyfEE8IIcTqkUxBCCFEiwQFIYQQLRIUhBBCtEhQEEII0SJBQQghRIsEBSGEEC0SFIQQ\nQrR0fO6j/7+9uw+RsoriOP6VIqrtRSPCXhGpflBGpJhFpWsFZkaSWgRiWZJEKb1YYBQRERRKaIn/\nRKKkBf4RhGVFZO2WQmEvFkKdqD+C1spSNMQwlfrj3h3G2WfZypl99JnfBxZ2ro/DOXvdPT53Z845\nUklaAlxBmnn0QERsLjmkpiuaew0sAlYDxwA/A7MiYl8pATaZpFGkZopL8tzvcynINXfkfZA0P+ul\niFhRWtCHqSDnVcAY0qhbgMURsb5iOS8itd4/FngW2Ez197kx55tp0T635Z2CpAnABRFxJTAHeLHk\nkFqpOyI688d84GlgeURcA3wP3F1ueM0hqQNYBmyoW+6Ta77uSdII2E7gIUmnDXK4TdFPzgCP1e35\n+orlPBEYlb93bwCWUv8XYNwAAAPRSURBVP19LsoZWrTPbVkUgOuANwAi4htgmKRTyg1p0HSS2pQD\nvEn6B1QF+4AbgW11a530zXUcsDkidkfEn8Am4KpBjLOZinIuUqWcPwJuzZ/vAjqo/j4X5Vw0g7gp\nObfr8dFw4PO6x7/ltT/KCaelDpl7DXTUHRdtB84sLbImiogDwIGG8a9FuQ4n7TcN60edfnIGmCfp\nYVJu86hWzgdJw7kg3eW/DUyq+D4X5XyQFu1zu94pNBpSdgAt0jv3eipp0t0KDv2PQFXzLtJfrlX7\nGqwGFkbEtcAW4KmCa476nCVNJf2AnNfwR5Xd54acW7bP7VoUtpGqaq+zSL+gqpSI6ImItRHxd0T8\nAPxCOio7IV9yNgMfPRzN9hTk2rj3lfoaRMSGiNiSH64DLqFiOUuaBDwOTI6I3bTBPjfm3Mp9btei\n8B4wA0DSaGBbnvxWKZJmSnokf94793olMD1fMh14t6TwBsP79M31U2CspKGSTiKduX5cUnxNJ+l1\nSSPzw05gKxXKWdKpwGLgpojYmZcrvc9FObdyn9u2dbak54DxpJdu3R8RX5UcUtNJOhl4DRgKHEc6\nSvoSeAU4HvgRuCsi9pcWZJNIGgM8D4wA9gM9wExgFQ25SpoBPEp6OfKyiHi1jJgPVz85LwMWAnuB\nPaSct1co57mko5Lv6pbvBF6muvtclPNK0jFS0/e5bYuCmZn11a7HR2ZmVsBFwczMalwUzMysxkXB\nzMxqXBTMzKzGRcGsRJJmS1pTdhxmvVwUzMysxu9TMPsXJM0HbiP1jvqWNJfiLeAd4NJ82e0R0SNp\nCqmF8d78MTevjyO1Pf4L2AncQXoH7jRSM8aLSG++mhYR/sa0UvhOwWwAki4HbgHG5572u0jtmUcC\nK3Mf/y5ggaQTSe+unR4RE0lF45n8VGuAeyJiAtANTMnrFwNzSUNTRgGjByMvsyLt2jrb7L/oBM4H\nPsxtqjtIzcZ2RERvC/ZNpIlXFwK/RsRPeb0LuFfS6cDQiNgKEBFLIf1OgdQDf29+3ENqS2JWChcF\ns4HtA9ZFRK1Ns6QRwBd11wwh9ZtpPPapX+/vzvxAwd8xK4WPj8wGtgmYnDtPIuk+0vCSYZIuy9dc\nDXxNalp2hqTz8vr1wCcRsQP4XdLY/BwL8vOYHVFcFMwGEBGfAcuBLkkbScdJu0ldSWdL+oDUpnhJ\nHoM4B1grqYs0+vWJ/FSzgBckdZM69PqlqHbE8auPzP6HfHy0MSLOKTsWs2bynYKZmdX4TsHMzGp8\np2BmZjUuCmZmVuOiYGZmNS4KZmZW46JgZmY1/wBRp32zv5dujwAAAABJRU5ErkJggg==\n","text/plain":["<matplotlib.figure.Figure at 0x7f94223dcb00>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"t4uDUpjMV8lm","colab_type":"text"},"cell_type":"markdown","source":["# **RCAE-MNIST 2_Vs_all**"]},{"metadata":{"id":"sRLgcDCmWBM7","colab_type":"code","outputId":"786fe3d3-b9c4-4a17-f397-45989d1624e8","executionInfo":{"status":"ok","timestamp":1541285027602,"user_tz":-660,"elapsed":4540201,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":99917}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/250\n","5450/5450 [==============================] - 6s 1ms/step - loss: 5.0781 - val_loss: 5.1949\n","Epoch 2/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 5.0037 - val_loss: 5.0221\n","Epoch 3/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9881 - val_loss: 4.9938\n","Epoch 4/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9806 - val_loss: 4.9845\n","Epoch 5/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9758 - val_loss: 4.9784\n","Epoch 6/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9724 - val_loss: 4.9775\n","Epoch 7/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9703 - val_loss: 4.9745\n","Epoch 8/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9688 - val_loss: 4.9746\n","Epoch 9/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9672 - val_loss: 4.9741\n","Epoch 10/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9663 - val_loss: 4.9710\n","Epoch 11/250\n","5450/5450 [==============================] - 2s 329us/step - loss: 4.9651 - val_loss: 4.9710\n","Epoch 12/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9642 - val_loss: 4.9718\n","Epoch 13/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9634 - val_loss: 4.9735\n","Epoch 14/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9626 - val_loss: 4.9673\n","Epoch 15/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9617 - val_loss: 4.9680\n","Epoch 16/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9609 - val_loss: 4.9672\n","Epoch 17/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9605 - val_loss: 4.9662\n","Epoch 18/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9601 - val_loss: 4.9647\n","Epoch 19/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9594 - val_loss: 4.9656\n","Epoch 20/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9591 - val_loss: 4.9659\n","Epoch 21/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9586 - val_loss: 4.9653\n","Epoch 22/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9581 - val_loss: 4.9642\n","Epoch 23/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9582 - val_loss: 4.9629\n","Epoch 24/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9578 - val_loss: 4.9637\n","Epoch 25/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9574 - val_loss: 4.9622\n","Epoch 26/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9571 - val_loss: 4.9624\n","Epoch 27/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9571 - val_loss: 4.9637\n","Epoch 28/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9568 - val_loss: 4.9607\n","Epoch 29/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9566 - val_loss: 4.9608\n","Epoch 30/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9565 - val_loss: 4.9610\n","Epoch 31/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9564 - val_loss: 4.9594\n","Epoch 32/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9562 - val_loss: 4.9595\n","Epoch 33/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9561 - val_loss: 4.9594\n","Epoch 34/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9562 - val_loss: 4.9603\n","Epoch 35/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9571 - val_loss: 4.9629\n","Epoch 36/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9563 - val_loss: 4.9602\n","Epoch 37/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9561 - val_loss: 4.9596\n","Epoch 38/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9559 - val_loss: 4.9590\n","Epoch 39/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9559 - val_loss: 4.9595\n","Epoch 40/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9557 - val_loss: 4.9584\n","Epoch 41/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 42/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9555 - val_loss: 4.9588\n","Epoch 43/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9554 - val_loss: 4.9584\n","Epoch 44/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9553 - val_loss: 4.9589\n","Epoch 45/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9552 - val_loss: 4.9582\n","Epoch 46/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9554 - val_loss: 4.9589\n","Epoch 47/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 48/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 49/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 50/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 51/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 52/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 53/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9548 - val_loss: 4.9574\n","Epoch 54/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 55/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9548 - val_loss: 4.9585\n","Epoch 56/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9565 - val_loss: 4.9613\n","Epoch 57/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9552 - val_loss: 4.9590\n","Epoch 58/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 59/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 60/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 61/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 62/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 63/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 64/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 65/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9546 - val_loss: 4.9571\n","Epoch 66/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 67/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 68/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 69/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 70/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 71/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 72/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 73/250\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 74/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 75/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9542 - val_loss: 4.9569\n","Epoch 76/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 77/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9545 - val_loss: 4.9569\n","Epoch 78/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9542 - val_loss: 4.9566\n","Epoch 79/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9543 - val_loss: 4.9567\n","Epoch 80/250\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 81/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 82/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 83/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9542 - val_loss: 4.9566\n","Epoch 84/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9542 - val_loss: 4.9569\n","Epoch 85/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 86/250\n","5450/5450 [==============================] - 2s 335us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 87/250\n","5450/5450 [==============================] - 2s 337us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 88/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 89/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 90/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 91/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 92/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 93/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 94/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 95/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9546 - val_loss: 4.9587\n","Epoch 96/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 97/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 98/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 99/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9539 - val_loss: 4.9564\n","Epoch 100/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 101/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 102/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9543 - val_loss: 4.9587\n","Epoch 103/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 104/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 105/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 106/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 107/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 108/250\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 109/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 110/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 111/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 112/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 113/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 114/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 115/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 116/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 117/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 118/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 119/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 120/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 121/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 122/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9608\n","Epoch 123/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 124/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 125/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9539 - val_loss: 4.9564\n","Epoch 126/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 127/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 128/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 129/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 130/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 131/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 132/250\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 133/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 134/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 135/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 136/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 137/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 138/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 139/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 140/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 141/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 142/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 143/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 144/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 145/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 146/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 147/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 148/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 149/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 150/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 151/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 152/250\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 153/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 154/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 155/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 156/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 157/250\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 158/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 159/250\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 160/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 161/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 162/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 163/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 164/250\n","5450/5450 [==============================] - 2s 350us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 165/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 166/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 167/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 168/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 169/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 170/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 171/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 172/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 173/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 174/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 175/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 176/250\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 177/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 178/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 179/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 180/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 181/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 182/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 183/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 184/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 185/250\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 186/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 187/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 188/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 189/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 190/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 191/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 192/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 193/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 194/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 195/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 196/250\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 197/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 198/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 199/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 200/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9532 - val_loss: 4.9599\n","Epoch 201/250\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 202/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 203/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 204/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 205/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 206/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 207/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 208/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 209/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 210/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 211/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 212/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 213/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 214/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 215/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 216/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 217/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 218/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 219/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 220/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 221/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 222/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 223/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 224/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 225/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 226/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 227/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 228/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 229/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 230/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 231/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 232/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 233/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 234/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 235/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 236/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 237/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 238/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 239/250\n","5450/5450 [==============================] - 2s 341us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 240/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 241/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 242/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 243/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 244/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 245/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 246/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 247/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 248/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 249/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 250/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9563\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 45833 0.5\n","The shape of N (6056, 784)\n","The minimum value of N  -0.7402011752128601\n","The max value of N 0.7458391189575195\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.987883772394672\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/250\n","5450/5450 [==============================] - 4s 729us/step - loss: 5.0687 - val_loss: 5.1827\n","Epoch 2/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9974 - val_loss: 5.0299\n","Epoch 3/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9844 - val_loss: 4.9945\n","Epoch 4/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9779 - val_loss: 4.9860\n","Epoch 5/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9740 - val_loss: 4.9807\n","Epoch 6/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9713 - val_loss: 4.9782\n","Epoch 7/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9694 - val_loss: 4.9745\n","Epoch 8/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9683 - val_loss: 4.9742\n","Epoch 9/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9674 - val_loss: 4.9731\n","Epoch 10/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9663 - val_loss: 4.9726\n","Epoch 11/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9651 - val_loss: 4.9703\n","Epoch 12/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9649 - val_loss: 4.9697\n","Epoch 13/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9643 - val_loss: 4.9696\n","Epoch 14/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9637 - val_loss: 4.9703\n","Epoch 15/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9634 - val_loss: 4.9689\n","Epoch 16/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9627 - val_loss: 4.9678\n","Epoch 17/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9623 - val_loss: 4.9663\n","Epoch 18/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9619 - val_loss: 4.9663\n","Epoch 19/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9616 - val_loss: 4.9701\n","Epoch 20/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9607 - val_loss: 4.9652\n","Epoch 21/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9600 - val_loss: 4.9661\n","Epoch 22/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9592 - val_loss: 4.9647\n","Epoch 23/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9587 - val_loss: 4.9629\n","Epoch 24/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9582 - val_loss: 4.9614\n","Epoch 25/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9576 - val_loss: 4.9617\n","Epoch 26/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9573 - val_loss: 4.9613\n","Epoch 27/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9567 - val_loss: 4.9598\n","Epoch 28/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9564 - val_loss: 4.9604\n","Epoch 29/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9562 - val_loss: 4.9603\n","Epoch 30/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9562 - val_loss: 4.9605\n","Epoch 31/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9557 - val_loss: 4.9589\n","Epoch 32/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9555 - val_loss: 4.9587\n","Epoch 33/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9555 - val_loss: 4.9592\n","Epoch 34/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9553 - val_loss: 4.9585\n","Epoch 35/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9552 - val_loss: 4.9594\n","Epoch 36/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9552 - val_loss: 4.9586\n","Epoch 37/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9549 - val_loss: 4.9584\n","Epoch 38/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9549 - val_loss: 4.9591\n","Epoch 39/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 40/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9549 - val_loss: 4.9596\n","Epoch 41/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 42/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 43/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 44/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 45/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 46/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 47/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 48/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 49/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 50/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9540 - val_loss: 4.9565\n","Epoch 51/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9539 - val_loss: 4.9620\n","Epoch 52/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 53/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 54/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 55/250\n","5450/5450 [==============================] - 2s 332us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 56/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 57/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 58/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 59/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 60/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 61/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9566\n","Epoch 62/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9537 - val_loss: 4.9584\n","Epoch 63/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 64/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 65/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 66/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 67/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 68/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 69/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 70/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 71/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 72/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 73/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 74/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 75/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 76/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 77/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9532 - val_loss: 4.9555\n","Epoch 78/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 79/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 80/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 81/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 82/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9529 - val_loss: 4.9592\n","Epoch 83/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 84/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 85/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 86/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 87/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 88/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 89/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 90/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9530 - val_loss: 4.9554\n","Epoch 91/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 92/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 93/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 94/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 95/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 96/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 97/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 98/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9526 - val_loss: 4.9552\n","Epoch 99/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 100/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 101/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 102/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 103/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 104/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 105/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 106/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 107/250\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 108/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 109/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 110/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 111/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 112/250\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 113/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 114/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 115/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 116/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 117/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 118/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 119/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 120/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 121/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 122/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 123/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 124/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 125/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 126/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 127/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 128/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 129/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 130/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 131/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 132/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 133/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 134/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 135/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 136/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 137/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 138/250\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 139/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 140/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 141/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 142/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9591\n","Epoch 143/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 144/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 145/250\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 146/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 147/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 148/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 149/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 150/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 151/250\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 152/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 153/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 154/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9519 - val_loss: 4.9549\n","Epoch 155/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9520 - val_loss: 4.9557\n","Epoch 156/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 157/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 158/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 159/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 160/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 161/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 162/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 163/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 164/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 165/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 166/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 167/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 168/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 169/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 170/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 171/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 172/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 173/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 174/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 175/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 176/250\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 177/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 178/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 179/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9519 - val_loss: 4.9556\n","Epoch 180/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 181/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 182/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 183/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 184/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9518 - val_loss: 4.9551\n","Epoch 185/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 186/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9518 - val_loss: 4.9551\n","Epoch 187/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 188/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 189/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 190/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 191/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 192/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9518 - val_loss: 4.9555\n","Epoch 193/250\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 194/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 195/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9518 - val_loss: 4.9552\n","Epoch 196/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 197/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9517 - val_loss: 4.9551\n","Epoch 198/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9518 - val_loss: 4.9552\n","Epoch 199/250\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 200/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9518 - val_loss: 4.9556\n","Epoch 201/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9518 - val_loss: 4.9552\n","Epoch 202/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9552\n","Epoch 203/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 204/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9556\n","Epoch 205/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9518 - val_loss: 4.9554\n","Epoch 206/250\n","5450/5450 [==============================] - 2s 348us/step - loss: 4.9518 - val_loss: 4.9551\n","Epoch 207/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9518 - val_loss: 4.9552\n","Epoch 208/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9517 - val_loss: 4.9560\n","Epoch 209/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9518 - val_loss: 4.9553\n","Epoch 210/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 211/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 212/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9516 - val_loss: 4.9552\n","Epoch 213/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 214/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 215/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9518 - val_loss: 4.9553\n","Epoch 216/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9518 - val_loss: 4.9552\n","Epoch 217/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9518 - val_loss: 4.9552\n","Epoch 218/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 219/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 220/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9516 - val_loss: 4.9551\n","Epoch 221/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 222/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 223/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9517 - val_loss: 4.9553\n","Epoch 224/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 225/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 226/250\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9550\n","Epoch 227/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9516 - val_loss: 4.9551\n","Epoch 228/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9517 - val_loss: 4.9555\n","Epoch 229/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9516 - val_loss: 4.9552\n","Epoch 230/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9516 - val_loss: 4.9555\n","Epoch 231/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9516 - val_loss: 4.9556\n","Epoch 232/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 233/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9516 - val_loss: 4.9552\n","Epoch 234/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 235/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 236/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 237/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9516 - val_loss: 4.9554\n","Epoch 238/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 239/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 240/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9516 - val_loss: 4.9552\n","Epoch 241/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9514 - val_loss: 4.9551\n","Epoch 242/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 243/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9516 - val_loss: 4.9555\n","Epoch 244/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9516 - val_loss: 4.9554\n","Epoch 245/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9516 - val_loss: 4.9556\n","Epoch 246/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9516 - val_loss: 4.9553\n","Epoch 247/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9516 - val_loss: 4.9552\n","Epoch 248/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9515 - val_loss: 4.9555\n","Epoch 249/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9519 - val_loss: 4.9558\n","Epoch 250/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9517 - val_loss: 4.9555\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 54419 0.5\n","The shape of N (6056, 784)\n","The minimum value of N  -0.7499575614929199\n","The max value of N 0.7455791234970093\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9861343100928994\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/250\n","5450/5450 [==============================] - 5s 827us/step - loss: 5.0674 - val_loss: 5.1993\n","Epoch 2/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9994 - val_loss: 5.0219\n","Epoch 3/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9859 - val_loss: 4.9933\n","Epoch 4/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9789 - val_loss: 4.9823\n","Epoch 5/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9749 - val_loss: 4.9806\n","Epoch 6/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9725 - val_loss: 4.9749\n","Epoch 7/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9709 - val_loss: 4.9771\n","Epoch 8/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9690 - val_loss: 4.9737\n","Epoch 9/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9681 - val_loss: 4.9727\n","Epoch 10/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9669 - val_loss: 4.9737\n","Epoch 11/250\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9658 - val_loss: 4.9724\n","Epoch 12/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9653 - val_loss: 4.9703\n","Epoch 13/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9645 - val_loss: 4.9708\n","Epoch 14/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9636 - val_loss: 4.9743\n","Epoch 15/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9627 - val_loss: 4.9694\n","Epoch 16/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9619 - val_loss: 4.9685\n","Epoch 17/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9616 - val_loss: 4.9686\n","Epoch 18/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9608 - val_loss: 4.9665\n","Epoch 19/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9604 - val_loss: 4.9668\n","Epoch 20/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9596 - val_loss: 4.9668\n","Epoch 21/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9589 - val_loss: 4.9653\n","Epoch 22/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9585 - val_loss: 4.9641\n","Epoch 23/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9583 - val_loss: 4.9631\n","Epoch 24/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9577 - val_loss: 4.9643\n","Epoch 25/250\n","5450/5450 [==============================] - 2s 351us/step - loss: 4.9577 - val_loss: 4.9626\n","Epoch 26/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9575 - val_loss: 4.9612\n","Epoch 27/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9571 - val_loss: 4.9611\n","Epoch 28/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9568 - val_loss: 4.9617\n","Epoch 29/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9566 - val_loss: 4.9602\n","Epoch 30/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9567 - val_loss: 4.9621\n","Epoch 31/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9565 - val_loss: 4.9599\n","Epoch 32/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9561 - val_loss: 4.9608\n","Epoch 33/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9560 - val_loss: 4.9608\n","Epoch 34/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9562 - val_loss: 4.9622\n","Epoch 35/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9558 - val_loss: 4.9617\n","Epoch 36/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9557 - val_loss: 4.9597\n","Epoch 37/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9556 - val_loss: 4.9595\n","Epoch 38/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9555 - val_loss: 4.9595\n","Epoch 39/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9556 - val_loss: 4.9595\n","Epoch 40/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9556 - val_loss: 4.9618\n","Epoch 41/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9554 - val_loss: 4.9609\n","Epoch 42/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9554 - val_loss: 4.9591\n","Epoch 43/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9552 - val_loss: 4.9597\n","Epoch 44/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9551 - val_loss: 4.9586\n","Epoch 45/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9550 - val_loss: 4.9592\n","Epoch 46/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9550 - val_loss: 4.9587\n","Epoch 47/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9550 - val_loss: 4.9582\n","Epoch 48/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9550 - val_loss: 4.9601\n","Epoch 49/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9565 - val_loss: 4.9631\n","Epoch 50/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9553 - val_loss: 4.9613\n","Epoch 51/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 52/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9550 - val_loss: 4.9590\n","Epoch 53/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9547 - val_loss: 4.9588\n","Epoch 54/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9583\n","Epoch 55/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 56/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9544 - val_loss: 4.9567\n","Epoch 57/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 58/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 59/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 60/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 61/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9549 - val_loss: 4.9571\n","Epoch 62/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9544 - val_loss: 4.9568\n","Epoch 63/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9542 - val_loss: 4.9569\n","Epoch 64/250\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9564\n","Epoch 65/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9542 - val_loss: 4.9566\n","Epoch 66/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9542 - val_loss: 4.9564\n","Epoch 67/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 68/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9541 - val_loss: 4.9565\n","Epoch 69/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 70/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 71/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 72/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9540 - val_loss: 4.9565\n","Epoch 73/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9542 - val_loss: 4.9565\n","Epoch 74/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9539 - val_loss: 4.9564\n","Epoch 75/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 76/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9545 - val_loss: 4.9569\n","Epoch 77/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9540 - val_loss: 4.9565\n","Epoch 78/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9540 - val_loss: 4.9564\n","Epoch 79/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9540 - val_loss: 4.9565\n","Epoch 80/250\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 81/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 82/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 83/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 84/250\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 85/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 86/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 87/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 88/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 89/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 90/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 91/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 92/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 93/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 94/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 95/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 96/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 97/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 98/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 99/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 100/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 101/250\n","5450/5450 [==============================] - 2s 338us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 102/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 103/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 104/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 105/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 106/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 107/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 108/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 109/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 110/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 111/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9550 - val_loss: 4.9616\n","Epoch 112/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 113/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 114/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 115/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 116/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 117/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9542 - val_loss: 4.9590\n","Epoch 118/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 119/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 120/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 121/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 122/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 123/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 124/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 125/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 126/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 127/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 128/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 129/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 130/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 131/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 132/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 133/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 134/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 135/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 136/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 137/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 138/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 139/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 140/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 141/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 142/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 143/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 144/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 145/250\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 146/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 147/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 148/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 149/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 150/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 151/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 152/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 153/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 154/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 155/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 156/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 157/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 158/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 159/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 160/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 161/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 162/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 163/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 164/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 165/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 166/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 167/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 168/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 169/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 170/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 171/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 172/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 173/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 174/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 175/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 176/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 177/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 178/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 179/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 180/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 181/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 182/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 183/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 184/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 185/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 186/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 187/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 188/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 189/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 190/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 191/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 192/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 193/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 194/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 195/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 196/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 197/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 198/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 199/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 200/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 201/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 202/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 203/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 204/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 205/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 206/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 207/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 208/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 209/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 210/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 211/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 212/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 213/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 214/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 215/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 216/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 217/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9528 - val_loss: 4.9592\n","Epoch 218/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 219/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 220/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 221/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 222/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 223/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 224/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 225/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 226/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 227/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 228/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 229/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 230/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 231/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 232/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 233/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 234/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 235/250\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 236/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 237/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 238/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 239/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 240/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 241/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 242/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 243/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 244/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 245/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 246/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 247/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 248/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 249/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 250/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9526 - val_loss: 4.9565\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 56144 0.5\n","The shape of N (6056, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7495497465133667\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9704456748148086\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/250\n","5450/5450 [==============================] - 5s 968us/step - loss: 5.0714 - val_loss: 5.1169\n","Epoch 2/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9980 - val_loss: 5.0164\n","Epoch 3/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9839 - val_loss: 4.9913\n","Epoch 4/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9773 - val_loss: 4.9836\n","Epoch 5/250\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9736 - val_loss: 4.9779\n","Epoch 6/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9713 - val_loss: 4.9764\n","Epoch 7/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9696 - val_loss: 4.9733\n","Epoch 8/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9684 - val_loss: 4.9741\n","Epoch 9/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9673 - val_loss: 4.9737\n","Epoch 10/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9664 - val_loss: 4.9705\n","Epoch 11/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9656 - val_loss: 4.9711\n","Epoch 12/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9650 - val_loss: 4.9706\n","Epoch 13/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9645 - val_loss: 4.9689\n","Epoch 14/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9638 - val_loss: 4.9690\n","Epoch 15/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9633 - val_loss: 4.9679\n","Epoch 16/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9625 - val_loss: 4.9670\n","Epoch 17/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9617 - val_loss: 4.9657\n","Epoch 18/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9612 - val_loss: 4.9654\n","Epoch 19/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9605 - val_loss: 4.9646\n","Epoch 20/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9595 - val_loss: 4.9649\n","Epoch 21/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9592 - val_loss: 4.9675\n","Epoch 22/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9589 - val_loss: 4.9640\n","Epoch 23/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9582 - val_loss: 4.9637\n","Epoch 24/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9579 - val_loss: 4.9627\n","Epoch 25/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9574 - val_loss: 4.9621\n","Epoch 26/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9571 - val_loss: 4.9619\n","Epoch 27/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9567 - val_loss: 4.9621\n","Epoch 28/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9566 - val_loss: 4.9661\n","Epoch 29/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9565 - val_loss: 4.9612\n","Epoch 30/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9563 - val_loss: 4.9617\n","Epoch 31/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9561 - val_loss: 4.9616\n","Epoch 32/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9560 - val_loss: 4.9598\n","Epoch 33/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9559 - val_loss: 4.9596\n","Epoch 34/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9555 - val_loss: 4.9589\n","Epoch 35/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9555 - val_loss: 4.9597\n","Epoch 36/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9555 - val_loss: 4.9593\n","Epoch 37/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9553 - val_loss: 4.9594\n","Epoch 38/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9552 - val_loss: 4.9594\n","Epoch 39/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9552 - val_loss: 4.9594\n","Epoch 40/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9551 - val_loss: 4.9592\n","Epoch 41/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 42/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9548 - val_loss: 4.9585\n","Epoch 43/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9547 - val_loss: 4.9584\n","Epoch 44/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 45/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 46/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 47/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 48/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 49/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 50/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 51/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 52/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 53/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 54/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 55/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 56/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 57/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 58/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 59/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 60/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 61/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 62/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 63/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 64/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 65/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 66/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 67/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 68/250\n","5450/5450 [==============================] - 2s 341us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 69/250\n","5450/5450 [==============================] - 2s 338us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 70/250\n","5450/5450 [==============================] - 2s 340us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 71/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 72/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 73/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 74/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 75/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 76/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 77/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 78/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 79/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 80/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 81/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 82/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 83/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 84/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 85/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 86/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 87/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 88/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 89/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 90/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 91/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 92/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 93/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 94/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 95/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 96/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 97/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 98/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 99/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 100/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 101/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 102/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 103/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 104/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 105/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 106/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 107/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 108/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 109/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 110/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 111/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 112/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 113/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 114/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 115/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 116/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 117/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 118/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 119/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 120/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 121/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 122/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 123/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 124/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 125/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 126/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 127/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 128/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 129/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 130/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 131/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 132/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 133/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 134/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 135/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 136/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 137/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 138/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 139/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 140/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 141/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 142/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 143/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 144/250\n","5450/5450 [==============================] - 2s 335us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 145/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 146/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 147/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 148/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 149/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 150/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 151/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 152/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 153/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 154/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 155/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 156/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 157/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 158/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 159/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 160/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 161/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 162/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 163/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 164/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 165/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 166/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 167/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 168/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 169/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 170/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 171/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 172/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 173/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 174/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 175/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 176/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 177/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 178/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 179/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 180/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 181/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 182/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 183/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 184/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 185/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 186/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 187/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 188/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 189/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 190/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 191/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 192/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 193/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 194/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 195/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 196/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 197/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 198/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 199/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 200/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 201/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 202/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 203/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 204/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 205/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 206/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 207/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 208/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 209/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 210/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 211/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 212/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 213/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 214/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 215/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 216/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 217/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 218/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 219/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 220/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 221/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 222/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 223/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9522 - val_loss: 4.9570\n","Epoch 224/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 225/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 226/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 227/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 228/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 229/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 230/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 231/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 232/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 233/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 234/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 235/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 236/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 237/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9523 - val_loss: 4.9566\n","Epoch 238/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 239/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 240/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 241/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 242/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 243/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 244/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 245/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 246/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 247/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 248/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 249/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 250/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9522 - val_loss: 4.9566\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 54163 0.5\n","The shape of N (6056, 784)\n","The minimum value of N  -0.7444043159484863\n","The max value of N 0.7459104657173157\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9886609971652492\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/250\n","5450/5450 [==============================] - 6s 1ms/step - loss: 5.0774 - val_loss: 5.2106\n","Epoch 2/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 5.0042 - val_loss: 5.0178\n","Epoch 3/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9877 - val_loss: 4.9931\n","Epoch 4/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9797 - val_loss: 4.9866\n","Epoch 5/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9751 - val_loss: 4.9799\n","Epoch 6/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9722 - val_loss: 4.9783\n","Epoch 7/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9704 - val_loss: 4.9748\n","Epoch 8/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9689 - val_loss: 4.9745\n","Epoch 9/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9679 - val_loss: 4.9738\n","Epoch 10/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9669 - val_loss: 4.9720\n","Epoch 11/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9665 - val_loss: 4.9715\n","Epoch 12/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9654 - val_loss: 4.9717\n","Epoch 13/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9649 - val_loss: 4.9704\n","Epoch 14/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9640 - val_loss: 4.9694\n","Epoch 15/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9629 - val_loss: 4.9700\n","Epoch 16/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9623 - val_loss: 4.9694\n","Epoch 17/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9615 - val_loss: 4.9671\n","Epoch 18/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9609 - val_loss: 4.9667\n","Epoch 19/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9601 - val_loss: 4.9653\n","Epoch 20/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9597 - val_loss: 4.9647\n","Epoch 21/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9592 - val_loss: 4.9652\n","Epoch 22/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9588 - val_loss: 4.9640\n","Epoch 23/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9587 - val_loss: 4.9639\n","Epoch 24/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9581 - val_loss: 4.9639\n","Epoch 25/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9575 - val_loss: 4.9619\n","Epoch 26/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9573 - val_loss: 4.9616\n","Epoch 27/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9571 - val_loss: 4.9618\n","Epoch 28/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9570 - val_loss: 4.9613\n","Epoch 29/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9571 - val_loss: 4.9617\n","Epoch 30/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9570 - val_loss: 4.9614\n","Epoch 31/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9567 - val_loss: 4.9604\n","Epoch 32/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9565 - val_loss: 4.9613\n","Epoch 33/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9563 - val_loss: 4.9605\n","Epoch 34/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9562 - val_loss: 4.9598\n","Epoch 35/250\n","5450/5450 [==============================] - 2s 335us/step - loss: 4.9558 - val_loss: 4.9596\n","Epoch 36/250\n","5450/5450 [==============================] - 2s 342us/step - loss: 4.9558 - val_loss: 4.9604\n","Epoch 37/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9560 - val_loss: 4.9604\n","Epoch 38/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9557 - val_loss: 4.9592\n","Epoch 39/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9557 - val_loss: 4.9594\n","Epoch 40/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9553 - val_loss: 4.9593\n","Epoch 41/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9557 - val_loss: 4.9624\n","Epoch 42/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9554 - val_loss: 4.9603\n","Epoch 43/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9552 - val_loss: 4.9603\n","Epoch 44/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9551 - val_loss: 4.9586\n","Epoch 45/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 46/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9552 - val_loss: 4.9583\n","Epoch 47/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9594\n","Epoch 48/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9553 - val_loss: 4.9608\n","Epoch 49/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9550 - val_loss: 4.9591\n","Epoch 50/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9549 - val_loss: 4.9585\n","Epoch 51/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9549 - val_loss: 4.9586\n","Epoch 52/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 53/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 54/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9546 - val_loss: 4.9587\n","Epoch 55/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9553 - val_loss: 4.9603\n","Epoch 56/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9548 - val_loss: 4.9585\n","Epoch 57/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9546 - val_loss: 4.9590\n","Epoch 58/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9547 - val_loss: 4.9587\n","Epoch 59/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 60/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 61/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 62/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 63/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 64/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 65/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 66/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 67/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 68/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 69/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 70/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 71/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 72/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 73/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 74/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 75/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 76/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 77/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 78/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 79/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 80/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 81/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 82/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 83/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 84/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 85/250\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 86/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 87/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 88/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 89/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 90/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 91/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 92/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 93/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 94/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 95/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 96/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 97/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 98/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 99/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 100/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 101/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 102/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 103/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 104/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 105/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 106/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 107/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 108/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 109/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 110/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 111/250\n","5450/5450 [==============================] - 2s 336us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 112/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 113/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 114/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 115/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 116/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 117/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 118/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 119/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 120/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 121/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 122/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 123/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 124/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 125/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 126/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 127/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 128/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 129/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 130/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 131/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 132/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 133/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 134/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 135/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 136/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 137/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 138/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 139/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 140/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 141/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 142/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9534 - val_loss: 4.9584\n","Epoch 143/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 144/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 145/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 146/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 147/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 148/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 149/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 150/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 151/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 152/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 153/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 154/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 155/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 156/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 157/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 158/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 159/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 160/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 161/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 162/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 163/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 164/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 165/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 166/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 167/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 168/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 169/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 170/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 171/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 172/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 173/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 174/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 175/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 176/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 177/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 178/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 179/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 180/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 181/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 182/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 183/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 184/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 185/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 186/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 187/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 188/250\n","5450/5450 [==============================] - 2s 338us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 189/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 190/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 191/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 192/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 193/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 194/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 195/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 196/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 197/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 198/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 199/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 200/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 201/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 202/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 203/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 204/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 205/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 206/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 207/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 208/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 209/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 210/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 211/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 212/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 213/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 214/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 215/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 216/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9527 - val_loss: 4.9581\n","Epoch 217/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 218/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 219/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 220/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 221/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 222/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 223/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 224/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 225/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 226/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 227/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 228/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 229/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 230/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 231/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 232/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 233/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 234/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 235/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 236/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 237/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 238/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 239/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 240/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 241/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 242/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 243/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 244/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 245/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 246/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 247/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 248/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 249/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 250/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9525 - val_loss: 4.9564\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 44702 0.5\n","The shape of N (6056, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7496106624603271\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9917190233534847\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/250\n","5450/5450 [==============================] - 7s 1ms/step - loss: 5.0695 - val_loss: 5.1701\n","Epoch 2/250\n","5450/5450 [==============================] - 2s 293us/step - loss: 5.0014 - val_loss: 5.0301\n","Epoch 3/250\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9867 - val_loss: 4.9957\n","Epoch 4/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9795 - val_loss: 4.9852\n","Epoch 5/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9756 - val_loss: 4.9786\n","Epoch 6/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9725 - val_loss: 4.9777\n","Epoch 7/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9703 - val_loss: 4.9750\n","Epoch 8/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9685 - val_loss: 4.9747\n","Epoch 9/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9671 - val_loss: 4.9722\n","Epoch 10/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9661 - val_loss: 4.9700\n","Epoch 11/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9649 - val_loss: 4.9696\n","Epoch 12/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9636 - val_loss: 4.9694\n","Epoch 13/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9629 - val_loss: 4.9703\n","Epoch 14/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9620 - val_loss: 4.9674\n","Epoch 15/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9617 - val_loss: 4.9670\n","Epoch 16/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9610 - val_loss: 4.9681\n","Epoch 17/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9604 - val_loss: 4.9661\n","Epoch 18/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9598 - val_loss: 4.9655\n","Epoch 19/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9594 - val_loss: 4.9655\n","Epoch 20/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9595 - val_loss: 4.9648\n","Epoch 21/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9588 - val_loss: 4.9661\n","Epoch 22/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9587 - val_loss: 4.9639\n","Epoch 23/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9581 - val_loss: 4.9640\n","Epoch 24/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9582 - val_loss: 4.9636\n","Epoch 25/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9582 - val_loss: 4.9658\n","Epoch 26/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9578 - val_loss: 4.9643\n","Epoch 27/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9577 - val_loss: 4.9615\n","Epoch 28/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9575 - val_loss: 4.9612\n","Epoch 29/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9574 - val_loss: 4.9611\n","Epoch 30/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9570 - val_loss: 4.9619\n","Epoch 31/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9568 - val_loss: 4.9613\n","Epoch 32/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9566 - val_loss: 4.9609\n","Epoch 33/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9567 - val_loss: 4.9600\n","Epoch 34/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9565 - val_loss: 4.9598\n","Epoch 35/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9565 - val_loss: 4.9598\n","Epoch 36/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9569 - val_loss: 4.9658\n","Epoch 37/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9567 - val_loss: 4.9631\n","Epoch 38/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9564 - val_loss: 4.9620\n","Epoch 39/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9562 - val_loss: 4.9610\n","Epoch 40/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9569 - val_loss: 4.9621\n","Epoch 41/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9563 - val_loss: 4.9619\n","Epoch 42/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9560 - val_loss: 4.9602\n","Epoch 43/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9559 - val_loss: 4.9599\n","Epoch 44/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9559 - val_loss: 4.9592\n","Epoch 45/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9558 - val_loss: 4.9596\n","Epoch 46/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9558 - val_loss: 4.9590\n","Epoch 47/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9556 - val_loss: 4.9596\n","Epoch 48/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9556 - val_loss: 4.9596\n","Epoch 49/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9555 - val_loss: 4.9593\n","Epoch 50/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9556 - val_loss: 4.9581\n","Epoch 51/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9555 - val_loss: 4.9589\n","Epoch 52/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 53/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9553 - val_loss: 4.9577\n","Epoch 54/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9552 - val_loss: 4.9579\n","Epoch 55/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 56/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9552 - val_loss: 4.9576\n","Epoch 57/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9576\n","Epoch 58/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9576\n","Epoch 59/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9551 - val_loss: 4.9576\n","Epoch 60/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 61/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9557 - val_loss: 4.9583\n","Epoch 62/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9552 - val_loss: 4.9572\n","Epoch 63/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 64/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9547 - val_loss: 4.9570\n","Epoch 65/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9551 - val_loss: 4.9573\n","Epoch 66/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 67/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9549 - val_loss: 4.9572\n","Epoch 68/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9551 - val_loss: 4.9576\n","Epoch 69/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 70/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9547 - val_loss: 4.9574\n","Epoch 71/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 72/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 73/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9547 - val_loss: 4.9571\n","Epoch 74/250\n","5450/5450 [==============================] - 2s 332us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 75/250\n","5450/5450 [==============================] - 2s 354us/step - loss: 4.9559 - val_loss: 4.9613\n","Epoch 76/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 77/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9548 - val_loss: 4.9574\n","Epoch 78/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9547 - val_loss: 4.9571\n","Epoch 79/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9550 - val_loss: 4.9576\n","Epoch 80/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9547 - val_loss: 4.9574\n","Epoch 81/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9546 - val_loss: 4.9587\n","Epoch 82/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 83/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 84/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9571\n","Epoch 85/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 86/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 87/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 88/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 89/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 90/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 91/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 92/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 93/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 94/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 95/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 96/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 97/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 98/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 99/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 100/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 101/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 102/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9541 - val_loss: 4.9593\n","Epoch 103/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 104/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 105/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 106/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9548 - val_loss: 4.9604\n","Epoch 107/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9591\n","Epoch 108/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 109/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 110/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 111/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 112/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 113/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 114/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 115/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 116/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9540 - val_loss: 4.9565\n","Epoch 117/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 118/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 119/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 120/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 121/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 122/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 123/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 124/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 125/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 126/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 127/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 128/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 129/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 130/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 131/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 132/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9546 - val_loss: 4.9588\n","Epoch 133/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 134/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 135/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 136/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 137/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 138/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 139/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 140/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 141/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 142/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 143/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 144/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 145/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 146/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 147/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 148/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 149/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 150/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 151/250\n","5450/5450 [==============================] - 2s 341us/step - loss: 4.9542 - val_loss: 4.9569\n","Epoch 152/250\n","5450/5450 [==============================] - 2s 341us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 153/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 154/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 155/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 156/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 157/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 158/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 159/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 160/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 161/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 162/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 163/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 164/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 165/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 166/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 167/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 168/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 169/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 170/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 171/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 172/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 173/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 174/250\n","5450/5450 [==============================] - 2s 329us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 175/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 176/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 177/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 178/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 179/250\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 180/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 181/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 182/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 183/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 184/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 185/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 186/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 187/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 188/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 189/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 190/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 191/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 192/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 193/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 194/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 195/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 196/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 197/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 198/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 199/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 200/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 201/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 202/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 203/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 204/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 205/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 206/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 207/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 208/250\n","5450/5450 [==============================] - 2s 329us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 209/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 210/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 211/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 212/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 213/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 214/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 215/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 216/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 217/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 218/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 219/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 220/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 221/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 222/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 223/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 224/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 225/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 226/250\n","5450/5450 [==============================] - 2s 342us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 227/250\n","5450/5450 [==============================] - 2s 340us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 228/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9536 - val_loss: 4.9584\n","Epoch 229/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 230/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 231/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 232/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 233/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 234/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9585\n","Epoch 235/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 236/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 237/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 238/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 239/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 240/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 241/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 242/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 243/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9546 - val_loss: 4.9662\n","Epoch 244/250\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 245/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 246/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 247/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 248/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 249/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 250/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9533 - val_loss: 4.9572\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 61285 0.5\n","The shape of N (6056, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7490910887718201\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9543387512965523\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/250\n","5450/5450 [==============================] - 7s 1ms/step - loss: 5.0745 - val_loss: 5.1834\n","Epoch 2/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9994 - val_loss: 5.0233\n","Epoch 3/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9844 - val_loss: 4.9931\n","Epoch 4/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9777 - val_loss: 4.9862\n","Epoch 5/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9740 - val_loss: 4.9814\n","Epoch 6/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9711 - val_loss: 4.9777\n","Epoch 7/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9691 - val_loss: 4.9753\n","Epoch 8/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9673 - val_loss: 4.9740\n","Epoch 9/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9660 - val_loss: 4.9729\n","Epoch 10/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9649 - val_loss: 4.9737\n","Epoch 11/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9639 - val_loss: 4.9721\n","Epoch 12/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9632 - val_loss: 4.9694\n","Epoch 13/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9623 - val_loss: 4.9680\n","Epoch 14/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9614 - val_loss: 4.9682\n","Epoch 15/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9605 - val_loss: 4.9674\n","Epoch 16/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9604 - val_loss: 4.9674\n","Epoch 17/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9596 - val_loss: 4.9650\n","Epoch 18/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9589 - val_loss: 4.9646\n","Epoch 19/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9582 - val_loss: 4.9637\n","Epoch 20/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9580 - val_loss: 4.9639\n","Epoch 21/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9575 - val_loss: 4.9614\n","Epoch 22/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9570 - val_loss: 4.9623\n","Epoch 23/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9574 - val_loss: 4.9616\n","Epoch 24/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9570 - val_loss: 4.9615\n","Epoch 25/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9565 - val_loss: 4.9623\n","Epoch 26/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9567 - val_loss: 4.9613\n","Epoch 27/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9563 - val_loss: 4.9603\n","Epoch 28/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9560 - val_loss: 4.9600\n","Epoch 29/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9557 - val_loss: 4.9599\n","Epoch 30/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9555 - val_loss: 4.9586\n","Epoch 31/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9557 - val_loss: 4.9590\n","Epoch 32/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9553 - val_loss: 4.9583\n","Epoch 33/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9552 - val_loss: 4.9583\n","Epoch 34/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9550 - val_loss: 4.9585\n","Epoch 35/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9552 - val_loss: 4.9577\n","Epoch 36/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9550 - val_loss: 4.9587\n","Epoch 37/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 38/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 39/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 40/250\n","5450/5450 [==============================] - 2s 336us/step - loss: 4.9545 - val_loss: 4.9569\n","Epoch 41/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9544 - val_loss: 4.9567\n","Epoch 42/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9544 - val_loss: 4.9566\n","Epoch 43/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 44/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 45/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9543 - val_loss: 4.9567\n","Epoch 46/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 47/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9546 - val_loss: 4.9686\n","Epoch 48/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9555 - val_loss: 4.9597\n","Epoch 49/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 50/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 51/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 52/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 53/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9541 - val_loss: 4.9565\n","Epoch 54/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9539 - val_loss: 4.9561\n","Epoch 55/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 56/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9539 - val_loss: 4.9561\n","Epoch 57/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9540 - val_loss: 4.9564\n","Epoch 58/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9540 - val_loss: 4.9563\n","Epoch 59/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9536 - val_loss: 4.9559\n","Epoch 60/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 61/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 62/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 63/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9536 - val_loss: 4.9557\n","Epoch 64/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9537 - val_loss: 4.9558\n","Epoch 65/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 66/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 67/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 68/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9559\n","Epoch 69/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 70/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 71/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9534 - val_loss: 4.9557\n","Epoch 72/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 73/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 74/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 75/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9533 - val_loss: 4.9555\n","Epoch 76/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 77/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 78/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 79/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 80/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9555\n","Epoch 81/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 82/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 83/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 84/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9531 - val_loss: 4.9554\n","Epoch 85/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 86/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9532 - val_loss: 4.9555\n","Epoch 87/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 88/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 89/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 90/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 91/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 92/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 93/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9531 - val_loss: 4.9554\n","Epoch 94/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 95/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 96/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 97/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 98/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9533 - val_loss: 4.9604\n","Epoch 99/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 100/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 101/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 102/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9530 - val_loss: 4.9553\n","Epoch 103/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9529 - val_loss: 4.9553\n","Epoch 104/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 105/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 106/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 107/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 108/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 109/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 110/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 111/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 112/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9547 - val_loss: 4.9601\n","Epoch 113/250\n","5450/5450 [==============================] - 2s 342us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 114/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 115/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 116/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 117/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 118/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 119/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 120/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 121/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 122/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 123/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9527 - val_loss: 4.9550\n","Epoch 124/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9527 - val_loss: 4.9551\n","Epoch 125/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9526 - val_loss: 4.9551\n","Epoch 126/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 127/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9526 - val_loss: 4.9552\n","Epoch 128/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9526 - val_loss: 4.9551\n","Epoch 129/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9525 - val_loss: 4.9551\n","Epoch 130/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9526 - val_loss: 4.9551\n","Epoch 131/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 132/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 133/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9525 - val_loss: 4.9551\n","Epoch 134/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9525 - val_loss: 4.9551\n","Epoch 135/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 136/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 137/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 138/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 139/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 140/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 141/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 142/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9527 - val_loss: 4.9550\n","Epoch 143/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 144/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 145/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9525 - val_loss: 4.9551\n","Epoch 146/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 147/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 148/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9524 - val_loss: 4.9551\n","Epoch 149/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9523 - val_loss: 4.9551\n","Epoch 150/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 151/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 152/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 153/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 154/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 155/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9525 - val_loss: 4.9551\n","Epoch 156/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 157/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9524 - val_loss: 4.9551\n","Epoch 158/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 159/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 160/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 161/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9522 - val_loss: 4.9550\n","Epoch 162/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 163/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 164/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 165/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 166/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 167/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 168/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 169/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 170/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 171/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 172/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9523 - val_loss: 4.9551\n","Epoch 173/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9551\n","Epoch 174/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 175/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9523 - val_loss: 4.9551\n","Epoch 176/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 177/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 178/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9523 - val_loss: 4.9551\n","Epoch 179/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9522 - val_loss: 4.9551\n","Epoch 180/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9522 - val_loss: 4.9550\n","Epoch 181/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 182/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9523 - val_loss: 4.9551\n","Epoch 183/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9522 - val_loss: 4.9551\n","Epoch 184/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 185/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 186/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 187/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 188/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 189/250\n","5450/5450 [==============================] - 2s 341us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 190/250\n","5450/5450 [==============================] - 2s 339us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 191/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 192/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 193/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 194/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 195/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 196/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 197/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 198/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 199/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 200/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 201/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 202/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 203/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9521 - val_loss: 4.9550\n","Epoch 204/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9522 - val_loss: 4.9550\n","Epoch 205/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 206/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 207/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 208/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 209/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 210/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 211/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 212/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 213/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 214/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 215/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 216/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 217/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 218/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 219/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 220/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 221/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 222/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 223/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 224/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 225/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 226/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 227/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 228/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 229/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 230/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 231/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9520 - val_loss: 4.9550\n","Epoch 232/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 233/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 234/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 235/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 236/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 237/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 238/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 239/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9520 - val_loss: 4.9552\n","Epoch 240/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9519 - val_loss: 4.9550\n","Epoch 241/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 242/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9520 - val_loss: 4.9553\n","Epoch 243/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 244/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9529 - val_loss: 4.9689\n","Epoch 245/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 246/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9521 - val_loss: 4.9558\n","Epoch 247/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 248/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 249/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9519 - val_loss: 4.9551\n","Epoch 250/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9520 - val_loss: 4.9551\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 43771 0.5\n","The shape of N (6056, 784)\n","The minimum value of N  -0.7499985694885254\n","The max value of N 0.7499167323112488\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9678596360327055\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/250\n","5450/5450 [==============================] - 8s 1ms/step - loss: 5.0624 - val_loss: 5.0916\n","Epoch 2/250\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9994 - val_loss: 5.0115\n","Epoch 3/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9862 - val_loss: 4.9942\n","Epoch 4/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9791 - val_loss: 4.9836\n","Epoch 5/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9752 - val_loss: 4.9799\n","Epoch 6/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9721 - val_loss: 4.9751\n","Epoch 7/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9694 - val_loss: 4.9721\n","Epoch 8/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9674 - val_loss: 4.9725\n","Epoch 9/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9665 - val_loss: 4.9698\n","Epoch 10/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9644 - val_loss: 4.9674\n","Epoch 11/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9632 - val_loss: 4.9683\n","Epoch 12/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9627 - val_loss: 4.9675\n","Epoch 13/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9636 - val_loss: 4.9694\n","Epoch 14/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9617 - val_loss: 4.9656\n","Epoch 15/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9607 - val_loss: 4.9679\n","Epoch 16/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9596 - val_loss: 4.9659\n","Epoch 17/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9592 - val_loss: 4.9646\n","Epoch 18/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9591 - val_loss: 4.9641\n","Epoch 19/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9589 - val_loss: 4.9677\n","Epoch 20/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9586 - val_loss: 4.9643\n","Epoch 21/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9581 - val_loss: 4.9633\n","Epoch 22/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9575 - val_loss: 4.9630\n","Epoch 23/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9573 - val_loss: 4.9632\n","Epoch 24/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9577 - val_loss: 4.9611\n","Epoch 25/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9571 - val_loss: 4.9605\n","Epoch 26/250\n","5450/5450 [==============================] - 2s 332us/step - loss: 4.9567 - val_loss: 4.9602\n","Epoch 27/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9569 - val_loss: 4.9704\n","Epoch 28/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9567 - val_loss: 4.9608\n","Epoch 29/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9564 - val_loss: 4.9596\n","Epoch 30/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9564 - val_loss: 4.9595\n","Epoch 31/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9565 - val_loss: 4.9598\n","Epoch 32/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9561 - val_loss: 4.9589\n","Epoch 33/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9561 - val_loss: 4.9650\n","Epoch 34/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9561 - val_loss: 4.9596\n","Epoch 35/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9559 - val_loss: 4.9593\n","Epoch 36/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9556 - val_loss: 4.9594\n","Epoch 37/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9591 - val_loss: 5.0027\n","Epoch 38/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9572 - val_loss: 4.9793\n","Epoch 39/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9617 - val_loss: 4.9882\n","Epoch 40/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9579 - val_loss: 4.9707\n","Epoch 41/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9568 - val_loss: 4.9638\n","Epoch 42/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9564 - val_loss: 4.9608\n","Epoch 43/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9560 - val_loss: 4.9593\n","Epoch 44/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9560 - val_loss: 4.9593\n","Epoch 45/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9558 - val_loss: 4.9581\n","Epoch 46/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9558 - val_loss: 4.9584\n","Epoch 47/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9556 - val_loss: 4.9578\n","Epoch 48/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9555 - val_loss: 4.9581\n","Epoch 49/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9553 - val_loss: 4.9573\n","Epoch 50/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 51/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 52/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9551 - val_loss: 4.9580\n","Epoch 53/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9549 - val_loss: 4.9577\n","Epoch 54/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9550 - val_loss: 4.9590\n","Epoch 55/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9583 - val_loss: 4.9668\n","Epoch 56/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9562 - val_loss: 4.9600\n","Epoch 57/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 58/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9553 - val_loss: 4.9574\n","Epoch 59/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9550 - val_loss: 4.9571\n","Epoch 60/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9549 - val_loss: 4.9572\n","Epoch 61/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9547 - val_loss: 4.9566\n","Epoch 62/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9546 - val_loss: 4.9566\n","Epoch 63/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 64/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9545 - val_loss: 4.9569\n","Epoch 65/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9546 - val_loss: 4.9566\n","Epoch 66/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9545 - val_loss: 4.9565\n","Epoch 67/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9545 - val_loss: 4.9565\n","Epoch 68/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9545 - val_loss: 4.9564\n","Epoch 69/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 70/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9543 - val_loss: 4.9566\n","Epoch 71/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9544 - val_loss: 4.9563\n","Epoch 72/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9544 - val_loss: 4.9567\n","Epoch 73/250\n","5450/5450 [==============================] - 2s 342us/step - loss: 4.9544 - val_loss: 4.9563\n","Epoch 74/250\n","5450/5450 [==============================] - 2s 340us/step - loss: 4.9541 - val_loss: 4.9562\n","Epoch 75/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 76/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9541 - val_loss: 4.9563\n","Epoch 77/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9542 - val_loss: 4.9565\n","Epoch 78/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9541 - val_loss: 4.9565\n","Epoch 79/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9561\n","Epoch 80/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9540 - val_loss: 4.9561\n","Epoch 81/250\n","5450/5450 [==============================] - 2s 329us/step - loss: 4.9541 - val_loss: 4.9562\n","Epoch 82/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9540 - val_loss: 4.9560\n","Epoch 83/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 84/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9540 - val_loss: 4.9561\n","Epoch 85/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9539 - val_loss: 4.9561\n","Epoch 86/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9540 - val_loss: 4.9564\n","Epoch 87/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9541 - val_loss: 4.9563\n","Epoch 88/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9540 - val_loss: 4.9561\n","Epoch 89/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9561\n","Epoch 90/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9538 - val_loss: 4.9559\n","Epoch 91/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 92/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9538 - val_loss: 4.9559\n","Epoch 93/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 94/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 95/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 96/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 97/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 98/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 99/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 100/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 101/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9536 - val_loss: 4.9559\n","Epoch 102/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9536 - val_loss: 4.9557\n","Epoch 103/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 104/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9536 - val_loss: 4.9559\n","Epoch 105/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 106/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 107/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 108/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9536 - val_loss: 4.9589\n","Epoch 109/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 110/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 111/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 112/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 113/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 114/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 115/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 116/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9546 - val_loss: 4.9676\n","Epoch 117/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9541 - val_loss: 4.9610\n","Epoch 118/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 119/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 120/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 121/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 122/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 123/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 124/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 125/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9534 - val_loss: 4.9560\n","Epoch 126/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 127/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9534 - val_loss: 4.9560\n","Epoch 128/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 129/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 130/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 131/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 132/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 133/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9534 - val_loss: 4.9560\n","Epoch 134/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 135/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 136/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 137/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 138/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 139/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 140/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 141/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 142/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 143/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 144/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 145/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 146/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 147/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 148/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 149/250\n","5450/5450 [==============================] - 2s 341us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 150/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 151/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 152/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 153/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 154/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 155/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 156/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 157/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 158/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 159/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 160/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 161/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 162/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 163/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 164/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9543 - val_loss: 4.9669\n","Epoch 165/250\n","5450/5450 [==============================] - 2s 330us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 166/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 167/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 168/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 169/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 170/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 171/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 172/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 173/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 174/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 175/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 176/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 177/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 178/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 179/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 180/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 181/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 182/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 183/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 184/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 185/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 186/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 187/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 188/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 189/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 190/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 191/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 192/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 193/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 194/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 195/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 196/250\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 197/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 198/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 199/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 200/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 201/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 202/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 203/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 204/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 205/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 206/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 207/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 208/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 209/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 210/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 211/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 212/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 213/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 214/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 215/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 216/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 217/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 218/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 219/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 220/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 221/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 222/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 223/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 224/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 225/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 226/250\n","5450/5450 [==============================] - 2s 338us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 227/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 228/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 229/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 230/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 231/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 232/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 233/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 234/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 235/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 236/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 237/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 238/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 239/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 240/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 241/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 242/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 243/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 244/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 245/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 246/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 247/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 248/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 249/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 250/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9526 - val_loss: 4.9558\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 48541 0.5\n","The shape of N (6056, 784)\n","The minimum value of N  -0.7447847723960876\n","The max value of N 0.7492112517356873\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9744024554650207\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/250\n","5450/5450 [==============================] - 9s 2ms/step - loss: 5.0773 - val_loss: 5.1176\n","Epoch 2/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9997 - val_loss: 5.0091\n","Epoch 3/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9848 - val_loss: 4.9886\n","Epoch 4/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9779 - val_loss: 4.9819\n","Epoch 5/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9742 - val_loss: 4.9797\n","Epoch 6/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9718 - val_loss: 4.9788\n","Epoch 7/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9699 - val_loss: 4.9766\n","Epoch 8/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9687 - val_loss: 4.9752\n","Epoch 9/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9675 - val_loss: 4.9741\n","Epoch 10/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9668 - val_loss: 4.9770\n","Epoch 11/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9659 - val_loss: 4.9719\n","Epoch 12/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9653 - val_loss: 4.9735\n","Epoch 13/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9652 - val_loss: 4.9718\n","Epoch 14/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9642 - val_loss: 4.9712\n","Epoch 15/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9632 - val_loss: 4.9687\n","Epoch 16/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9629 - val_loss: 4.9680\n","Epoch 17/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9622 - val_loss: 4.9667\n","Epoch 18/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9612 - val_loss: 4.9695\n","Epoch 19/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9607 - val_loss: 4.9663\n","Epoch 20/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9599 - val_loss: 4.9666\n","Epoch 21/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9592 - val_loss: 4.9640\n","Epoch 22/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9586 - val_loss: 4.9646\n","Epoch 23/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9582 - val_loss: 4.9642\n","Epoch 24/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9580 - val_loss: 4.9639\n","Epoch 25/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9576 - val_loss: 4.9640\n","Epoch 26/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9575 - val_loss: 4.9623\n","Epoch 27/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9571 - val_loss: 4.9624\n","Epoch 28/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9569 - val_loss: 4.9611\n","Epoch 29/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9566 - val_loss: 4.9608\n","Epoch 30/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9566 - val_loss: 4.9605\n","Epoch 31/250\n","5450/5450 [==============================] - 2s 340us/step - loss: 4.9565 - val_loss: 4.9604\n","Epoch 32/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9561 - val_loss: 4.9601\n","Epoch 33/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9559 - val_loss: 4.9611\n","Epoch 34/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9558 - val_loss: 4.9588\n","Epoch 35/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9556 - val_loss: 4.9590\n","Epoch 36/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9556 - val_loss: 4.9593\n","Epoch 37/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9555 - val_loss: 4.9585\n","Epoch 38/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9556 - val_loss: 4.9594\n","Epoch 39/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9553 - val_loss: 4.9590\n","Epoch 40/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9553 - val_loss: 4.9591\n","Epoch 41/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9551 - val_loss: 4.9589\n","Epoch 42/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9551 - val_loss: 4.9588\n","Epoch 43/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9550 - val_loss: 4.9586\n","Epoch 44/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9549 - val_loss: 4.9585\n","Epoch 45/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 46/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 47/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 48/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 49/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9551 - val_loss: 4.9576\n","Epoch 50/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 51/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 52/250\n","5450/5450 [==============================] - 2s 332us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 53/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 54/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 55/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9558 - val_loss: 4.9644\n","Epoch 56/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9552 - val_loss: 4.9597\n","Epoch 57/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 58/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 59/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 60/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 61/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 62/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9567\n","Epoch 63/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9543 - val_loss: 4.9567\n","Epoch 64/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 65/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 66/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 67/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9543 - val_loss: 4.9567\n","Epoch 68/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 69/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 70/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9566\n","Epoch 71/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 72/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9566\n","Epoch 73/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 74/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 75/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 76/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 77/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 78/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 79/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 80/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 81/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 82/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 83/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 84/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 85/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 86/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 87/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 88/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 89/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 90/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 91/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 92/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 93/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 94/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 95/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 96/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 97/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 98/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 99/250\n","5450/5450 [==============================] - 2s 329us/step - loss: 4.9535 - val_loss: 4.9585\n","Epoch 100/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 101/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 102/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 103/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 104/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 105/250\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 106/250\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 107/250\n","5450/5450 [==============================] - 2s 330us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 108/250\n","5450/5450 [==============================] - 2s 339us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 109/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 110/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 111/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 112/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 113/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 114/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 115/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 116/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 117/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 118/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 119/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 120/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 121/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 122/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 123/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 124/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 125/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 126/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 127/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 128/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 129/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 130/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 131/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 132/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 133/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 134/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 135/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 136/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 137/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 138/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 139/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 140/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 141/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 142/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 143/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 144/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 145/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 146/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 147/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 148/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 149/250\n","5450/5450 [==============================] - 2s 329us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 150/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 151/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9529 - val_loss: 4.9588\n","Epoch 152/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 153/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 154/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 155/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 156/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 157/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 158/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 159/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 160/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 161/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 162/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 163/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 164/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 165/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 166/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 167/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 168/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 169/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 170/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 171/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 172/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 173/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 174/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 175/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 176/250\n","5450/5450 [==============================] - 2s 330us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 177/250\n","5450/5450 [==============================] - 2s 332us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 178/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 179/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 180/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 181/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 182/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 183/250\n","5450/5450 [==============================] - 2s 342us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 184/250\n","5450/5450 [==============================] - 2s 329us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 185/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 186/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 187/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 188/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 189/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 190/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9528 - val_loss: 4.9589\n","Epoch 191/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 192/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 193/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 194/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 195/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 196/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 197/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 198/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 199/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 200/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 201/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 202/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 203/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 204/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 205/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 206/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 207/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 208/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 209/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 210/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 211/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 212/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 213/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 214/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 215/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 216/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 217/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 218/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 219/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 220/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 221/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 222/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 223/250\n","5450/5450 [==============================] - 2s 330us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 224/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 225/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 226/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 227/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 228/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 229/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 230/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 231/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 232/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 233/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 234/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 235/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 236/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 237/250\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 238/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 239/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 240/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 241/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 242/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 243/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 244/250\n","5450/5450 [==============================] - 2s 329us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 245/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 246/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 247/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 248/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 249/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 250/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9525 - val_loss: 4.9566\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 53019 0.5\n","The shape of N (6056, 784)\n","The minimum value of N  -0.7290446162223816\n","The max value of N 0.7475768327713013\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9913120402008915\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/250\n","5450/5450 [==============================] - 9s 2ms/step - loss: 5.0686 - val_loss: 5.1311\n","Epoch 2/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 5.0008 - val_loss: 5.0122\n","Epoch 3/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9869 - val_loss: 4.9900\n","Epoch 4/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9795 - val_loss: 4.9808\n","Epoch 5/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9754 - val_loss: 4.9780\n","Epoch 6/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9723 - val_loss: 4.9746\n","Epoch 7/250\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9701 - val_loss: 4.9733\n","Epoch 8/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9687 - val_loss: 4.9728\n","Epoch 9/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9676 - val_loss: 4.9714\n","Epoch 10/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9660 - val_loss: 4.9698\n","Epoch 11/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9647 - val_loss: 4.9691\n","Epoch 12/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9639 - val_loss: 4.9689\n","Epoch 13/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9630 - val_loss: 4.9681\n","Epoch 14/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9620 - val_loss: 4.9682\n","Epoch 15/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9614 - val_loss: 4.9672\n","Epoch 16/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9608 - val_loss: 4.9650\n","Epoch 17/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9602 - val_loss: 4.9655\n","Epoch 18/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9596 - val_loss: 4.9651\n","Epoch 19/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9592 - val_loss: 4.9642\n","Epoch 20/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9586 - val_loss: 4.9638\n","Epoch 21/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9584 - val_loss: 4.9634\n","Epoch 22/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9579 - val_loss: 4.9636\n","Epoch 23/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9578 - val_loss: 4.9639\n","Epoch 24/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9577 - val_loss: 4.9619\n","Epoch 25/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9575 - val_loss: 4.9621\n","Epoch 26/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9570 - val_loss: 4.9614\n","Epoch 27/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9576 - val_loss: 4.9632\n","Epoch 28/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9570 - val_loss: 4.9607\n","Epoch 29/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9567 - val_loss: 4.9599\n","Epoch 30/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9567 - val_loss: 4.9608\n","Epoch 31/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9564 - val_loss: 4.9607\n","Epoch 32/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9563 - val_loss: 4.9599\n","Epoch 33/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9562 - val_loss: 4.9601\n","Epoch 34/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9562 - val_loss: 4.9592\n","Epoch 35/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9561 - val_loss: 4.9593\n","Epoch 36/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9560 - val_loss: 4.9594\n","Epoch 37/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9562 - val_loss: 4.9622\n","Epoch 38/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9558 - val_loss: 4.9597\n","Epoch 39/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9558 - val_loss: 4.9590\n","Epoch 40/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9557 - val_loss: 4.9585\n","Epoch 41/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9556 - val_loss: 4.9584\n","Epoch 42/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 43/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9554 - val_loss: 4.9582\n","Epoch 44/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9554 - val_loss: 4.9584\n","Epoch 45/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 46/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9553 - val_loss: 4.9585\n","Epoch 47/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 48/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9551 - val_loss: 4.9580\n","Epoch 49/250\n","5450/5450 [==============================] - 2s 330us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 50/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9550 - val_loss: 4.9584\n","Epoch 51/250\n","5450/5450 [==============================] - 2s 331us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 52/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 53/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 54/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 55/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9548 - val_loss: 4.9584\n","Epoch 56/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 57/250\n","5450/5450 [==============================] - 2s 329us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 58/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 59/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9551 - val_loss: 4.9595\n","Epoch 60/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 61/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 62/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9559 - val_loss: 4.9603\n","Epoch 63/250\n","5450/5450 [==============================] - 2s 332us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 64/250\n","5450/5450 [==============================] - 2s 345us/step - loss: 4.9547 - val_loss: 4.9575\n","Epoch 65/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 66/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 67/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9544 - val_loss: 4.9568\n","Epoch 68/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 69/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 70/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 71/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 72/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 73/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 74/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 75/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 76/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 77/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 78/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 79/250\n","5450/5450 [==============================] - 2s 331us/step - loss: 4.9542 - val_loss: 4.9590\n","Epoch 80/250\n","5450/5450 [==============================] - 2s 330us/step - loss: 4.9601 - val_loss: 4.9976\n","Epoch 81/250\n","5450/5450 [==============================] - 2s 329us/step - loss: 4.9577 - val_loss: 4.9787\n","Epoch 82/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9565 - val_loss: 4.9646\n","Epoch 83/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9560 - val_loss: 4.9613\n","Epoch 84/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9556 - val_loss: 4.9592\n","Epoch 85/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9558 - val_loss: 4.9589\n","Epoch 86/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9554 - val_loss: 4.9579\n","Epoch 87/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 88/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9551 - val_loss: 4.9576\n","Epoch 89/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9549 - val_loss: 4.9575\n","Epoch 90/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9574\n","Epoch 91/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9573\n","Epoch 92/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 93/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 94/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9547 - val_loss: 4.9573\n","Epoch 95/250\n","5450/5450 [==============================] - 2s 321us/step - loss: 4.9550 - val_loss: 4.9574\n","Epoch 96/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 97/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 98/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9546 - val_loss: 4.9573\n","Epoch 99/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 100/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 101/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 102/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 103/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 104/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 105/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 106/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 107/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 108/250\n","5450/5450 [==============================] - 2s 329us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 109/250\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 110/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 111/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 112/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 113/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 114/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 115/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 116/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 117/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 118/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 119/250\n","5450/5450 [==============================] - 2s 330us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 120/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 121/250\n","5450/5450 [==============================] - 2s 320us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 122/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 123/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 124/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 125/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 126/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 127/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 128/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 129/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 130/250\n","5450/5450 [==============================] - 2s 330us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 131/250\n","5450/5450 [==============================] - 2s 329us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 132/250\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 133/250\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 134/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 135/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 136/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 137/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 138/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 139/250\n","5450/5450 [==============================] - 2s 330us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 140/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 141/250\n","5450/5450 [==============================] - 2s 329us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 142/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 143/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 144/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 145/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 146/250\n","5450/5450 [==============================] - 2s 331us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 147/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 148/250\n","5450/5450 [==============================] - 2s 329us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 149/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 150/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 151/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 152/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 153/250\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 154/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 155/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 156/250\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 157/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 158/250\n","5450/5450 [==============================] - 2s 330us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 159/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 160/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 161/250\n","5450/5450 [==============================] - 2s 319us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 162/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 163/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 164/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 165/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 166/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 167/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 168/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 169/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 170/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 171/250\n","5450/5450 [==============================] - 2s 329us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 172/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 173/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 174/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 175/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 176/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 177/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 178/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 179/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 180/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 181/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 182/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 183/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 184/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 185/250\n","5450/5450 [==============================] - 2s 335us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 186/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 187/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 188/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 189/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 190/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 191/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 192/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 193/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 194/250\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 195/250\n","5450/5450 [==============================] - 2s 323us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 196/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 197/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 198/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 199/250\n","5450/5450 [==============================] - 2s 322us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 200/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 201/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 202/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 203/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 204/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 205/250\n","5450/5450 [==============================] - 2s 329us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 206/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 207/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 208/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 209/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 210/250\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 211/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 212/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 213/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 214/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 215/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 216/250\n","5450/5450 [==============================] - 2s 331us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 217/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 218/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 219/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 220/250\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 221/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 222/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 223/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 224/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 225/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 226/250\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 227/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 228/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 229/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 230/250\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 231/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 232/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 233/250\n","5450/5450 [==============================] - 2s 312us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 234/250\n","5450/5450 [==============================] - 2s 325us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 235/250\n","5450/5450 [==============================] - 2s 326us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 236/250\n","5450/5450 [==============================] - 2s 324us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 237/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 238/250\n","5450/5450 [==============================] - 2s 318us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 239/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 240/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 241/250\n","5450/5450 [==============================] - 2s 328us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 242/250\n","5450/5450 [==============================] - 2s 313us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 243/250\n","5450/5450 [==============================] - 2s 316us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 244/250\n","5450/5450 [==============================] - 2s 327us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 245/250\n","5450/5450 [==============================] - 2s 315us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 246/250\n","5450/5450 [==============================] - 2s 314us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 247/250\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 248/250\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 249/250\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 250/250\n","5450/5450 [==============================] - 2s 317us/step - loss: 4.9530 - val_loss: 4.9568\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 46088 0.5\n","The shape of N (6056, 784)\n","The minimum value of N  -0.7413690686225891\n","The max value of N 0.7497393488883972\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9887655692252906\n","=======================\n","Saved model to disk....\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.987883772394672, 0.9861343100928994, 0.9704456748148086, 0.9886609971652492, 0.9917190233534847, 0.9543387512965523, 0.9678596360327055, 0.9744024554650207, 0.9913120402008915, 0.9887655692252906]\n","AUROC ===== 0.9801522230041574 +/- 0.012021661713369122\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n","text/plain":["<matplotlib.figure.Figure at 0x7f28983e4cc0>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"2tXWQZ9FJiNw","colab_type":"text"},"cell_type":"markdown","source":["# **RCAE-MNIST 1_Vs_all**"]},{"metadata":{"id":"_X9HA3E5JeIZ","colab_type":"code","outputId":"d1a863fc-7cf6-4d6a-8afd-ce7d6bd5cebb","executionInfo":{"status":"ok","timestamp":1541292671890,"user_tz":-660,"elapsed":207640,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5677, 28, 28, 1)\n","Train Label Shape:  (5677,)\n","Validation Data Shape:  (1134, 28, 28, 1)\n","Validation Label Shape:  (1134,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (67, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (67, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 6138 samples, validate on 683 samples\n","Epoch 1/250\n","6138/6138 [==============================] - 5s 775us/step - loss: 5.0519 - val_loss: 5.7709\n","Epoch 2/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9837 - val_loss: 5.1468\n","Epoch 3/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9779 - val_loss: 5.1170\n","Epoch 4/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9680 - val_loss: 5.0120\n","Epoch 5/250\n","6138/6138 [==============================] - 2s 290us/step - loss: 4.9635 - val_loss: 4.9898\n","Epoch 6/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9598 - val_loss: 4.9843\n","Epoch 7/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9620 - val_loss: 4.9882\n","Epoch 8/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9600 - val_loss: 4.9880\n","Epoch 9/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9593 - val_loss: 4.9757\n","Epoch 10/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9572 - val_loss: 4.9689\n","Epoch 11/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9566 - val_loss: 4.9693\n","Epoch 12/250\n","6138/6138 [==============================] - 2s 290us/step - loss: 4.9557 - val_loss: 4.9661\n","Epoch 13/250\n","6138/6138 [==============================] - 2s 299us/step - loss: 4.9557 - val_loss: 4.9655\n","Epoch 14/250\n","6138/6138 [==============================] - 2s 297us/step - loss: 4.9554 - val_loss: 4.9643\n","Epoch 15/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9549 - val_loss: 4.9638\n","Epoch 16/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9544 - val_loss: 4.9621\n","Epoch 17/250\n","6138/6138 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9619\n","Epoch 18/250\n","6138/6138 [==============================] - 2s 326us/step - loss: 4.9539 - val_loss: 4.9611\n","Epoch 19/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9536 - val_loss: 4.9608\n","Epoch 20/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9535 - val_loss: 4.9612\n","Epoch 21/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9604\n","Epoch 22/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9607\n","Epoch 23/250\n","6138/6138 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9606\n","Epoch 24/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9531 - val_loss: 4.9603\n","Epoch 25/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9533 - val_loss: 4.9607\n","Epoch 26/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9534 - val_loss: 4.9675\n","Epoch 27/250\n","6138/6138 [==============================] - 2s 327us/step - loss: 4.9537 - val_loss: 4.9618\n","Epoch 28/250\n","6138/6138 [==============================] - 2s 342us/step - loss: 4.9532 - val_loss: 4.9607\n","Epoch 29/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9530 - val_loss: 4.9606\n","Epoch 30/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9609\n","Epoch 31/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9530 - val_loss: 4.9628\n","Epoch 32/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9528 - val_loss: 4.9608\n","Epoch 33/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9601\n","Epoch 34/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9527 - val_loss: 4.9598\n","Epoch 35/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9529 - val_loss: 4.9610\n","Epoch 36/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9600\n","Epoch 37/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9528 - val_loss: 4.9597\n","Epoch 38/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9526 - val_loss: 4.9598\n","Epoch 39/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9525 - val_loss: 4.9593\n","Epoch 40/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9524 - val_loss: 4.9592\n","Epoch 41/250\n","6138/6138 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9587\n","Epoch 42/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9524 - val_loss: 4.9589\n","Epoch 43/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9523 - val_loss: 4.9586\n","Epoch 44/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9523 - val_loss: 4.9588\n","Epoch 45/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9523 - val_loss: 4.9587\n","Epoch 46/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9523 - val_loss: 4.9586\n","Epoch 47/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9522 - val_loss: 4.9585\n","Epoch 48/250\n","6138/6138 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9591\n","Epoch 49/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9523 - val_loss: 4.9591\n","Epoch 50/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9522 - val_loss: 4.9584\n","Epoch 51/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9521 - val_loss: 4.9585\n","Epoch 52/250\n","6138/6138 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9595\n","Epoch 53/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9524 - val_loss: 4.9588\n","Epoch 54/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9522 - val_loss: 4.9588\n","Epoch 55/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9521 - val_loss: 4.9582\n","Epoch 56/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9520 - val_loss: 4.9579\n","Epoch 57/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9520 - val_loss: 4.9578\n","Epoch 58/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9519 - val_loss: 4.9578\n","Epoch 59/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9519 - val_loss: 4.9578\n","Epoch 60/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9579\n","Epoch 61/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9519 - val_loss: 4.9575\n","Epoch 62/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9519 - val_loss: 4.9578\n","Epoch 63/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9518 - val_loss: 4.9576\n","Epoch 64/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9520 - val_loss: 4.9578\n","Epoch 65/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9519 - val_loss: 4.9576\n","Epoch 66/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9519 - val_loss: 4.9574\n","Epoch 67/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9518 - val_loss: 4.9574\n","Epoch 68/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9517 - val_loss: 4.9575\n","Epoch 69/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9518 - val_loss: 4.9574\n","Epoch 70/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9517 - val_loss: 4.9575\n","Epoch 71/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9518 - val_loss: 4.9578\n","Epoch 72/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9518 - val_loss: 4.9573\n","Epoch 73/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9516 - val_loss: 4.9570\n","Epoch 74/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9517 - val_loss: 4.9572\n","Epoch 75/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9517 - val_loss: 4.9572\n","Epoch 76/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9571\n","Epoch 77/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9516 - val_loss: 4.9571\n","Epoch 78/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9516 - val_loss: 4.9570\n","Epoch 79/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9518 - val_loss: 4.9572\n","Epoch 80/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9516 - val_loss: 4.9570\n","Epoch 81/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9515 - val_loss: 4.9567\n","Epoch 82/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9515 - val_loss: 4.9570\n","Epoch 83/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9517 - val_loss: 4.9574\n","Epoch 84/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9516 - val_loss: 4.9570\n","Epoch 85/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9515 - val_loss: 4.9568\n","Epoch 86/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9515 - val_loss: 4.9569\n","Epoch 87/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9515 - val_loss: 4.9566\n","Epoch 88/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9516 - val_loss: 4.9569\n","Epoch 89/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9516 - val_loss: 4.9606\n","Epoch 90/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9515 - val_loss: 4.9575\n","Epoch 91/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9515 - val_loss: 4.9572\n","Epoch 92/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9515 - val_loss: 4.9579\n","Epoch 93/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9515 - val_loss: 4.9578\n","Epoch 94/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9515 - val_loss: 4.9571\n","Epoch 95/250\n","6138/6138 [==============================] - 2s 333us/step - loss: 4.9516 - val_loss: 4.9570\n","Epoch 96/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9514 - val_loss: 4.9570\n","Epoch 97/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9515 - val_loss: 4.9567\n","Epoch 98/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9514 - val_loss: 4.9568\n","Epoch 99/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9514 - val_loss: 4.9567\n","Epoch 100/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9514 - val_loss: 4.9567\n","Epoch 101/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9513 - val_loss: 4.9564\n","Epoch 102/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9513 - val_loss: 4.9567\n","Epoch 103/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9513 - val_loss: 4.9564\n","Epoch 104/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9513 - val_loss: 4.9564\n","Epoch 105/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9513 - val_loss: 4.9566\n","Epoch 106/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9513 - val_loss: 4.9563\n","Epoch 107/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9513 - val_loss: 4.9564\n","Epoch 108/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9513 - val_loss: 4.9563\n","Epoch 109/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9513 - val_loss: 4.9564\n","Epoch 110/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9512 - val_loss: 4.9564\n","Epoch 111/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9513 - val_loss: 4.9566\n","Epoch 112/250\n","6138/6138 [==============================] - 2s 290us/step - loss: 4.9513 - val_loss: 4.9563\n","Epoch 113/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9512 - val_loss: 4.9564\n","Epoch 114/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9512 - val_loss: 4.9562\n","Epoch 115/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9512 - val_loss: 4.9563\n","Epoch 116/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9512 - val_loss: 4.9563\n","Epoch 117/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9512 - val_loss: 4.9563\n","Epoch 118/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9512 - val_loss: 4.9563\n","Epoch 119/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9511 - val_loss: 4.9562\n","Epoch 120/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9511 - val_loss: 4.9562\n","Epoch 121/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9511 - val_loss: 4.9563\n","Epoch 122/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9512 - val_loss: 4.9565\n","Epoch 123/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9512 - val_loss: 4.9563\n","Epoch 124/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9511 - val_loss: 4.9561\n","Epoch 125/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9511 - val_loss: 4.9562\n","Epoch 126/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9511 - val_loss: 4.9560\n","Epoch 127/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9511 - val_loss: 4.9561\n","Epoch 128/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9511 - val_loss: 4.9561\n","Epoch 129/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9510 - val_loss: 4.9560\n","Epoch 130/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9510 - val_loss: 4.9559\n","Epoch 131/250\n","6138/6138 [==============================] - 2s 292us/step - loss: 4.9510 - val_loss: 4.9570\n","Epoch 132/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9521 - val_loss: 4.9672\n","Epoch 133/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9518 - val_loss: 4.9605\n","Epoch 134/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9516 - val_loss: 4.9582\n","Epoch 135/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9515 - val_loss: 4.9577\n","Epoch 136/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9514 - val_loss: 4.9573\n","Epoch 137/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9513 - val_loss: 4.9578\n","Epoch 138/250\n","6138/6138 [==============================] - 2s 301us/step - loss: 4.9513 - val_loss: 4.9568\n","Epoch 139/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9512 - val_loss: 4.9565\n","Epoch 140/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9512 - val_loss: 4.9563\n","Epoch 141/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9512 - val_loss: 4.9562\n","Epoch 142/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9512 - val_loss: 4.9562\n","Epoch 143/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9512 - val_loss: 4.9562\n","Epoch 144/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9511 - val_loss: 4.9565\n","Epoch 145/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9512 - val_loss: 4.9561\n","Epoch 146/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9511 - val_loss: 4.9563\n","Epoch 147/250\n","6138/6138 [==============================] - 2s 291us/step - loss: 4.9511 - val_loss: 4.9561\n","Epoch 148/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9511 - val_loss: 4.9561\n","Epoch 149/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9511 - val_loss: 4.9560\n","Epoch 150/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9511 - val_loss: 4.9562\n","Epoch 151/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9510 - val_loss: 4.9558\n","Epoch 152/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9510 - val_loss: 4.9561\n","Epoch 153/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9510 - val_loss: 4.9561\n","Epoch 154/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9510 - val_loss: 4.9559\n","Epoch 155/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9510 - val_loss: 4.9559\n","Epoch 156/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9510 - val_loss: 4.9559\n","Epoch 157/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9510 - val_loss: 4.9559\n","Epoch 158/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9510 - val_loss: 4.9557\n","Epoch 159/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9510 - val_loss: 4.9558\n","Epoch 160/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9510 - val_loss: 4.9559\n","Epoch 161/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9510 - val_loss: 4.9559\n","Epoch 162/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9509 - val_loss: 4.9558\n","Epoch 163/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9509 - val_loss: 4.9558\n","Epoch 164/250\n","6138/6138 [==============================] - 2s 327us/step - loss: 4.9509 - val_loss: 4.9557\n","Epoch 165/250\n","6138/6138 [==============================] - 2s 328us/step - loss: 4.9509 - val_loss: 4.9557\n","Epoch 166/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9510 - val_loss: 4.9558\n","Epoch 167/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9509 - val_loss: 4.9558\n","Epoch 168/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9509 - val_loss: 4.9556\n","Epoch 169/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9509 - val_loss: 4.9557\n","Epoch 170/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9510 - val_loss: 4.9557\n","Epoch 171/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9509 - val_loss: 4.9558\n","Epoch 172/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9509 - val_loss: 4.9557\n","Epoch 173/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9509 - val_loss: 4.9558\n","Epoch 174/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9509 - val_loss: 4.9559\n","Epoch 175/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9509 - val_loss: 4.9557\n","Epoch 176/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9509 - val_loss: 4.9556\n","Epoch 177/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9509 - val_loss: 4.9558\n","Epoch 178/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9510 - val_loss: 4.9557\n","Epoch 179/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9509 - val_loss: 4.9556\n","Epoch 180/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9509 - val_loss: 4.9557\n","Epoch 181/250\n","6138/6138 [==============================] - 2s 294us/step - loss: 4.9509 - val_loss: 4.9557\n","Epoch 182/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9508 - val_loss: 4.9557\n","Epoch 183/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9509 - val_loss: 4.9556\n","Epoch 184/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9509 - val_loss: 4.9555\n","Epoch 185/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9508 - val_loss: 4.9557\n","Epoch 186/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9508 - val_loss: 4.9556\n","Epoch 187/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9508 - val_loss: 4.9556\n","Epoch 188/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9508 - val_loss: 4.9556\n","Epoch 189/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9508 - val_loss: 4.9556\n","Epoch 190/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9508 - val_loss: 4.9557\n","Epoch 191/250\n","6138/6138 [==============================] - 2s 288us/step - loss: 4.9508 - val_loss: 4.9555\n","Epoch 192/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9508 - val_loss: 4.9556\n","Epoch 193/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9508 - val_loss: 4.9557\n","Epoch 194/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9508 - val_loss: 4.9556\n","Epoch 195/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9508 - val_loss: 4.9556\n","Epoch 196/250\n","6138/6138 [==============================] - 2s 301us/step - loss: 4.9508 - val_loss: 4.9557\n","Epoch 197/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9509 - val_loss: 4.9561\n","Epoch 198/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9508 - val_loss: 4.9556\n","Epoch 199/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9508 - val_loss: 4.9555\n","Epoch 200/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9508 - val_loss: 4.9556\n","Epoch 201/250\n","6138/6138 [==============================] - 2s 291us/step - loss: 4.9507 - val_loss: 4.9556\n","Epoch 202/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9508 - val_loss: 4.9555\n","Epoch 203/250\n","6138/6138 [==============================] - 2s 293us/step - loss: 4.9507 - val_loss: 4.9556\n","Epoch 204/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9507 - val_loss: 4.9555\n","Epoch 205/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9507 - val_loss: 4.9555\n","Epoch 206/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9507 - val_loss: 4.9556\n","Epoch 207/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9507 - val_loss: 4.9555\n","Epoch 208/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9507 - val_loss: 4.9555\n","Epoch 209/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9507 - val_loss: 4.9556\n","Epoch 210/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9508 - val_loss: 4.9556\n","Epoch 211/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9507 - val_loss: 4.9555\n","Epoch 212/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9507 - val_loss: 4.9555\n","Epoch 213/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9507 - val_loss: 4.9557\n","Epoch 214/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9507 - val_loss: 4.9556\n","Epoch 215/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9507 - val_loss: 4.9557\n","Epoch 216/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9507 - val_loss: 4.9557\n","Epoch 217/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9507 - val_loss: 4.9558\n","Epoch 218/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9507 - val_loss: 4.9557\n","Epoch 219/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9507 - val_loss: 4.9557\n","Epoch 220/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9507 - val_loss: 4.9557\n","Epoch 221/250\n","6138/6138 [==============================] - 2s 292us/step - loss: 4.9507 - val_loss: 4.9556\n","Epoch 222/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9507 - val_loss: 4.9556\n","Epoch 223/250\n","6138/6138 [==============================] - 2s 299us/step - loss: 4.9507 - val_loss: 4.9556\n","Epoch 224/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9507 - val_loss: 4.9555\n","Epoch 225/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9506 - val_loss: 4.9557\n","Epoch 226/250\n","6138/6138 [==============================] - 2s 301us/step - loss: 4.9507 - val_loss: 4.9557\n","Epoch 227/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9507 - val_loss: 4.9557\n","Epoch 228/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9506 - val_loss: 4.9556\n","Epoch 229/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9507 - val_loss: 4.9556\n","Epoch 230/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9507 - val_loss: 4.9557\n","Epoch 231/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9507 - val_loss: 4.9556\n","Epoch 232/250\n","6138/6138 [==============================] - 2s 332us/step - loss: 4.9507 - val_loss: 4.9556\n","Epoch 233/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9506 - val_loss: 4.9556\n","Epoch 234/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9507 - val_loss: 4.9555\n","Epoch 235/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9506 - val_loss: 4.9555\n","Epoch 236/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9506 - val_loss: 4.9555\n","Epoch 237/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9506 - val_loss: 4.9556\n","Epoch 238/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9506 - val_loss: 4.9555\n","Epoch 239/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9506 - val_loss: 4.9555\n","Epoch 240/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9507 - val_loss: 4.9557\n","Epoch 241/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9507 - val_loss: 4.9556\n","Epoch 242/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9507 - val_loss: 4.9555\n","Epoch 243/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9506 - val_loss: 4.9555\n","Epoch 244/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9506 - val_loss: 4.9557\n","Epoch 245/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9506 - val_loss: 4.9555\n","Epoch 246/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9506 - val_loss: 4.9557\n","Epoch 247/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9506 - val_loss: 4.9555\n","Epoch 248/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9506 - val_loss: 4.9556\n","Epoch 249/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9507 - val_loss: 4.9556\n","Epoch 250/250\n","6138/6138 [==============================] - 2s 298us/step - loss: 4.9506 - val_loss: 4.9556\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6821, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 21741 0.5\n","The shape of N (6821, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9993215739484396\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5677, 28, 28, 1)\n","Train Label Shape:  (5677,)\n","Validation Data Shape:  (1134, 28, 28, 1)\n","Validation Label Shape:  (1134,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (67, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (67, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 6138 samples, validate on 683 samples\n","Epoch 1/250\n","6138/6138 [==============================] - 4s 682us/step - loss: 5.0622 - val_loss: 5.7605\n","Epoch 2/250\n","6138/6138 [==============================] - 2s 290us/step - loss: 4.9921 - val_loss: 5.2763\n","Epoch 3/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9785 - val_loss: 5.2451\n","Epoch 4/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9723 - val_loss: 5.0060\n","Epoch 5/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9673 - val_loss: 4.9862\n","Epoch 6/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9623 - val_loss: 4.9874\n","Epoch 7/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9630 - val_loss: 4.9940\n","Epoch 8/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9595 - val_loss: 4.9773\n","Epoch 9/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9576 - val_loss: 4.9721\n","Epoch 10/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9563 - val_loss: 4.9689\n","Epoch 11/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9558 - val_loss: 4.9674\n","Epoch 12/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9555 - val_loss: 4.9663\n","Epoch 13/250\n","6138/6138 [==============================] - 2s 292us/step - loss: 4.9551 - val_loss: 4.9658\n","Epoch 14/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9557 - val_loss: 4.9674\n","Epoch 15/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9552 - val_loss: 4.9663\n","Epoch 16/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9549 - val_loss: 4.9644\n","Epoch 17/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9546 - val_loss: 4.9657\n","Epoch 18/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9548 - val_loss: 4.9645\n","Epoch 19/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9545 - val_loss: 4.9646\n","Epoch 20/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9542 - val_loss: 4.9635\n","Epoch 21/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9540 - val_loss: 4.9626\n","Epoch 22/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9541 - val_loss: 4.9627\n","Epoch 23/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9546 - val_loss: 4.9958\n","Epoch 24/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9543 - val_loss: 4.9642\n","Epoch 25/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9539 - val_loss: 4.9627\n","Epoch 26/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9541 - val_loss: 4.9625\n","Epoch 27/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9537 - val_loss: 4.9624\n","Epoch 28/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9536 - val_loss: 4.9614\n","Epoch 29/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9608\n","Epoch 30/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9534 - val_loss: 4.9626\n","Epoch 31/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9616\n","Epoch 32/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9535 - val_loss: 4.9617\n","Epoch 33/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9532 - val_loss: 4.9608\n","Epoch 34/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9532 - val_loss: 4.9608\n","Epoch 35/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9532 - val_loss: 4.9603\n","Epoch 36/250\n","6138/6138 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9604\n","Epoch 37/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9530 - val_loss: 4.9598\n","Epoch 38/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9529 - val_loss: 4.9594\n","Epoch 39/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9596\n","Epoch 40/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9600\n","Epoch 41/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9530 - val_loss: 4.9599\n","Epoch 42/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9529 - val_loss: 4.9597\n","Epoch 43/250\n","6138/6138 [==============================] - 2s 329us/step - loss: 4.9528 - val_loss: 4.9594\n","Epoch 44/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9527 - val_loss: 4.9596\n","Epoch 45/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9526 - val_loss: 4.9598\n","Epoch 46/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9527 - val_loss: 4.9593\n","Epoch 47/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9525 - val_loss: 4.9593\n","Epoch 48/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9526 - val_loss: 4.9592\n","Epoch 49/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9526 - val_loss: 4.9591\n","Epoch 50/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9526 - val_loss: 4.9589\n","Epoch 51/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9525 - val_loss: 4.9595\n","Epoch 52/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9526 - val_loss: 4.9591\n","Epoch 53/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9525 - val_loss: 4.9595\n","Epoch 54/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9525 - val_loss: 4.9590\n","Epoch 55/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9524 - val_loss: 4.9589\n","Epoch 56/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9524 - val_loss: 4.9592\n","Epoch 57/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9523 - val_loss: 4.9591\n","Epoch 58/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9523 - val_loss: 4.9588\n","Epoch 59/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9523 - val_loss: 4.9586\n","Epoch 60/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9522 - val_loss: 4.9588\n","Epoch 61/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9523 - val_loss: 4.9588\n","Epoch 62/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9522 - val_loss: 4.9586\n","Epoch 63/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9522 - val_loss: 4.9584\n","Epoch 64/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9522 - val_loss: 4.9583\n","Epoch 65/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9522 - val_loss: 4.9583\n","Epoch 66/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9522 - val_loss: 4.9581\n","Epoch 67/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9522 - val_loss: 4.9584\n","Epoch 68/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9521 - val_loss: 4.9582\n","Epoch 69/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9522 - val_loss: 4.9586\n","Epoch 70/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9520 - val_loss: 4.9580\n","Epoch 71/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9521 - val_loss: 4.9583\n","Epoch 72/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9522 - val_loss: 4.9582\n","Epoch 73/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9520 - val_loss: 4.9582\n","Epoch 74/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9520 - val_loss: 4.9583\n","Epoch 75/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9520 - val_loss: 4.9580\n","Epoch 76/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9521 - val_loss: 4.9582\n","Epoch 77/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9520 - val_loss: 4.9581\n","Epoch 78/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9520 - val_loss: 4.9582\n","Epoch 79/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9521 - val_loss: 4.9580\n","Epoch 80/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9521 - val_loss: 4.9580\n","Epoch 81/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9520 - val_loss: 4.9581\n","Epoch 82/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9520 - val_loss: 4.9579\n","Epoch 83/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9520 - val_loss: 4.9579\n","Epoch 84/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9519 - val_loss: 4.9580\n","Epoch 85/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9521 - val_loss: 4.9581\n","Epoch 86/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9519 - val_loss: 4.9580\n","Epoch 87/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9519 - val_loss: 4.9581\n","Epoch 88/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9519 - val_loss: 4.9582\n","Epoch 89/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9519 - val_loss: 4.9579\n","Epoch 90/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9519 - val_loss: 4.9581\n","Epoch 91/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9580\n","Epoch 92/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9518 - val_loss: 4.9582\n","Epoch 93/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9519 - val_loss: 4.9583\n","Epoch 94/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9519 - val_loss: 4.9580\n","Epoch 95/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9518 - val_loss: 4.9577\n","Epoch 96/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9518 - val_loss: 4.9579\n","Epoch 97/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9518 - val_loss: 4.9578\n","Epoch 98/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9518 - val_loss: 4.9577\n","Epoch 99/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9517 - val_loss: 4.9577\n","Epoch 100/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9576\n","Epoch 101/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9518 - val_loss: 4.9576\n","Epoch 102/250\n","6138/6138 [==============================] - 2s 290us/step - loss: 4.9518 - val_loss: 4.9577\n","Epoch 103/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9518 - val_loss: 4.9577\n","Epoch 104/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9517 - val_loss: 4.9575\n","Epoch 105/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9517 - val_loss: 4.9577\n","Epoch 106/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9518 - val_loss: 4.9577\n","Epoch 107/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9518 - val_loss: 4.9576\n","Epoch 108/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9518 - val_loss: 4.9578\n","Epoch 109/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9518 - val_loss: 4.9579\n","Epoch 110/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9518 - val_loss: 4.9578\n","Epoch 111/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9517 - val_loss: 4.9577\n","Epoch 112/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9518 - val_loss: 4.9576\n","Epoch 113/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9517 - val_loss: 4.9576\n","Epoch 114/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9517 - val_loss: 4.9578\n","Epoch 115/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9517 - val_loss: 4.9576\n","Epoch 116/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9517 - val_loss: 4.9575\n","Epoch 117/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9517 - val_loss: 4.9576\n","Epoch 118/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9516 - val_loss: 4.9577\n","Epoch 119/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9517 - val_loss: 4.9575\n","Epoch 120/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9518 - val_loss: 4.9577\n","Epoch 121/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9518 - val_loss: 4.9577\n","Epoch 122/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9517 - val_loss: 4.9577\n","Epoch 123/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9517 - val_loss: 4.9576\n","Epoch 124/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9517 - val_loss: 4.9578\n","Epoch 125/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9517 - val_loss: 4.9574\n","Epoch 126/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9516 - val_loss: 4.9578\n","Epoch 127/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9516 - val_loss: 4.9578\n","Epoch 128/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9516 - val_loss: 4.9577\n","Epoch 129/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9516 - val_loss: 4.9575\n","Epoch 130/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9516 - val_loss: 4.9575\n","Epoch 131/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9516 - val_loss: 4.9575\n","Epoch 132/250\n","6138/6138 [==============================] - 2s 291us/step - loss: 4.9516 - val_loss: 4.9576\n","Epoch 133/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9516 - val_loss: 4.9574\n","Epoch 134/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9516 - val_loss: 4.9574\n","Epoch 135/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9516 - val_loss: 4.9573\n","Epoch 136/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9515 - val_loss: 4.9573\n","Epoch 137/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9516 - val_loss: 4.9575\n","Epoch 138/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9516 - val_loss: 4.9575\n","Epoch 139/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9516 - val_loss: 4.9577\n","Epoch 140/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9516 - val_loss: 4.9575\n","Epoch 141/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9515 - val_loss: 4.9576\n","Epoch 142/250\n","6138/6138 [==============================] - 2s 294us/step - loss: 4.9515 - val_loss: 4.9575\n","Epoch 143/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9516 - val_loss: 4.9580\n","Epoch 144/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9575\n","Epoch 145/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9515 - val_loss: 4.9573\n","Epoch 146/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9515 - val_loss: 4.9575\n","Epoch 147/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9515 - val_loss: 4.9574\n","Epoch 148/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9515 - val_loss: 4.9574\n","Epoch 149/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9516 - val_loss: 4.9575\n","Epoch 150/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9515 - val_loss: 4.9573\n","Epoch 151/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9515 - val_loss: 4.9575\n","Epoch 152/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9516 - val_loss: 4.9575\n","Epoch 153/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9515 - val_loss: 4.9574\n","Epoch 154/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9515 - val_loss: 4.9574\n","Epoch 155/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9515 - val_loss: 4.9573\n","Epoch 156/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9515 - val_loss: 4.9574\n","Epoch 157/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9514 - val_loss: 4.9576\n","Epoch 158/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9514 - val_loss: 4.9572\n","Epoch 159/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9514 - val_loss: 4.9572\n","Epoch 160/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9514 - val_loss: 4.9572\n","Epoch 161/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9514 - val_loss: 4.9573\n","Epoch 162/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9514 - val_loss: 4.9573\n","Epoch 163/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9514 - val_loss: 4.9574\n","Epoch 164/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9514 - val_loss: 4.9574\n","Epoch 165/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9515 - val_loss: 4.9575\n","Epoch 166/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9514 - val_loss: 4.9573\n","Epoch 167/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9514 - val_loss: 4.9572\n","Epoch 168/250\n","6138/6138 [==============================] - 2s 293us/step - loss: 4.9514 - val_loss: 4.9572\n","Epoch 169/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9514 - val_loss: 4.9573\n","Epoch 170/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9514 - val_loss: 4.9573\n","Epoch 171/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9514 - val_loss: 4.9573\n","Epoch 172/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9514 - val_loss: 4.9572\n","Epoch 173/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9514 - val_loss: 4.9573\n","Epoch 174/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9521 - val_loss: 4.9856\n","Epoch 175/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9624\n","Epoch 176/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9522 - val_loss: 4.9588\n","Epoch 177/250\n","6138/6138 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9578\n","Epoch 178/250\n","6138/6138 [==============================] - 2s 327us/step - loss: 4.9518 - val_loss: 4.9575\n","Epoch 179/250\n","6138/6138 [==============================] - 2s 325us/step - loss: 4.9517 - val_loss: 4.9572\n","Epoch 180/250\n","6138/6138 [==============================] - 2s 333us/step - loss: 4.9517 - val_loss: 4.9570\n","Epoch 181/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9516 - val_loss: 4.9569\n","Epoch 182/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9515 - val_loss: 4.9568\n","Epoch 183/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9515 - val_loss: 4.9567\n","Epoch 184/250\n","6138/6138 [==============================] - 2s 301us/step - loss: 4.9515 - val_loss: 4.9568\n","Epoch 185/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9515 - val_loss: 4.9567\n","Epoch 186/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9515 - val_loss: 4.9565\n","Epoch 187/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9515 - val_loss: 4.9567\n","Epoch 188/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9515 - val_loss: 4.9567\n","Epoch 189/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9514 - val_loss: 4.9568\n","Epoch 190/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9514 - val_loss: 4.9569\n","Epoch 191/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9514 - val_loss: 4.9596\n","Epoch 192/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9515 - val_loss: 4.9569\n","Epoch 193/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9514 - val_loss: 4.9569\n","Epoch 194/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9515 - val_loss: 4.9567\n","Epoch 195/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9514 - val_loss: 4.9569\n","Epoch 196/250\n","6138/6138 [==============================] - 2s 291us/step - loss: 4.9514 - val_loss: 4.9566\n","Epoch 197/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9513 - val_loss: 4.9567\n","Epoch 198/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9514 - val_loss: 4.9567\n","Epoch 199/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9513 - val_loss: 4.9569\n","Epoch 200/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9514 - val_loss: 4.9568\n","Epoch 201/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9514 - val_loss: 4.9568\n","Epoch 202/250\n","6138/6138 [==============================] - 2s 301us/step - loss: 4.9513 - val_loss: 4.9570\n","Epoch 203/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9513 - val_loss: 4.9566\n","Epoch 204/250\n","6138/6138 [==============================] - 2s 300us/step - loss: 4.9514 - val_loss: 4.9567\n","Epoch 205/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9513 - val_loss: 4.9568\n","Epoch 206/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9513 - val_loss: 4.9567\n","Epoch 207/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9513 - val_loss: 4.9567\n","Epoch 208/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9513 - val_loss: 4.9569\n","Epoch 209/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9513 - val_loss: 4.9568\n","Epoch 210/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9513 - val_loss: 4.9567\n","Epoch 211/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9513 - val_loss: 4.9568\n","Epoch 212/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9513 - val_loss: 4.9566\n","Epoch 213/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9513 - val_loss: 4.9569\n","Epoch 214/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9513 - val_loss: 4.9569\n","Epoch 215/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9513 - val_loss: 4.9571\n","Epoch 216/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9513 - val_loss: 4.9566\n","Epoch 217/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9513 - val_loss: 4.9568\n","Epoch 218/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9514 - val_loss: 4.9566\n","Epoch 219/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9513 - val_loss: 4.9569\n","Epoch 220/250\n","6138/6138 [==============================] - 2s 294us/step - loss: 4.9513 - val_loss: 4.9567\n","Epoch 221/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9512 - val_loss: 4.9567\n","Epoch 222/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9512 - val_loss: 4.9569\n","Epoch 223/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9512 - val_loss: 4.9567\n","Epoch 224/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9513 - val_loss: 4.9567\n","Epoch 225/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9513 - val_loss: 4.9567\n","Epoch 226/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9513 - val_loss: 4.9567\n","Epoch 227/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9513 - val_loss: 4.9567\n","Epoch 228/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9512 - val_loss: 4.9566\n","Epoch 229/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9512 - val_loss: 4.9567\n","Epoch 230/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9512 - val_loss: 4.9566\n","Epoch 231/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9512 - val_loss: 4.9565\n","Epoch 232/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9512 - val_loss: 4.9566\n","Epoch 233/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9512 - val_loss: 4.9575\n","Epoch 234/250\n","6138/6138 [==============================] - 2s 295us/step - loss: 4.9512 - val_loss: 4.9567\n","Epoch 235/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9512 - val_loss: 4.9567\n","Epoch 236/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9512 - val_loss: 4.9567\n","Epoch 237/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9512 - val_loss: 4.9566\n","Epoch 238/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9512 - val_loss: 4.9566\n","Epoch 239/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9512 - val_loss: 4.9567\n","Epoch 240/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9512 - val_loss: 4.9569\n","Epoch 241/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9512 - val_loss: 4.9568\n","Epoch 242/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9512 - val_loss: 4.9567\n","Epoch 243/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9512 - val_loss: 4.9568\n","Epoch 244/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9512 - val_loss: 4.9569\n","Epoch 245/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9512 - val_loss: 4.9568\n","Epoch 246/250\n","6138/6138 [==============================] - 2s 294us/step - loss: 4.9512 - val_loss: 4.9567\n","Epoch 247/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9512 - val_loss: 4.9566\n","Epoch 248/250\n","6138/6138 [==============================] - 2s 330us/step - loss: 4.9512 - val_loss: 4.9567\n","Epoch 249/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9512 - val_loss: 4.9566\n","Epoch 250/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9512 - val_loss: 4.9568\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6821, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19497 0.5\n","The shape of N (6821, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9998342607365894\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5677, 28, 28, 1)\n","Train Label Shape:  (5677,)\n","Validation Data Shape:  (1134, 28, 28, 1)\n","Validation Label Shape:  (1134,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (67, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (67, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 6138 samples, validate on 683 samples\n","Epoch 1/250\n","6138/6138 [==============================] - 5s 788us/step - loss: 5.0530 - val_loss: 5.6693\n","Epoch 2/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9816 - val_loss: 5.4022\n","Epoch 3/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9718 - val_loss: 5.0482\n","Epoch 4/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9676 - val_loss: 5.2679\n","Epoch 5/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9610 - val_loss: 5.0033\n","Epoch 6/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9604 - val_loss: 4.9843\n","Epoch 7/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9603 - val_loss: 4.9757\n","Epoch 8/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9580 - val_loss: 4.9694\n","Epoch 9/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9579 - val_loss: 4.9878\n","Epoch 10/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9569 - val_loss: 4.9710\n","Epoch 11/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9554 - val_loss: 4.9673\n","Epoch 12/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9548 - val_loss: 4.9661\n","Epoch 13/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9542 - val_loss: 4.9652\n","Epoch 14/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9540 - val_loss: 4.9638\n","Epoch 15/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9538 - val_loss: 4.9642\n","Epoch 16/250\n","6138/6138 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9632\n","Epoch 17/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9535 - val_loss: 4.9626\n","Epoch 18/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9622\n","Epoch 19/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9533 - val_loss: 4.9624\n","Epoch 20/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9534 - val_loss: 4.9630\n","Epoch 21/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9531 - val_loss: 4.9624\n","Epoch 22/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9530 - val_loss: 4.9624\n","Epoch 23/250\n","6138/6138 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9620\n","Epoch 24/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9616\n","Epoch 25/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9614\n","Epoch 26/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9527 - val_loss: 4.9612\n","Epoch 27/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9526 - val_loss: 4.9612\n","Epoch 28/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9526 - val_loss: 4.9610\n","Epoch 29/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9526 - val_loss: 4.9607\n","Epoch 30/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9525 - val_loss: 4.9609\n","Epoch 31/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9525 - val_loss: 4.9613\n","Epoch 32/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9524 - val_loss: 4.9601\n","Epoch 33/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9523 - val_loss: 4.9598\n","Epoch 34/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9521 - val_loss: 4.9596\n","Epoch 35/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9520 - val_loss: 4.9594\n","Epoch 36/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9521 - val_loss: 4.9599\n","Epoch 37/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9522 - val_loss: 4.9599\n","Epoch 38/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9521 - val_loss: 4.9597\n","Epoch 39/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9520 - val_loss: 4.9594\n","Epoch 40/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9520 - val_loss: 4.9597\n","Epoch 41/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9519 - val_loss: 4.9593\n","Epoch 42/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9519 - val_loss: 4.9591\n","Epoch 43/250\n","6138/6138 [==============================] - 2s 327us/step - loss: 4.9519 - val_loss: 4.9590\n","Epoch 44/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9517 - val_loss: 4.9589\n","Epoch 45/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9518 - val_loss: 4.9590\n","Epoch 46/250\n","6138/6138 [==============================] - 2s 300us/step - loss: 4.9517 - val_loss: 4.9590\n","Epoch 47/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9518 - val_loss: 4.9590\n","Epoch 48/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9517 - val_loss: 4.9588\n","Epoch 49/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9517 - val_loss: 4.9589\n","Epoch 50/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9516 - val_loss: 4.9586\n","Epoch 51/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9517 - val_loss: 4.9586\n","Epoch 52/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9517 - val_loss: 4.9586\n","Epoch 53/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9517 - val_loss: 4.9593\n","Epoch 54/250\n","6138/6138 [==============================] - 2s 293us/step - loss: 4.9517 - val_loss: 4.9587\n","Epoch 55/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9517 - val_loss: 4.9586\n","Epoch 56/250\n","6138/6138 [==============================] - 2s 330us/step - loss: 4.9515 - val_loss: 4.9586\n","Epoch 57/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9515 - val_loss: 4.9588\n","Epoch 58/250\n","6138/6138 [==============================] - 2s 339us/step - loss: 4.9515 - val_loss: 4.9586\n","Epoch 59/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9515 - val_loss: 4.9584\n","Epoch 60/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9516 - val_loss: 4.9586\n","Epoch 61/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9514 - val_loss: 4.9581\n","Epoch 62/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9515 - val_loss: 4.9584\n","Epoch 63/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9514 - val_loss: 4.9584\n","Epoch 64/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9515 - val_loss: 4.9583\n","Epoch 65/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9514 - val_loss: 4.9582\n","Epoch 66/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9514 - val_loss: 4.9585\n","Epoch 67/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9514 - val_loss: 4.9583\n","Epoch 68/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9515 - val_loss: 4.9585\n","Epoch 69/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9513 - val_loss: 4.9583\n","Epoch 70/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9514 - val_loss: 4.9581\n","Epoch 71/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9513 - val_loss: 4.9580\n","Epoch 72/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9514 - val_loss: 4.9583\n","Epoch 73/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9513 - val_loss: 4.9581\n","Epoch 74/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9517 - val_loss: 4.9601\n","Epoch 75/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9516 - val_loss: 4.9586\n","Epoch 76/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9514 - val_loss: 4.9584\n","Epoch 77/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9514 - val_loss: 4.9582\n","Epoch 78/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9513 - val_loss: 4.9583\n","Epoch 79/250\n","6138/6138 [==============================] - 2s 297us/step - loss: 4.9514 - val_loss: 4.9580\n","Epoch 80/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9513 - val_loss: 4.9582\n","Epoch 81/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9513 - val_loss: 4.9582\n","Epoch 82/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9513 - val_loss: 4.9581\n","Epoch 83/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9513 - val_loss: 4.9580\n","Epoch 84/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9513 - val_loss: 4.9578\n","Epoch 85/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9512 - val_loss: 4.9581\n","Epoch 86/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9513 - val_loss: 4.9579\n","Epoch 87/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9513 - val_loss: 4.9581\n","Epoch 88/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9512 - val_loss: 4.9578\n","Epoch 89/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9511 - val_loss: 4.9579\n","Epoch 90/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9511 - val_loss: 4.9579\n","Epoch 91/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9512 - val_loss: 4.9578\n","Epoch 92/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9511 - val_loss: 4.9579\n","Epoch 93/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9512 - val_loss: 4.9576\n","Epoch 94/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9511 - val_loss: 4.9578\n","Epoch 95/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9511 - val_loss: 4.9578\n","Epoch 96/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9511 - val_loss: 4.9576\n","Epoch 97/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9511 - val_loss: 4.9580\n","Epoch 98/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9511 - val_loss: 4.9580\n","Epoch 99/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9511 - val_loss: 4.9577\n","Epoch 100/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9511 - val_loss: 4.9577\n","Epoch 101/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9511 - val_loss: 4.9577\n","Epoch 102/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9510 - val_loss: 4.9577\n","Epoch 103/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9511 - val_loss: 4.9578\n","Epoch 104/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9511 - val_loss: 4.9577\n","Epoch 105/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9510 - val_loss: 4.9577\n","Epoch 106/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9510 - val_loss: 4.9578\n","Epoch 107/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9510 - val_loss: 4.9578\n","Epoch 108/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9510 - val_loss: 4.9580\n","Epoch 109/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9510 - val_loss: 4.9576\n","Epoch 110/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9510 - val_loss: 4.9574\n","Epoch 111/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9509 - val_loss: 4.9576\n","Epoch 112/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9510 - val_loss: 4.9575\n","Epoch 113/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9510 - val_loss: 4.9576\n","Epoch 114/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9509 - val_loss: 4.9576\n","Epoch 115/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9509 - val_loss: 4.9577\n","Epoch 116/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9510 - val_loss: 4.9575\n","Epoch 117/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9509 - val_loss: 4.9574\n","Epoch 118/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9510 - val_loss: 4.9574\n","Epoch 119/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9510 - val_loss: 4.9575\n","Epoch 120/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9510 - val_loss: 4.9575\n","Epoch 121/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9508 - val_loss: 4.9576\n","Epoch 122/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9510 - val_loss: 4.9575\n","Epoch 123/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9509 - val_loss: 4.9573\n","Epoch 124/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9509 - val_loss: 4.9575\n","Epoch 125/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9509 - val_loss: 4.9572\n","Epoch 126/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9509 - val_loss: 4.9571\n","Epoch 127/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9509 - val_loss: 4.9573\n","Epoch 128/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9509 - val_loss: 4.9572\n","Epoch 129/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9509 - val_loss: 4.9573\n","Epoch 130/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9509 - val_loss: 4.9573\n","Epoch 131/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 132/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 133/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9509 - val_loss: 4.9574\n","Epoch 134/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9509 - val_loss: 4.9572\n","Epoch 135/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 136/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9509 - val_loss: 4.9574\n","Epoch 137/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9509 - val_loss: 4.9573\n","Epoch 138/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9508 - val_loss: 4.9577\n","Epoch 139/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9511 - val_loss: 4.9573\n","Epoch 140/250\n","6138/6138 [==============================] - 2s 294us/step - loss: 4.9509 - val_loss: 4.9572\n","Epoch 141/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9509 - val_loss: 4.9571\n","Epoch 142/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9508 - val_loss: 4.9571\n","Epoch 143/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9508 - val_loss: 4.9571\n","Epoch 144/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9508 - val_loss: 4.9571\n","Epoch 145/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9508 - val_loss: 4.9571\n","Epoch 146/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9508 - val_loss: 4.9572\n","Epoch 147/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9508 - val_loss: 4.9571\n","Epoch 148/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9508 - val_loss: 4.9571\n","Epoch 149/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9507 - val_loss: 4.9571\n","Epoch 150/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9507 - val_loss: 4.9572\n","Epoch 151/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9507 - val_loss: 4.9573\n","Epoch 152/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9508 - val_loss: 4.9570\n","Epoch 153/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 154/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9507 - val_loss: 4.9571\n","Epoch 155/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9508 - val_loss: 4.9571\n","Epoch 156/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9508 - val_loss: 4.9577\n","Epoch 157/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9510 - val_loss: 4.9573\n","Epoch 158/250\n","6138/6138 [==============================] - 2s 325us/step - loss: 4.9508 - val_loss: 4.9572\n","Epoch 159/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9508 - val_loss: 4.9571\n","Epoch 160/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9507 - val_loss: 4.9572\n","Epoch 161/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9507 - val_loss: 4.9573\n","Epoch 162/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9507 - val_loss: 4.9571\n","Epoch 163/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 164/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9508 - val_loss: 4.9571\n","Epoch 165/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9507 - val_loss: 4.9571\n","Epoch 166/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9507 - val_loss: 4.9572\n","Epoch 167/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9507 - val_loss: 4.9572\n","Epoch 168/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 169/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9507 - val_loss: 4.9571\n","Epoch 170/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9507 - val_loss: 4.9569\n","Epoch 171/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 172/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9507 - val_loss: 4.9569\n","Epoch 173/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9507 - val_loss: 4.9569\n","Epoch 174/250\n","6138/6138 [==============================] - 2s 299us/step - loss: 4.9506 - val_loss: 4.9572\n","Epoch 175/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 176/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 177/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9507 - val_loss: 4.9571\n","Epoch 178/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 179/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9506 - val_loss: 4.9571\n","Epoch 180/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 181/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 182/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 183/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9506 - val_loss: 4.9571\n","Epoch 184/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9507 - val_loss: 4.9571\n","Epoch 185/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 186/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9506 - val_loss: 4.9571\n","Epoch 187/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 188/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 189/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 190/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 191/250\n","6138/6138 [==============================] - 2s 334us/step - loss: 4.9506 - val_loss: 4.9571\n","Epoch 192/250\n","6138/6138 [==============================] - 2s 341us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 193/250\n","6138/6138 [==============================] - 2s 337us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 194/250\n","6138/6138 [==============================] - 2s 329us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 195/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 196/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 197/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 198/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 199/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 200/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9505 - val_loss: 4.9570\n","Epoch 201/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 202/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9506 - val_loss: 4.9571\n","Epoch 203/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 204/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9506 - val_loss: 4.9572\n","Epoch 205/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9507 - val_loss: 4.9568\n","Epoch 206/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9507 - val_loss: 4.9569\n","Epoch 207/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9506 - val_loss: 4.9572\n","Epoch 208/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 209/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9506 - val_loss: 4.9571\n","Epoch 210/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9506 - val_loss: 4.9571\n","Epoch 211/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 212/250\n","6138/6138 [==============================] - 2s 292us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 213/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 214/250\n","6138/6138 [==============================] - 2s 295us/step - loss: 4.9506 - val_loss: 4.9572\n","Epoch 215/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9505 - val_loss: 4.9572\n","Epoch 216/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 217/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 218/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9505 - val_loss: 4.9570\n","Epoch 219/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9505 - val_loss: 4.9570\n","Epoch 220/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9506 - val_loss: 4.9681\n","Epoch 221/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9506 - val_loss: 4.9571\n","Epoch 222/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 223/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 224/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 225/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 226/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 227/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 228/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 229/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9505 - val_loss: 4.9567\n","Epoch 230/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 231/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 232/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9505 - val_loss: 4.9571\n","Epoch 233/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 234/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 235/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 236/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 237/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9504 - val_loss: 4.9568\n","Epoch 238/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 239/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9505 - val_loss: 4.9567\n","Epoch 240/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9504 - val_loss: 4.9568\n","Epoch 241/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9504 - val_loss: 4.9568\n","Epoch 242/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 243/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 244/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 245/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9504 - val_loss: 4.9568\n","Epoch 246/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9504 - val_loss: 4.9568\n","Epoch 247/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9504 - val_loss: 4.9567\n","Epoch 248/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9504 - val_loss: 4.9567\n","Epoch 249/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9504 - val_loss: 4.9568\n","Epoch 250/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9505 - val_loss: 4.9567\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6821, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 18794 0.5\n","The shape of N (6821, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7499978542327881\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9999292845809449\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5677, 28, 28, 1)\n","Train Label Shape:  (5677,)\n","Validation Data Shape:  (1134, 28, 28, 1)\n","Validation Label Shape:  (1134,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (67, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (67, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 6138 samples, validate on 683 samples\n","Epoch 1/250\n","6138/6138 [==============================] - 6s 904us/step - loss: 5.0534 - val_loss: 5.7424\n","Epoch 2/250\n","6138/6138 [==============================] - 2s 297us/step - loss: 4.9869 - val_loss: 5.3330\n","Epoch 3/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9809 - val_loss: 5.2912\n","Epoch 4/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9709 - val_loss: 5.1680\n","Epoch 5/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9664 - val_loss: 4.9952\n","Epoch 6/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9607 - val_loss: 4.9780\n","Epoch 7/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9616 - val_loss: 4.9786\n","Epoch 8/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9616 - val_loss: 4.9809\n","Epoch 9/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9595 - val_loss: 4.9778\n","Epoch 10/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9575 - val_loss: 4.9692\n","Epoch 11/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9564 - val_loss: 4.9658\n","Epoch 12/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9562 - val_loss: 4.9672\n","Epoch 13/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9557 - val_loss: 4.9637\n","Epoch 14/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9549 - val_loss: 4.9629\n","Epoch 15/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9552 - val_loss: 4.9653\n","Epoch 16/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9548 - val_loss: 4.9632\n","Epoch 17/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9546 - val_loss: 4.9627\n","Epoch 18/250\n","6138/6138 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9615\n","Epoch 19/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9613\n","Epoch 20/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9539 - val_loss: 4.9612\n","Epoch 21/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9608\n","Epoch 22/250\n","6138/6138 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9606\n","Epoch 23/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9586 - val_loss: 5.0350\n","Epoch 24/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9555 - val_loss: 4.9775\n","Epoch 25/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9548 - val_loss: 4.9694\n","Epoch 26/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9544 - val_loss: 4.9653\n","Epoch 27/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9540 - val_loss: 4.9634\n","Epoch 28/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9539 - val_loss: 4.9627\n","Epoch 29/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9537 - val_loss: 4.9613\n","Epoch 30/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9537 - val_loss: 4.9612\n","Epoch 31/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9534 - val_loss: 4.9610\n","Epoch 32/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9606\n","Epoch 33/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9533 - val_loss: 4.9602\n","Epoch 34/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9539 - val_loss: 4.9668\n","Epoch 35/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9539 - val_loss: 4.9628\n","Epoch 36/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9533 - val_loss: 4.9616\n","Epoch 37/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9609\n","Epoch 38/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9531 - val_loss: 4.9605\n","Epoch 39/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9530 - val_loss: 4.9601\n","Epoch 40/250\n","6138/6138 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9599\n","Epoch 41/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9529 - val_loss: 4.9596\n","Epoch 42/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9597\n","Epoch 43/250\n","6138/6138 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9595\n","Epoch 44/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9613\n","Epoch 45/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9542 - val_loss: 4.9640\n","Epoch 46/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9535 - val_loss: 4.9608\n","Epoch 47/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9601\n","Epoch 48/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9529 - val_loss: 4.9597\n","Epoch 49/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9598\n","Epoch 50/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9528 - val_loss: 4.9594\n","Epoch 51/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9527 - val_loss: 4.9593\n","Epoch 52/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9526 - val_loss: 4.9591\n","Epoch 53/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9525 - val_loss: 4.9594\n","Epoch 54/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9526 - val_loss: 4.9592\n","Epoch 55/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9526 - val_loss: 4.9591\n","Epoch 56/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9526 - val_loss: 4.9589\n","Epoch 57/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9526 - val_loss: 4.9588\n","Epoch 58/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9525 - val_loss: 4.9592\n","Epoch 59/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9525 - val_loss: 4.9586\n","Epoch 60/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9524 - val_loss: 4.9587\n","Epoch 61/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9524 - val_loss: 4.9589\n","Epoch 62/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9524 - val_loss: 4.9589\n","Epoch 63/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9524 - val_loss: 4.9589\n","Epoch 64/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9523 - val_loss: 4.9586\n","Epoch 65/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9523 - val_loss: 4.9585\n","Epoch 66/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9523 - val_loss: 4.9581\n","Epoch 67/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9522 - val_loss: 4.9581\n","Epoch 68/250\n","6138/6138 [==============================] - 2s 332us/step - loss: 4.9522 - val_loss: 4.9582\n","Epoch 69/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9523 - val_loss: 4.9585\n","Epoch 70/250\n","6138/6138 [==============================] - 2s 330us/step - loss: 4.9523 - val_loss: 4.9583\n","Epoch 71/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9521 - val_loss: 4.9581\n","Epoch 72/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9521 - val_loss: 4.9581\n","Epoch 73/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9520 - val_loss: 4.9581\n","Epoch 74/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9521 - val_loss: 4.9582\n","Epoch 75/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9521 - val_loss: 4.9580\n","Epoch 76/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9520 - val_loss: 4.9578\n","Epoch 77/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9520 - val_loss: 4.9578\n","Epoch 78/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9520 - val_loss: 4.9580\n","Epoch 79/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9519 - val_loss: 4.9579\n","Epoch 80/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9520 - val_loss: 4.9579\n","Epoch 81/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9520 - val_loss: 4.9580\n","Epoch 82/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9520 - val_loss: 4.9579\n","Epoch 83/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9520 - val_loss: 4.9577\n","Epoch 84/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9519 - val_loss: 4.9576\n","Epoch 85/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9519 - val_loss: 4.9577\n","Epoch 86/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9519 - val_loss: 4.9577\n","Epoch 87/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9579\n","Epoch 88/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9519 - val_loss: 4.9577\n","Epoch 89/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9518 - val_loss: 4.9578\n","Epoch 90/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9518 - val_loss: 4.9577\n","Epoch 91/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9518 - val_loss: 4.9576\n","Epoch 92/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9519 - val_loss: 4.9575\n","Epoch 93/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9518 - val_loss: 4.9576\n","Epoch 94/250\n","6138/6138 [==============================] - 2s 292us/step - loss: 4.9518 - val_loss: 4.9576\n","Epoch 95/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9520 - val_loss: 4.9584\n","Epoch 96/250\n","6138/6138 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9626\n","Epoch 97/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9522 - val_loss: 4.9598\n","Epoch 98/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9521 - val_loss: 4.9585\n","Epoch 99/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9520 - val_loss: 4.9577\n","Epoch 100/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9519 - val_loss: 4.9578\n","Epoch 101/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9577\n","Epoch 102/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9519 - val_loss: 4.9574\n","Epoch 103/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9519 - val_loss: 4.9575\n","Epoch 104/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9517 - val_loss: 4.9572\n","Epoch 105/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9517 - val_loss: 4.9575\n","Epoch 106/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9518 - val_loss: 4.9574\n","Epoch 107/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9517 - val_loss: 4.9575\n","Epoch 108/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9517 - val_loss: 4.9572\n","Epoch 109/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9517 - val_loss: 4.9572\n","Epoch 110/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9516 - val_loss: 4.9570\n","Epoch 111/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9517 - val_loss: 4.9571\n","Epoch 112/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9516 - val_loss: 4.9572\n","Epoch 113/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9516 - val_loss: 4.9574\n","Epoch 114/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9517 - val_loss: 4.9572\n","Epoch 115/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9516 - val_loss: 4.9571\n","Epoch 116/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9516 - val_loss: 4.9571\n","Epoch 117/250\n","6138/6138 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9571\n","Epoch 118/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9515 - val_loss: 4.9572\n","Epoch 119/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9515 - val_loss: 4.9570\n","Epoch 120/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9516 - val_loss: 4.9570\n","Epoch 121/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9515 - val_loss: 4.9570\n","Epoch 122/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9515 - val_loss: 4.9569\n","Epoch 123/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9515 - val_loss: 4.9604\n","Epoch 124/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9516 - val_loss: 4.9569\n","Epoch 125/250\n","6138/6138 [==============================] - 2s 295us/step - loss: 4.9515 - val_loss: 4.9567\n","Epoch 126/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9515 - val_loss: 4.9574\n","Epoch 127/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9515 - val_loss: 4.9569\n","Epoch 128/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9515 - val_loss: 4.9568\n","Epoch 129/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9515 - val_loss: 4.9568\n","Epoch 130/250\n","6138/6138 [==============================] - 2s 297us/step - loss: 4.9514 - val_loss: 4.9569\n","Epoch 131/250\n","6138/6138 [==============================] - 2s 299us/step - loss: 4.9514 - val_loss: 4.9567\n","Epoch 132/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9515 - val_loss: 4.9570\n","Epoch 133/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9515 - val_loss: 4.9566\n","Epoch 134/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9515 - val_loss: 4.9571\n","Epoch 135/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9515 - val_loss: 4.9568\n","Epoch 136/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9514 - val_loss: 4.9567\n","Epoch 137/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9514 - val_loss: 4.9566\n","Epoch 138/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9514 - val_loss: 4.9569\n","Epoch 139/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9514 - val_loss: 4.9566\n","Epoch 140/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9514 - val_loss: 4.9566\n","Epoch 141/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9514 - val_loss: 4.9568\n","Epoch 142/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9514 - val_loss: 4.9566\n","Epoch 143/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9514 - val_loss: 4.9569\n","Epoch 144/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9514 - val_loss: 4.9567\n","Epoch 145/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9513 - val_loss: 4.9567\n","Epoch 146/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9514 - val_loss: 4.9565\n","Epoch 147/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9514 - val_loss: 4.9568\n","Epoch 148/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9513 - val_loss: 4.9565\n","Epoch 149/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9513 - val_loss: 4.9564\n","Epoch 150/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9513 - val_loss: 4.9566\n","Epoch 151/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9513 - val_loss: 4.9567\n","Epoch 152/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9517 - val_loss: 4.9577\n","Epoch 153/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9515 - val_loss: 4.9568\n","Epoch 154/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9514 - val_loss: 4.9567\n","Epoch 155/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9514 - val_loss: 4.9566\n","Epoch 156/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9513 - val_loss: 4.9565\n","Epoch 157/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9513 - val_loss: 4.9565\n","Epoch 158/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9513 - val_loss: 4.9563\n","Epoch 159/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9513 - val_loss: 4.9568\n","Epoch 160/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9513 - val_loss: 4.9565\n","Epoch 161/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9513 - val_loss: 4.9564\n","Epoch 162/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9512 - val_loss: 4.9563\n","Epoch 163/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9513 - val_loss: 4.9563\n","Epoch 164/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9512 - val_loss: 4.9563\n","Epoch 165/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9512 - val_loss: 4.9562\n","Epoch 166/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9513 - val_loss: 4.9562\n","Epoch 167/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9512 - val_loss: 4.9562\n","Epoch 168/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9512 - val_loss: 4.9564\n","Epoch 169/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9513 - val_loss: 4.9565\n","Epoch 170/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9513 - val_loss: 4.9568\n","Epoch 171/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9512 - val_loss: 4.9564\n","Epoch 172/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9513 - val_loss: 4.9565\n","Epoch 173/250\n","6138/6138 [==============================] - 2s 295us/step - loss: 4.9512 - val_loss: 4.9562\n","Epoch 174/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9512 - val_loss: 4.9562\n","Epoch 175/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9515 - val_loss: 4.9630\n","Epoch 176/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9515 - val_loss: 4.9595\n","Epoch 177/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9514 - val_loss: 4.9576\n","Epoch 178/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9514 - val_loss: 4.9575\n","Epoch 179/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9514 - val_loss: 4.9569\n","Epoch 180/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9514 - val_loss: 4.9564\n","Epoch 181/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9513 - val_loss: 4.9566\n","Epoch 182/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9513 - val_loss: 4.9562\n","Epoch 183/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9512 - val_loss: 4.9564\n","Epoch 184/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9513 - val_loss: 4.9563\n","Epoch 185/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9512 - val_loss: 4.9565\n","Epoch 186/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9513 - val_loss: 4.9564\n","Epoch 187/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9512 - val_loss: 4.9563\n","Epoch 188/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9512 - val_loss: 4.9565\n","Epoch 189/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9512 - val_loss: 4.9564\n","Epoch 190/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9512 - val_loss: 4.9563\n","Epoch 191/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9512 - val_loss: 4.9564\n","Epoch 192/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9512 - val_loss: 4.9566\n","Epoch 193/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9512 - val_loss: 4.9564\n","Epoch 194/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9512 - val_loss: 4.9600\n","Epoch 195/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9512 - val_loss: 4.9564\n","Epoch 196/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9512 - val_loss: 4.9565\n","Epoch 197/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9511 - val_loss: 4.9564\n","Epoch 198/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9512 - val_loss: 4.9564\n","Epoch 199/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9511 - val_loss: 4.9563\n","Epoch 200/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9511 - val_loss: 4.9562\n","Epoch 201/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9511 - val_loss: 4.9564\n","Epoch 202/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9512 - val_loss: 4.9564\n","Epoch 203/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9511 - val_loss: 4.9566\n","Epoch 204/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9511 - val_loss: 4.9563\n","Epoch 205/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9511 - val_loss: 4.9564\n","Epoch 206/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9511 - val_loss: 4.9565\n","Epoch 207/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9511 - val_loss: 4.9564\n","Epoch 208/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9511 - val_loss: 4.9563\n","Epoch 209/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9511 - val_loss: 4.9563\n","Epoch 210/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9511 - val_loss: 4.9564\n","Epoch 211/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9511 - val_loss: 4.9564\n","Epoch 212/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9511 - val_loss: 4.9564\n","Epoch 213/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9511 - val_loss: 4.9563\n","Epoch 214/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9510 - val_loss: 4.9563\n","Epoch 215/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9510 - val_loss: 4.9563\n","Epoch 216/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9511 - val_loss: 4.9563\n","Epoch 217/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9510 - val_loss: 4.9563\n","Epoch 218/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9511 - val_loss: 4.9564\n","Epoch 219/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9510 - val_loss: 4.9564\n","Epoch 220/250\n","6138/6138 [==============================] - 2s 294us/step - loss: 4.9511 - val_loss: 4.9563\n","Epoch 221/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9510 - val_loss: 4.9563\n","Epoch 222/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9510 - val_loss: 4.9562\n","Epoch 223/250\n","6138/6138 [==============================] - 2s 292us/step - loss: 4.9510 - val_loss: 4.9562\n","Epoch 224/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9510 - val_loss: 4.9562\n","Epoch 225/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9510 - val_loss: 4.9563\n","Epoch 226/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9511 - val_loss: 4.9562\n","Epoch 227/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9510 - val_loss: 4.9563\n","Epoch 228/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9510 - val_loss: 4.9563\n","Epoch 229/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9510 - val_loss: 4.9564\n","Epoch 230/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9510 - val_loss: 4.9563\n","Epoch 231/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9510 - val_loss: 4.9562\n","Epoch 232/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9510 - val_loss: 4.9562\n","Epoch 233/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9510 - val_loss: 4.9563\n","Epoch 234/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9510 - val_loss: 4.9562\n","Epoch 235/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9510 - val_loss: 4.9562\n","Epoch 236/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9510 - val_loss: 4.9562\n","Epoch 237/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9510 - val_loss: 4.9563\n","Epoch 238/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9509 - val_loss: 4.9563\n","Epoch 239/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9510 - val_loss: 4.9564\n","Epoch 240/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9510 - val_loss: 4.9563\n","Epoch 241/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9510 - val_loss: 4.9567\n","Epoch 242/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9509 - val_loss: 4.9563\n","Epoch 243/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9510 - val_loss: 4.9563\n","Epoch 244/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9509 - val_loss: 4.9562\n","Epoch 245/250\n","6138/6138 [==============================] - 2s 301us/step - loss: 4.9509 - val_loss: 4.9563\n","Epoch 246/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9510 - val_loss: 4.9563\n","Epoch 247/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9509 - val_loss: 4.9562\n","Epoch 248/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9509 - val_loss: 4.9563\n","Epoch 249/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9510 - val_loss: 4.9564\n","Epoch 250/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9510 - val_loss: 4.9562\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6821, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 18096 0.5\n","The shape of N (6821, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9999977901431546\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5677, 28, 28, 1)\n","Train Label Shape:  (5677,)\n","Validation Data Shape:  (1134, 28, 28, 1)\n","Validation Label Shape:  (1134,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (67, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (67, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 6138 samples, validate on 683 samples\n","Epoch 1/250\n","6138/6138 [==============================] - 6s 975us/step - loss: 5.0600 - val_loss: 5.1220\n","Epoch 2/250\n","6138/6138 [==============================] - 2s 295us/step - loss: 4.9813 - val_loss: 4.9901\n","Epoch 3/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9685 - val_loss: 4.9780\n","Epoch 4/250\n","6138/6138 [==============================] - 2s 298us/step - loss: 4.9624 - val_loss: 4.9748\n","Epoch 5/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9597 - val_loss: 4.9682\n","Epoch 6/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9581 - val_loss: 4.9658\n","Epoch 7/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9570 - val_loss: 4.9640\n","Epoch 8/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9557 - val_loss: 4.9628\n","Epoch 9/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9553 - val_loss: 4.9627\n","Epoch 10/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9551 - val_loss: 4.9620\n","Epoch 11/250\n","6138/6138 [==============================] - 2s 334us/step - loss: 4.9547 - val_loss: 4.9620\n","Epoch 12/250\n","6138/6138 [==============================] - 2s 355us/step - loss: 4.9544 - val_loss: 4.9612\n","Epoch 13/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9543 - val_loss: 4.9611\n","Epoch 14/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9542 - val_loss: 4.9608\n","Epoch 15/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9542 - val_loss: 4.9612\n","Epoch 16/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9540 - val_loss: 4.9608\n","Epoch 17/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9538 - val_loss: 4.9605\n","Epoch 18/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9537 - val_loss: 4.9602\n","Epoch 19/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9536 - val_loss: 4.9603\n","Epoch 20/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9536 - val_loss: 4.9604\n","Epoch 21/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9536 - val_loss: 4.9601\n","Epoch 22/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9533 - val_loss: 4.9599\n","Epoch 23/250\n","6138/6138 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9599\n","Epoch 24/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9533 - val_loss: 4.9600\n","Epoch 25/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9533 - val_loss: 4.9597\n","Epoch 26/250\n","6138/6138 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9598\n","Epoch 27/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9533 - val_loss: 4.9611\n","Epoch 28/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9531 - val_loss: 4.9599\n","Epoch 29/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9530 - val_loss: 4.9598\n","Epoch 30/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9530 - val_loss: 4.9602\n","Epoch 31/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9529 - val_loss: 4.9596\n","Epoch 32/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9529 - val_loss: 4.9594\n","Epoch 33/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9529 - val_loss: 4.9595\n","Epoch 34/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9529 - val_loss: 4.9593\n","Epoch 35/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9529 - val_loss: 4.9593\n","Epoch 36/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9529 - val_loss: 4.9593\n","Epoch 37/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9529 - val_loss: 4.9593\n","Epoch 38/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9529 - val_loss: 4.9592\n","Epoch 39/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9528 - val_loss: 4.9595\n","Epoch 40/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9528 - val_loss: 4.9592\n","Epoch 41/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9528 - val_loss: 4.9590\n","Epoch 42/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9528 - val_loss: 4.9596\n","Epoch 43/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9527 - val_loss: 4.9592\n","Epoch 44/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9526 - val_loss: 4.9595\n","Epoch 45/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9526 - val_loss: 4.9591\n","Epoch 46/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9527 - val_loss: 4.9592\n","Epoch 47/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9527 - val_loss: 4.9590\n","Epoch 48/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9527 - val_loss: 4.9588\n","Epoch 49/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9526 - val_loss: 4.9592\n","Epoch 50/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9525 - val_loss: 4.9590\n","Epoch 51/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9526 - val_loss: 4.9589\n","Epoch 52/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9525 - val_loss: 4.9589\n","Epoch 53/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9525 - val_loss: 4.9587\n","Epoch 54/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9525 - val_loss: 4.9591\n","Epoch 55/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9524 - val_loss: 4.9586\n","Epoch 56/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9525 - val_loss: 4.9586\n","Epoch 57/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9524 - val_loss: 4.9589\n","Epoch 58/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9525 - val_loss: 4.9586\n","Epoch 59/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9524 - val_loss: 4.9587\n","Epoch 60/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9524 - val_loss: 4.9587\n","Epoch 61/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9525 - val_loss: 4.9587\n","Epoch 62/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9524 - val_loss: 4.9586\n","Epoch 63/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9524 - val_loss: 4.9586\n","Epoch 64/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9524 - val_loss: 4.9587\n","Epoch 65/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9523 - val_loss: 4.9586\n","Epoch 66/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9524 - val_loss: 4.9585\n","Epoch 67/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9524 - val_loss: 4.9586\n","Epoch 68/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9524 - val_loss: 4.9588\n","Epoch 69/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9524 - val_loss: 4.9584\n","Epoch 70/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9524 - val_loss: 4.9585\n","Epoch 71/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9585\n","Epoch 72/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9523 - val_loss: 4.9584\n","Epoch 73/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9523 - val_loss: 4.9584\n","Epoch 74/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9523 - val_loss: 4.9584\n","Epoch 75/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9523 - val_loss: 4.9584\n","Epoch 76/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9523 - val_loss: 4.9584\n","Epoch 77/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9523 - val_loss: 4.9582\n","Epoch 78/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9522 - val_loss: 4.9583\n","Epoch 79/250\n","6138/6138 [==============================] - 2s 346us/step - loss: 4.9522 - val_loss: 4.9583\n","Epoch 80/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9522 - val_loss: 4.9582\n","Epoch 81/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9522 - val_loss: 4.9583\n","Epoch 82/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9522 - val_loss: 4.9582\n","Epoch 83/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9523 - val_loss: 4.9582\n","Epoch 84/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9522 - val_loss: 4.9584\n","Epoch 85/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9522 - val_loss: 4.9582\n","Epoch 86/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9521 - val_loss: 4.9584\n","Epoch 87/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9522 - val_loss: 4.9580\n","Epoch 88/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9521 - val_loss: 4.9581\n","Epoch 89/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9522 - val_loss: 4.9582\n","Epoch 90/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9523 - val_loss: 4.9581\n","Epoch 91/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9522 - val_loss: 4.9584\n","Epoch 92/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9522 - val_loss: 4.9582\n","Epoch 93/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9521 - val_loss: 4.9581\n","Epoch 94/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9521 - val_loss: 4.9582\n","Epoch 95/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9522 - val_loss: 4.9582\n","Epoch 96/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9521 - val_loss: 4.9582\n","Epoch 97/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9521 - val_loss: 4.9580\n","Epoch 98/250\n","6138/6138 [==============================] - 2s 295us/step - loss: 4.9520 - val_loss: 4.9582\n","Epoch 99/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9522 - val_loss: 4.9581\n","Epoch 100/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9521 - val_loss: 4.9581\n","Epoch 101/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9521 - val_loss: 4.9584\n","Epoch 102/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9521 - val_loss: 4.9585\n","Epoch 103/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9521 - val_loss: 4.9582\n","Epoch 104/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9521 - val_loss: 4.9580\n","Epoch 105/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9521 - val_loss: 4.9581\n","Epoch 106/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9521 - val_loss: 4.9581\n","Epoch 107/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9520 - val_loss: 4.9582\n","Epoch 108/250\n","6138/6138 [==============================] - 2s 299us/step - loss: 4.9520 - val_loss: 4.9582\n","Epoch 109/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9521 - val_loss: 4.9584\n","Epoch 110/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9521 - val_loss: 4.9582\n","Epoch 111/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9520 - val_loss: 4.9581\n","Epoch 112/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9520 - val_loss: 4.9585\n","Epoch 113/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9521 - val_loss: 4.9582\n","Epoch 114/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9520 - val_loss: 4.9581\n","Epoch 115/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9520 - val_loss: 4.9581\n","Epoch 116/250\n","6138/6138 [==============================] - 2s 325us/step - loss: 4.9521 - val_loss: 4.9583\n","Epoch 117/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9520 - val_loss: 4.9580\n","Epoch 118/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9519 - val_loss: 4.9582\n","Epoch 119/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9520 - val_loss: 4.9580\n","Epoch 120/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9520 - val_loss: 4.9580\n","Epoch 121/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9520 - val_loss: 4.9581\n","Epoch 122/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9520 - val_loss: 4.9584\n","Epoch 123/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9520 - val_loss: 4.9580\n","Epoch 124/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9519 - val_loss: 4.9582\n","Epoch 125/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9520 - val_loss: 4.9582\n","Epoch 126/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9520 - val_loss: 4.9583\n","Epoch 127/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9520 - val_loss: 4.9581\n","Epoch 128/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9520 - val_loss: 4.9582\n","Epoch 129/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9520 - val_loss: 4.9581\n","Epoch 130/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9520 - val_loss: 4.9637\n","Epoch 131/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9520 - val_loss: 4.9582\n","Epoch 132/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9519 - val_loss: 4.9583\n","Epoch 133/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9519 - val_loss: 4.9583\n","Epoch 134/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9519 - val_loss: 4.9580\n","Epoch 135/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9519 - val_loss: 4.9581\n","Epoch 136/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9520 - val_loss: 4.9581\n","Epoch 137/250\n","6138/6138 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9580\n","Epoch 138/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9520 - val_loss: 4.9580\n","Epoch 139/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9583\n","Epoch 140/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9519 - val_loss: 4.9583\n","Epoch 141/250\n","6138/6138 [==============================] - 2s 295us/step - loss: 4.9519 - val_loss: 4.9579\n","Epoch 142/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9519 - val_loss: 4.9583\n","Epoch 143/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9520 - val_loss: 4.9580\n","Epoch 144/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9581\n","Epoch 145/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9519 - val_loss: 4.9581\n","Epoch 146/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9519 - val_loss: 4.9580\n","Epoch 147/250\n","6138/6138 [==============================] - 2s 345us/step - loss: 4.9519 - val_loss: 4.9636\n","Epoch 148/250\n","6138/6138 [==============================] - 2s 329us/step - loss: 4.9519 - val_loss: 4.9646\n","Epoch 149/250\n","6138/6138 [==============================] - 2s 295us/step - loss: 4.9519 - val_loss: 4.9581\n","Epoch 150/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9519 - val_loss: 4.9582\n","Epoch 151/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9519 - val_loss: 4.9581\n","Epoch 152/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9519 - val_loss: 4.9581\n","Epoch 153/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9519 - val_loss: 4.9581\n","Epoch 154/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9519 - val_loss: 4.9581\n","Epoch 155/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9519 - val_loss: 4.9581\n","Epoch 156/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9519 - val_loss: 4.9582\n","Epoch 157/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9518 - val_loss: 4.9582\n","Epoch 158/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9518 - val_loss: 4.9580\n","Epoch 159/250\n","6138/6138 [==============================] - 2s 297us/step - loss: 4.9519 - val_loss: 4.9581\n","Epoch 160/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9519 - val_loss: 4.9582\n","Epoch 161/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9519 - val_loss: 4.9580\n","Epoch 162/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9519 - val_loss: 4.9583\n","Epoch 163/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9518 - val_loss: 4.9582\n","Epoch 164/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9518 - val_loss: 4.9580\n","Epoch 165/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9518 - val_loss: 4.9581\n","Epoch 166/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9518 - val_loss: 4.9586\n","Epoch 167/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9518 - val_loss: 4.9583\n","Epoch 168/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9519 - val_loss: 4.9583\n","Epoch 169/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9518 - val_loss: 4.9582\n","Epoch 170/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9518 - val_loss: 4.9580\n","Epoch 171/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9518 - val_loss: 4.9580\n","Epoch 172/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9518 - val_loss: 4.9579\n","Epoch 173/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9518 - val_loss: 4.9581\n","Epoch 174/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9518 - val_loss: 4.9579\n","Epoch 175/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9581\n","Epoch 176/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9518 - val_loss: 4.9580\n","Epoch 177/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9518 - val_loss: 4.9580\n","Epoch 178/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9518 - val_loss: 4.9580\n","Epoch 179/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9517 - val_loss: 4.9581\n","Epoch 180/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9518 - val_loss: 4.9579\n","Epoch 181/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9518 - val_loss: 4.9581\n","Epoch 182/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9518 - val_loss: 4.9583\n","Epoch 183/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9518 - val_loss: 4.9580\n","Epoch 184/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9518 - val_loss: 4.9580\n","Epoch 185/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9517 - val_loss: 4.9580\n","Epoch 186/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9518 - val_loss: 4.9579\n","Epoch 187/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9518 - val_loss: 4.9580\n","Epoch 188/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9518 - val_loss: 4.9581\n","Epoch 189/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9518 - val_loss: 4.9580\n","Epoch 190/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9517 - val_loss: 4.9580\n","Epoch 191/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9517 - val_loss: 4.9583\n","Epoch 192/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 193/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9517 - val_loss: 4.9580\n","Epoch 194/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9518 - val_loss: 4.9581\n","Epoch 195/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9518 - val_loss: 4.9581\n","Epoch 196/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9517 - val_loss: 4.9580\n","Epoch 197/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9518 - val_loss: 4.9580\n","Epoch 198/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9518 - val_loss: 4.9581\n","Epoch 199/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9517 - val_loss: 4.9581\n","Epoch 200/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9517 - val_loss: 4.9624\n","Epoch 201/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9518 - val_loss: 4.9583\n","Epoch 202/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 203/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9517 - val_loss: 4.9580\n","Epoch 204/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9517 - val_loss: 4.9581\n","Epoch 205/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 206/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9517 - val_loss: 4.9578\n","Epoch 207/250\n","6138/6138 [==============================] - 2s 326us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 208/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9517 - val_loss: 4.9580\n","Epoch 209/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9517 - val_loss: 4.9580\n","Epoch 210/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9517 - val_loss: 4.9583\n","Epoch 211/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 212/250\n","6138/6138 [==============================] - 2s 325us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 213/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 214/250\n","6138/6138 [==============================] - 2s 331us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 215/250\n","6138/6138 [==============================] - 2s 334us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 216/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 217/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9517 - val_loss: 4.9580\n","Epoch 218/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9517 - val_loss: 4.9578\n","Epoch 219/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 220/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 221/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 222/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 223/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 224/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9517 - val_loss: 4.9580\n","Epoch 225/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 226/250\n","6138/6138 [==============================] - 2s 294us/step - loss: 4.9517 - val_loss: 4.9582\n","Epoch 227/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9517 - val_loss: 4.9580\n","Epoch 228/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9517 - val_loss: 4.9585\n","Epoch 229/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 230/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 231/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 232/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 233/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 234/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9517 - val_loss: 4.9581\n","Epoch 235/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9517 - val_loss: 4.9579\n","Epoch 236/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9516 - val_loss: 4.9581\n","Epoch 237/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9517 - val_loss: 4.9580\n","Epoch 238/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9517 - val_loss: 4.9578\n","Epoch 239/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9516 - val_loss: 4.9581\n","Epoch 240/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9516 - val_loss: 4.9582\n","Epoch 241/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9516 - val_loss: 4.9580\n","Epoch 242/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9517 - val_loss: 4.9580\n","Epoch 243/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9517 - val_loss: 4.9580\n","Epoch 244/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9516 - val_loss: 4.9578\n","Epoch 245/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9516 - val_loss: 4.9579\n","Epoch 246/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9516 - val_loss: 4.9578\n","Epoch 247/250\n","6138/6138 [==============================] - 2s 295us/step - loss: 4.9516 - val_loss: 4.9578\n","Epoch 248/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9516 - val_loss: 4.9579\n","Epoch 249/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9516 - val_loss: 4.9580\n","Epoch 250/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9516 - val_loss: 4.9580\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6821, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 16259 0.5\n","The shape of N (6821, 784)\n","The minimum value of N  -0.7499566078186035\n","The max value of N 0.7499982714653015\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9999867408589271\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5677, 28, 28, 1)\n","Train Label Shape:  (5677,)\n","Validation Data Shape:  (1134, 28, 28, 1)\n","Validation Label Shape:  (1134,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (67, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (67, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 6138 samples, validate on 683 samples\n","Epoch 1/250\n","6138/6138 [==============================] - 7s 1ms/step - loss: 5.0607 - val_loss: 5.7059\n","Epoch 2/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9819 - val_loss: 5.3171\n","Epoch 3/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9722 - val_loss: 5.0341\n","Epoch 4/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9629 - val_loss: 4.9769\n","Epoch 5/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9594 - val_loss: 4.9694\n","Epoch 6/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9580 - val_loss: 4.9684\n","Epoch 7/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9566 - val_loss: 4.9662\n","Epoch 8/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9555 - val_loss: 4.9649\n","Epoch 9/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9547 - val_loss: 4.9631\n","Epoch 10/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9541 - val_loss: 4.9632\n","Epoch 11/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9539 - val_loss: 4.9635\n","Epoch 12/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9536 - val_loss: 4.9621\n","Epoch 13/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9534 - val_loss: 4.9628\n","Epoch 14/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9533 - val_loss: 4.9644\n","Epoch 15/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9533 - val_loss: 4.9634\n","Epoch 16/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9531 - val_loss: 4.9642\n","Epoch 17/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9532 - val_loss: 4.9621\n","Epoch 18/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9529 - val_loss: 4.9626\n","Epoch 19/250\n","6138/6138 [==============================] - 2s 334us/step - loss: 4.9528 - val_loss: 4.9621\n","Epoch 20/250\n","6138/6138 [==============================] - 2s 334us/step - loss: 4.9526 - val_loss: 4.9618\n","Epoch 21/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9526 - val_loss: 4.9609\n","Epoch 22/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9526 - val_loss: 4.9615\n","Epoch 23/250\n","6138/6138 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9623\n","Epoch 24/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9525 - val_loss: 4.9610\n","Epoch 25/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9524 - val_loss: 4.9610\n","Epoch 26/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9524 - val_loss: 4.9614\n","Epoch 27/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9523 - val_loss: 4.9604\n","Epoch 28/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9523 - val_loss: 4.9604\n","Epoch 29/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9522 - val_loss: 4.9604\n","Epoch 30/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9521 - val_loss: 4.9602\n","Epoch 31/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9522 - val_loss: 4.9602\n","Epoch 32/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9521 - val_loss: 4.9604\n","Epoch 33/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9520 - val_loss: 4.9599\n","Epoch 34/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9520 - val_loss: 4.9601\n","Epoch 35/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9519 - val_loss: 4.9600\n","Epoch 36/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9519 - val_loss: 4.9596\n","Epoch 37/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9520 - val_loss: 4.9604\n","Epoch 38/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9520 - val_loss: 4.9601\n","Epoch 39/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9518 - val_loss: 4.9601\n","Epoch 40/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9519 - val_loss: 4.9599\n","Epoch 41/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9518 - val_loss: 4.9601\n","Epoch 42/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9517 - val_loss: 4.9598\n","Epoch 43/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9517 - val_loss: 4.9600\n","Epoch 44/250\n","6138/6138 [==============================] - 2s 325us/step - loss: 4.9517 - val_loss: 4.9596\n","Epoch 45/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9517 - val_loss: 4.9598\n","Epoch 46/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9517 - val_loss: 4.9596\n","Epoch 47/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9517 - val_loss: 4.9601\n","Epoch 48/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9516 - val_loss: 4.9594\n","Epoch 49/250\n","6138/6138 [==============================] - 2s 292us/step - loss: 4.9516 - val_loss: 4.9593\n","Epoch 50/250\n","6138/6138 [==============================] - 2s 294us/step - loss: 4.9516 - val_loss: 4.9599\n","Epoch 51/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9516 - val_loss: 4.9601\n","Epoch 52/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9595\n","Epoch 53/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9517 - val_loss: 4.9593\n","Epoch 54/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9516 - val_loss: 4.9595\n","Epoch 55/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9515 - val_loss: 4.9596\n","Epoch 56/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9516 - val_loss: 4.9591\n","Epoch 57/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9516 - val_loss: 4.9594\n","Epoch 58/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9515 - val_loss: 4.9593\n","Epoch 59/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9514 - val_loss: 4.9592\n","Epoch 60/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9514 - val_loss: 4.9593\n","Epoch 61/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9514 - val_loss: 4.9593\n","Epoch 62/250\n","6138/6138 [==============================] - 2s 299us/step - loss: 4.9514 - val_loss: 4.9594\n","Epoch 63/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9515 - val_loss: 4.9594\n","Epoch 64/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9515 - val_loss: 4.9594\n","Epoch 65/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9514 - val_loss: 4.9590\n","Epoch 66/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9515 - val_loss: 4.9594\n","Epoch 67/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9514 - val_loss: 4.9594\n","Epoch 68/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9513 - val_loss: 4.9590\n","Epoch 69/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9513 - val_loss: 4.9589\n","Epoch 70/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9513 - val_loss: 4.9590\n","Epoch 71/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9513 - val_loss: 4.9593\n","Epoch 72/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9513 - val_loss: 4.9592\n","Epoch 73/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9513 - val_loss: 4.9591\n","Epoch 74/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9514 - val_loss: 4.9591\n","Epoch 75/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9513 - val_loss: 4.9591\n","Epoch 76/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9513 - val_loss: 4.9592\n","Epoch 77/250\n","6138/6138 [==============================] - 2s 326us/step - loss: 4.9513 - val_loss: 4.9591\n","Epoch 78/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9512 - val_loss: 4.9589\n","Epoch 79/250\n","6138/6138 [==============================] - 2s 295us/step - loss: 4.9513 - val_loss: 4.9588\n","Epoch 80/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9513 - val_loss: 4.9591\n","Epoch 81/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9512 - val_loss: 4.9590\n","Epoch 82/250\n","6138/6138 [==============================] - 2s 327us/step - loss: 4.9512 - val_loss: 4.9592\n","Epoch 83/250\n","6138/6138 [==============================] - 2s 326us/step - loss: 4.9512 - val_loss: 4.9588\n","Epoch 84/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9512 - val_loss: 4.9589\n","Epoch 85/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9512 - val_loss: 4.9590\n","Epoch 86/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9512 - val_loss: 4.9590\n","Epoch 87/250\n","6138/6138 [==============================] - 2s 335us/step - loss: 4.9512 - val_loss: 4.9589\n","Epoch 88/250\n","6138/6138 [==============================] - 2s 325us/step - loss: 4.9512 - val_loss: 4.9591\n","Epoch 89/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9512 - val_loss: 4.9591\n","Epoch 90/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9512 - val_loss: 4.9590\n","Epoch 91/250\n","6138/6138 [==============================] - 2s 326us/step - loss: 4.9511 - val_loss: 4.9591\n","Epoch 92/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9513 - val_loss: 4.9596\n","Epoch 93/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9512 - val_loss: 4.9590\n","Epoch 94/250\n","6138/6138 [==============================] - 2s 326us/step - loss: 4.9511 - val_loss: 4.9590\n","Epoch 95/250\n","6138/6138 [==============================] - 2s 326us/step - loss: 4.9511 - val_loss: 4.9590\n","Epoch 96/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9512 - val_loss: 4.9588\n","Epoch 97/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9511 - val_loss: 4.9592\n","Epoch 98/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9511 - val_loss: 4.9590\n","Epoch 99/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9511 - val_loss: 4.9587\n","Epoch 100/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9512 - val_loss: 4.9587\n","Epoch 101/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9511 - val_loss: 4.9587\n","Epoch 102/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9511 - val_loss: 4.9587\n","Epoch 103/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9510 - val_loss: 4.9587\n","Epoch 104/250\n","6138/6138 [==============================] - 2s 302us/step - loss: 4.9510 - val_loss: 4.9588\n","Epoch 105/250\n","6138/6138 [==============================] - 2s 294us/step - loss: 4.9511 - val_loss: 4.9588\n","Epoch 106/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9511 - val_loss: 4.9593\n","Epoch 107/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9512 - val_loss: 4.9589\n","Epoch 108/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9511 - val_loss: 4.9584\n","Epoch 109/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9510 - val_loss: 4.9584\n","Epoch 110/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9510 - val_loss: 4.9593\n","Epoch 111/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9510 - val_loss: 4.9585\n","Epoch 112/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9511 - val_loss: 4.9585\n","Epoch 113/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9510 - val_loss: 4.9585\n","Epoch 114/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9510 - val_loss: 4.9585\n","Epoch 115/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9510 - val_loss: 4.9585\n","Epoch 116/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9510 - val_loss: 4.9587\n","Epoch 117/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9510 - val_loss: 4.9585\n","Epoch 118/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9510 - val_loss: 4.9584\n","Epoch 119/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9510 - val_loss: 4.9588\n","Epoch 120/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9510 - val_loss: 4.9583\n","Epoch 121/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9510 - val_loss: 4.9586\n","Epoch 122/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9510 - val_loss: 4.9584\n","Epoch 123/250\n","6138/6138 [==============================] - 2s 301us/step - loss: 4.9509 - val_loss: 4.9583\n","Epoch 124/250\n","6138/6138 [==============================] - 2s 330us/step - loss: 4.9510 - val_loss: 4.9583\n","Epoch 125/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9510 - val_loss: 4.9583\n","Epoch 126/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9510 - val_loss: 4.9586\n","Epoch 127/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9510 - val_loss: 4.9585\n","Epoch 128/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9509 - val_loss: 4.9584\n","Epoch 129/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9509 - val_loss: 4.9585\n","Epoch 130/250\n","6138/6138 [==============================] - 2s 295us/step - loss: 4.9510 - val_loss: 4.9583\n","Epoch 131/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9510 - val_loss: 4.9585\n","Epoch 132/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9510 - val_loss: 4.9598\n","Epoch 133/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9509 - val_loss: 4.9586\n","Epoch 134/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9509 - val_loss: 4.9586\n","Epoch 135/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9509 - val_loss: 4.9583\n","Epoch 136/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9510 - val_loss: 4.9586\n","Epoch 137/250\n","6138/6138 [==============================] - 2s 295us/step - loss: 4.9510 - val_loss: 4.9586\n","Epoch 138/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9509 - val_loss: 4.9586\n","Epoch 139/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9509 - val_loss: 4.9584\n","Epoch 140/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9509 - val_loss: 4.9586\n","Epoch 141/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9510 - val_loss: 4.9586\n","Epoch 142/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9509 - val_loss: 4.9584\n","Epoch 143/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9508 - val_loss: 4.9583\n","Epoch 144/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9509 - val_loss: 4.9584\n","Epoch 145/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9509 - val_loss: 4.9583\n","Epoch 146/250\n","6138/6138 [==============================] - 2s 326us/step - loss: 4.9508 - val_loss: 4.9583\n","Epoch 147/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9508 - val_loss: 4.9583\n","Epoch 148/250\n","6138/6138 [==============================] - 2s 325us/step - loss: 4.9508 - val_loss: 4.9584\n","Epoch 149/250\n","6138/6138 [==============================] - 2s 325us/step - loss: 4.9509 - val_loss: 4.9583\n","Epoch 150/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9508 - val_loss: 4.9584\n","Epoch 151/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9508 - val_loss: 4.9583\n","Epoch 152/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9509 - val_loss: 4.9584\n","Epoch 153/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9508 - val_loss: 4.9584\n","Epoch 154/250\n","6138/6138 [==============================] - 2s 336us/step - loss: 4.9508 - val_loss: 4.9583\n","Epoch 155/250\n","6138/6138 [==============================] - 2s 334us/step - loss: 4.9508 - val_loss: 4.9583\n","Epoch 156/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9509 - val_loss: 4.9586\n","Epoch 157/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9508 - val_loss: 4.9583\n","Epoch 158/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9509 - val_loss: 4.9582\n","Epoch 159/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9508 - val_loss: 4.9583\n","Epoch 160/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9508 - val_loss: 4.9583\n","Epoch 161/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9508 - val_loss: 4.9584\n","Epoch 162/250\n","6138/6138 [==============================] - 2s 298us/step - loss: 4.9508 - val_loss: 4.9585\n","Epoch 163/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9508 - val_loss: 4.9582\n","Epoch 164/250\n","6138/6138 [==============================] - 2s 298us/step - loss: 4.9508 - val_loss: 4.9583\n","Epoch 165/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9508 - val_loss: 4.9583\n","Epoch 166/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9507 - val_loss: 4.9583\n","Epoch 167/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9508 - val_loss: 4.9583\n","Epoch 168/250\n","6138/6138 [==============================] - 2s 325us/step - loss: 4.9509 - val_loss: 4.9584\n","Epoch 169/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9508 - val_loss: 4.9581\n","Epoch 170/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9508 - val_loss: 4.9583\n","Epoch 171/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9508 - val_loss: 4.9582\n","Epoch 172/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9508 - val_loss: 4.9582\n","Epoch 173/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9507 - val_loss: 4.9582\n","Epoch 174/250\n","6138/6138 [==============================] - 2s 331us/step - loss: 4.9507 - val_loss: 4.9582\n","Epoch 175/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9508 - val_loss: 4.9584\n","Epoch 176/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9508 - val_loss: 4.9581\n","Epoch 177/250\n","6138/6138 [==============================] - 2s 301us/step - loss: 4.9507 - val_loss: 4.9583\n","Epoch 178/250\n","6138/6138 [==============================] - 2s 328us/step - loss: 4.9507 - val_loss: 4.9583\n","Epoch 179/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9507 - val_loss: 4.9582\n","Epoch 180/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9507 - val_loss: 4.9590\n","Epoch 181/250\n","6138/6138 [==============================] - 2s 331us/step - loss: 4.9507 - val_loss: 4.9583\n","Epoch 182/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9507 - val_loss: 4.9583\n","Epoch 183/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9508 - val_loss: 4.9583\n","Epoch 184/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9507 - val_loss: 4.9590\n","Epoch 185/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9508 - val_loss: 4.9585\n","Epoch 186/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9507 - val_loss: 4.9583\n","Epoch 187/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9507 - val_loss: 4.9581\n","Epoch 188/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9507 - val_loss: 4.9581\n","Epoch 189/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9507 - val_loss: 4.9583\n","Epoch 190/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9507 - val_loss: 4.9582\n","Epoch 191/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9507 - val_loss: 4.9580\n","Epoch 192/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9506 - val_loss: 4.9582\n","Epoch 193/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9506 - val_loss: 4.9581\n","Epoch 194/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9507 - val_loss: 4.9585\n","Epoch 195/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9507 - val_loss: 4.9582\n","Epoch 196/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9507 - val_loss: 4.9581\n","Epoch 197/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9506 - val_loss: 4.9581\n","Epoch 198/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9507 - val_loss: 4.9582\n","Epoch 199/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9507 - val_loss: 4.9582\n","Epoch 200/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9507 - val_loss: 4.9582\n","Epoch 201/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9507 - val_loss: 4.9583\n","Epoch 202/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9506 - val_loss: 4.9581\n","Epoch 203/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9506 - val_loss: 4.9581\n","Epoch 204/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9507 - val_loss: 4.9582\n","Epoch 205/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9507 - val_loss: 4.9581\n","Epoch 206/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9506 - val_loss: 4.9582\n","Epoch 207/250\n","6138/6138 [==============================] - 2s 300us/step - loss: 4.9506 - val_loss: 4.9579\n","Epoch 208/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9506 - val_loss: 4.9581\n","Epoch 209/250\n","6138/6138 [==============================] - 2s 293us/step - loss: 4.9507 - val_loss: 4.9581\n","Epoch 210/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9507 - val_loss: 4.9584\n","Epoch 211/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9506 - val_loss: 4.9579\n","Epoch 212/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9506 - val_loss: 4.9581\n","Epoch 213/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9506 - val_loss: 4.9583\n","Epoch 214/250\n","6138/6138 [==============================] - 2s 297us/step - loss: 4.9506 - val_loss: 4.9580\n","Epoch 215/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9506 - val_loss: 4.9580\n","Epoch 216/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9506 - val_loss: 4.9578\n","Epoch 217/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9506 - val_loss: 4.9579\n","Epoch 218/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9506 - val_loss: 4.9581\n","Epoch 219/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9506 - val_loss: 4.9580\n","Epoch 220/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9506 - val_loss: 4.9579\n","Epoch 221/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9506 - val_loss: 4.9582\n","Epoch 222/250\n","6138/6138 [==============================] - 2s 338us/step - loss: 4.9506 - val_loss: 4.9579\n","Epoch 223/250\n","6138/6138 [==============================] - 2s 325us/step - loss: 4.9506 - val_loss: 4.9580\n","Epoch 224/250\n","6138/6138 [==============================] - 2s 335us/step - loss: 4.9507 - val_loss: 4.9579\n","Epoch 225/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9506 - val_loss: 4.9579\n","Epoch 226/250\n","6138/6138 [==============================] - 2s 297us/step - loss: 4.9506 - val_loss: 4.9587\n","Epoch 227/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9506 - val_loss: 4.9579\n","Epoch 228/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9506 - val_loss: 4.9580\n","Epoch 229/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9506 - val_loss: 4.9581\n","Epoch 230/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9506 - val_loss: 4.9579\n","Epoch 231/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9506 - val_loss: 4.9579\n","Epoch 232/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9506 - val_loss: 4.9579\n","Epoch 233/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9506 - val_loss: 4.9580\n","Epoch 234/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9506 - val_loss: 4.9581\n","Epoch 235/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9506 - val_loss: 4.9590\n","Epoch 236/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9506 - val_loss: 4.9580\n","Epoch 237/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9506 - val_loss: 4.9580\n","Epoch 238/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9506 - val_loss: 4.9579\n","Epoch 239/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9506 - val_loss: 4.9579\n","Epoch 240/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9506 - val_loss: 4.9580\n","Epoch 241/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9506 - val_loss: 4.9581\n","Epoch 242/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9506 - val_loss: 4.9579\n","Epoch 243/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9505 - val_loss: 4.9579\n","Epoch 244/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9506 - val_loss: 4.9579\n","Epoch 245/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9506 - val_loss: 4.9580\n","Epoch 246/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9506 - val_loss: 4.9579\n","Epoch 247/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9506 - val_loss: 4.9580\n","Epoch 248/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9506 - val_loss: 4.9579\n","Epoch 249/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9505 - val_loss: 4.9580\n","Epoch 250/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9505 - val_loss: 4.9579\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6821, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 16886 0.5\n","The shape of N (6821, 784)\n","The minimum value of N  -0.7499998807907104\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9999491732925541\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5677, 28, 28, 1)\n","Train Label Shape:  (5677,)\n","Validation Data Shape:  (1134, 28, 28, 1)\n","Validation Label Shape:  (1134,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (67, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (67, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 6138 samples, validate on 683 samples\n","Epoch 1/250\n","6138/6138 [==============================] - 7s 1ms/step - loss: 5.0549 - val_loss: 5.4645\n","Epoch 2/250\n","6138/6138 [==============================] - 2s 298us/step - loss: 4.9833 - val_loss: 5.3518\n","Epoch 3/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9690 - val_loss: 5.0077\n","Epoch 4/250\n","6138/6138 [==============================] - 2s 298us/step - loss: 4.9638 - val_loss: 4.9867\n","Epoch 5/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9598 - val_loss: 4.9717\n","Epoch 6/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9577 - val_loss: 4.9685\n","Epoch 7/250\n","6138/6138 [==============================] - 2s 297us/step - loss: 4.9579 - val_loss: 4.9723\n","Epoch 8/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9559 - val_loss: 4.9672\n","Epoch 9/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9552 - val_loss: 4.9644\n","Epoch 10/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9546 - val_loss: 4.9635\n","Epoch 11/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9541 - val_loss: 4.9634\n","Epoch 12/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9536 - val_loss: 4.9631\n","Epoch 13/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9534 - val_loss: 4.9635\n","Epoch 14/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9631\n","Epoch 15/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9531 - val_loss: 4.9629\n","Epoch 16/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9528 - val_loss: 4.9611\n","Epoch 17/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9528 - val_loss: 4.9609\n","Epoch 18/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9526 - val_loss: 4.9609\n","Epoch 19/250\n","6138/6138 [==============================] - 2s 328us/step - loss: 4.9527 - val_loss: 4.9598\n","Epoch 20/250\n","6138/6138 [==============================] - 2s 326us/step - loss: 4.9525 - val_loss: 4.9599\n","Epoch 21/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9524 - val_loss: 4.9599\n","Epoch 22/250\n","6138/6138 [==============================] - 2s 331us/step - loss: 4.9524 - val_loss: 4.9600\n","Epoch 23/250\n","6138/6138 [==============================] - 2s 328us/step - loss: 4.9523 - val_loss: 4.9596\n","Epoch 24/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9523 - val_loss: 4.9594\n","Epoch 25/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9523 - val_loss: 4.9593\n","Epoch 26/250\n","6138/6138 [==============================] - 2s 349us/step - loss: 4.9523 - val_loss: 4.9597\n","Epoch 27/250\n","6138/6138 [==============================] - 2s 338us/step - loss: 4.9522 - val_loss: 4.9592\n","Epoch 28/250\n","6138/6138 [==============================] - 2s 299us/step - loss: 4.9519 - val_loss: 4.9591\n","Epoch 29/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9521 - val_loss: 4.9590\n","Epoch 30/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9519 - val_loss: 4.9589\n","Epoch 31/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9518 - val_loss: 4.9588\n","Epoch 32/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9518 - val_loss: 4.9589\n","Epoch 33/250\n","6138/6138 [==============================] - 2s 294us/step - loss: 4.9518 - val_loss: 4.9587\n","Epoch 34/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9517 - val_loss: 4.9585\n","Epoch 35/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9517 - val_loss: 4.9585\n","Epoch 36/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9518 - val_loss: 4.9589\n","Epoch 37/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9517 - val_loss: 4.9583\n","Epoch 38/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9517 - val_loss: 4.9583\n","Epoch 39/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9515 - val_loss: 4.9582\n","Epoch 40/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9516 - val_loss: 4.9581\n","Epoch 41/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9516 - val_loss: 4.9585\n","Epoch 42/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9516 - val_loss: 4.9583\n","Epoch 43/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9516 - val_loss: 4.9583\n","Epoch 44/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9515 - val_loss: 4.9583\n","Epoch 45/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9516 - val_loss: 4.9582\n","Epoch 46/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9515 - val_loss: 4.9582\n","Epoch 47/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9514 - val_loss: 4.9583\n","Epoch 48/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9515 - val_loss: 4.9582\n","Epoch 49/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9515 - val_loss: 4.9583\n","Epoch 50/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9515 - val_loss: 4.9584\n","Epoch 51/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9514 - val_loss: 4.9581\n","Epoch 52/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9514 - val_loss: 4.9581\n","Epoch 53/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9514 - val_loss: 4.9581\n","Epoch 54/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9514 - val_loss: 4.9579\n","Epoch 55/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9513 - val_loss: 4.9579\n","Epoch 56/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9513 - val_loss: 4.9578\n","Epoch 57/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9513 - val_loss: 4.9579\n","Epoch 58/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9513 - val_loss: 4.9580\n","Epoch 59/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9513 - val_loss: 4.9579\n","Epoch 60/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9513 - val_loss: 4.9580\n","Epoch 61/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9513 - val_loss: 4.9578\n","Epoch 62/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9512 - val_loss: 4.9578\n","Epoch 63/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9513 - val_loss: 4.9580\n","Epoch 64/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9512 - val_loss: 4.9580\n","Epoch 65/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9513 - val_loss: 4.9578\n","Epoch 66/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9512 - val_loss: 4.9578\n","Epoch 67/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9513 - val_loss: 4.9582\n","Epoch 68/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9512 - val_loss: 4.9579\n","Epoch 69/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9513 - val_loss: 4.9581\n","Epoch 70/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9513 - val_loss: 4.9580\n","Epoch 71/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9512 - val_loss: 4.9578\n","Epoch 72/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9511 - val_loss: 4.9579\n","Epoch 73/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9513 - val_loss: 4.9581\n","Epoch 74/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9512 - val_loss: 4.9578\n","Epoch 75/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9512 - val_loss: 4.9578\n","Epoch 76/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9511 - val_loss: 4.9577\n","Epoch 77/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9511 - val_loss: 4.9577\n","Epoch 78/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9511 - val_loss: 4.9580\n","Epoch 79/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9511 - val_loss: 4.9578\n","Epoch 80/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9511 - val_loss: 4.9579\n","Epoch 81/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9512 - val_loss: 4.9579\n","Epoch 82/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9511 - val_loss: 4.9577\n","Epoch 83/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9511 - val_loss: 4.9578\n","Epoch 84/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9511 - val_loss: 4.9575\n","Epoch 85/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9510 - val_loss: 4.9580\n","Epoch 86/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9511 - val_loss: 4.9578\n","Epoch 87/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9511 - val_loss: 4.9581\n","Epoch 88/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9511 - val_loss: 4.9577\n","Epoch 89/250\n","6138/6138 [==============================] - 2s 325us/step - loss: 4.9511 - val_loss: 4.9578\n","Epoch 90/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9511 - val_loss: 4.9577\n","Epoch 91/250\n","6138/6138 [==============================] - 2s 331us/step - loss: 4.9510 - val_loss: 4.9576\n","Epoch 92/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9510 - val_loss: 4.9577\n","Epoch 93/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9510 - val_loss: 4.9578\n","Epoch 94/250\n","6138/6138 [==============================] - 2s 338us/step - loss: 4.9510 - val_loss: 4.9577\n","Epoch 95/250\n","6138/6138 [==============================] - 2s 345us/step - loss: 4.9511 - val_loss: 4.9576\n","Epoch 96/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9510 - val_loss: 4.9575\n","Epoch 97/250\n","6138/6138 [==============================] - 2s 327us/step - loss: 4.9510 - val_loss: 4.9576\n","Epoch 98/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9510 - val_loss: 4.9575\n","Epoch 99/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9510 - val_loss: 4.9575\n","Epoch 100/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9510 - val_loss: 4.9580\n","Epoch 101/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9510 - val_loss: 4.9576\n","Epoch 102/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9511 - val_loss: 4.9577\n","Epoch 103/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9510 - val_loss: 4.9574\n","Epoch 104/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9510 - val_loss: 4.9575\n","Epoch 105/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9509 - val_loss: 4.9576\n","Epoch 106/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9509 - val_loss: 4.9575\n","Epoch 107/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9509 - val_loss: 4.9575\n","Epoch 108/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9509 - val_loss: 4.9574\n","Epoch 109/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9510 - val_loss: 4.9575\n","Epoch 110/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9510 - val_loss: 4.9576\n","Epoch 111/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9509 - val_loss: 4.9576\n","Epoch 112/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9510 - val_loss: 4.9599\n","Epoch 113/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9510 - val_loss: 4.9576\n","Epoch 114/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9509 - val_loss: 4.9575\n","Epoch 115/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9509 - val_loss: 4.9576\n","Epoch 116/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9509 - val_loss: 4.9577\n","Epoch 117/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9509 - val_loss: 4.9575\n","Epoch 118/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9509 - val_loss: 4.9576\n","Epoch 119/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9509 - val_loss: 4.9575\n","Epoch 120/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9509 - val_loss: 4.9576\n","Epoch 121/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9509 - val_loss: 4.9574\n","Epoch 122/250\n","6138/6138 [==============================] - 2s 327us/step - loss: 4.9509 - val_loss: 4.9575\n","Epoch 123/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9509 - val_loss: 4.9576\n","Epoch 124/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9509 - val_loss: 4.9576\n","Epoch 125/250\n","6138/6138 [==============================] - 2s 327us/step - loss: 4.9509 - val_loss: 4.9576\n","Epoch 126/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9508 - val_loss: 4.9577\n","Epoch 127/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9509 - val_loss: 4.9576\n","Epoch 128/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9508 - val_loss: 4.9575\n","Epoch 129/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9508 - val_loss: 4.9574\n","Epoch 130/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9508 - val_loss: 4.9576\n","Epoch 131/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9508 - val_loss: 4.9575\n","Epoch 132/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9509 - val_loss: 4.9575\n","Epoch 133/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9508 - val_loss: 4.9576\n","Epoch 134/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9509 - val_loss: 4.9575\n","Epoch 135/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9509 - val_loss: 4.9576\n","Epoch 136/250\n","6138/6138 [==============================] - 2s 300us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 137/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9508 - val_loss: 4.9576\n","Epoch 138/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 139/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9508 - val_loss: 4.9575\n","Epoch 140/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 141/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 142/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 143/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9508 - val_loss: 4.9574\n","Epoch 144/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 145/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9508 - val_loss: 4.9574\n","Epoch 146/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9508 - val_loss: 4.9575\n","Epoch 147/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 148/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 149/250\n","6138/6138 [==============================] - 2s 297us/step - loss: 4.9508 - val_loss: 4.9574\n","Epoch 150/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 151/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9508 - val_loss: 4.9574\n","Epoch 152/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9507 - val_loss: 4.9572\n","Epoch 153/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9508 - val_loss: 4.9575\n","Epoch 154/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 155/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9508 - val_loss: 4.9575\n","Epoch 156/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 157/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9507 - val_loss: 4.9572\n","Epoch 158/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9507 - val_loss: 4.9572\n","Epoch 159/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9507 - val_loss: 4.9573\n","Epoch 160/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9508 - val_loss: 4.9572\n","Epoch 161/250\n","6138/6138 [==============================] - 2s 342us/step - loss: 4.9507 - val_loss: 4.9572\n","Epoch 162/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9507 - val_loss: 4.9573\n","Epoch 163/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9507 - val_loss: 4.9572\n","Epoch 164/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9507 - val_loss: 4.9573\n","Epoch 165/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9507 - val_loss: 4.9572\n","Epoch 166/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 167/250\n","6138/6138 [==============================] - 2s 327us/step - loss: 4.9507 - val_loss: 4.9571\n","Epoch 168/250\n","6138/6138 [==============================] - 2s 325us/step - loss: 4.9506 - val_loss: 4.9572\n","Epoch 169/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9507 - val_loss: 4.9572\n","Epoch 170/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9507 - val_loss: 4.9571\n","Epoch 171/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9507 - val_loss: 4.9571\n","Epoch 172/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 173/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 174/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9507 - val_loss: 4.9572\n","Epoch 175/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9507 - val_loss: 4.9572\n","Epoch 176/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 177/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9507 - val_loss: 4.9572\n","Epoch 178/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 179/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9507 - val_loss: 4.9572\n","Epoch 180/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 181/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9507 - val_loss: 4.9571\n","Epoch 182/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 183/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 184/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 185/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 186/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 187/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 188/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 189/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 190/250\n","6138/6138 [==============================] - 2s 298us/step - loss: 4.9506 - val_loss: 4.9568\n","Epoch 191/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 192/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9506 - val_loss: 4.9568\n","Epoch 193/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 194/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 195/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 196/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9506 - val_loss: 4.9574\n","Epoch 197/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 198/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9506 - val_loss: 4.9571\n","Epoch 199/250\n","6138/6138 [==============================] - 2s 328us/step - loss: 4.9506 - val_loss: 4.9570\n","Epoch 200/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 201/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 202/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 203/250\n","6138/6138 [==============================] - 2s 326us/step - loss: 4.9506 - val_loss: 4.9568\n","Epoch 204/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 205/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9506 - val_loss: 4.9568\n","Epoch 206/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 207/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9506 - val_loss: 4.9568\n","Epoch 208/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9506 - val_loss: 4.9568\n","Epoch 209/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9506 - val_loss: 4.9571\n","Epoch 210/250\n","6138/6138 [==============================] - 2s 299us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 211/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 212/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 213/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 214/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 215/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 216/250\n","6138/6138 [==============================] - 2s 297us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 217/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9506 - val_loss: 4.9568\n","Epoch 218/250\n","6138/6138 [==============================] - 2s 297us/step - loss: 4.9505 - val_loss: 4.9570\n","Epoch 219/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 220/250\n","6138/6138 [==============================] - 2s 304us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 221/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 222/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 223/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 224/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9505 - val_loss: 4.9567\n","Epoch 225/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 226/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 227/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9505 - val_loss: 4.9567\n","Epoch 228/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 229/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 230/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9505 - val_loss: 4.9570\n","Epoch 231/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9505 - val_loss: 4.9567\n","Epoch 232/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9505 - val_loss: 4.9567\n","Epoch 233/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9505 - val_loss: 4.9570\n","Epoch 234/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 235/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9505 - val_loss: 4.9570\n","Epoch 236/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9506 - val_loss: 4.9568\n","Epoch 237/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 238/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9505 - val_loss: 4.9570\n","Epoch 239/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9505 - val_loss: 4.9570\n","Epoch 240/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 241/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 242/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 243/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 244/250\n","6138/6138 [==============================] - 2s 299us/step - loss: 4.9505 - val_loss: 4.9571\n","Epoch 245/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9505 - val_loss: 4.9570\n","Epoch 246/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9505 - val_loss: 4.9570\n","Epoch 247/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 248/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9505 - val_loss: 4.9569\n","Epoch 249/250\n","6138/6138 [==============================] - 2s 297us/step - loss: 4.9505 - val_loss: 4.9571\n","Epoch 250/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9505 - val_loss: 4.9568\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6821, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19292 0.5\n","The shape of N (6821, 784)\n","The minimum value of N  -0.7499998807907104\n","The max value of N 0.7499958872795105\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9998674085892716\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5677, 28, 28, 1)\n","Train Label Shape:  (5677,)\n","Validation Data Shape:  (1134, 28, 28, 1)\n","Validation Label Shape:  (1134,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (67, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (67, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 6138 samples, validate on 683 samples\n","Epoch 1/250\n","6138/6138 [==============================] - 8s 1ms/step - loss: 5.0611 - val_loss: 5.7611\n","Epoch 2/250\n","6138/6138 [==============================] - 2s 301us/step - loss: 4.9866 - val_loss: 5.2783\n","Epoch 3/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9737 - val_loss: 5.0349\n","Epoch 4/250\n","6138/6138 [==============================] - 2s 316us/step - loss: 4.9669 - val_loss: 4.9860\n","Epoch 5/250\n","6138/6138 [==============================] - 2s 299us/step - loss: 4.9626 - val_loss: 4.9751\n","Epoch 6/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9594 - val_loss: 4.9737\n","Epoch 7/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9574 - val_loss: 4.9715\n","Epoch 8/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9558 - val_loss: 4.9664\n","Epoch 9/250\n","6138/6138 [==============================] - 2s 295us/step - loss: 4.9601 - val_loss: 5.1013\n","Epoch 10/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9579 - val_loss: 5.0110\n","Epoch 11/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9561 - val_loss: 4.9765\n","Epoch 12/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9578 - val_loss: 4.9891\n","Epoch 13/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9554 - val_loss: 4.9698\n","Epoch 14/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9544 - val_loss: 4.9647\n","Epoch 15/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9541 - val_loss: 4.9637\n","Epoch 16/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9535 - val_loss: 4.9627\n","Epoch 17/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9533 - val_loss: 4.9620\n","Epoch 18/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9529 - val_loss: 4.9619\n","Epoch 19/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9527 - val_loss: 4.9610\n","Epoch 20/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9526 - val_loss: 4.9600\n","Epoch 21/250\n","6138/6138 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9597\n","Epoch 22/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9524 - val_loss: 4.9604\n","Epoch 23/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9523 - val_loss: 4.9595\n","Epoch 24/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9522 - val_loss: 4.9600\n","Epoch 25/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9522 - val_loss: 4.9597\n","Epoch 26/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9521 - val_loss: 4.9596\n","Epoch 27/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9522 - val_loss: 4.9598\n","Epoch 28/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9521 - val_loss: 4.9591\n","Epoch 29/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9520 - val_loss: 4.9590\n","Epoch 30/250\n","6138/6138 [==============================] - 2s 334us/step - loss: 4.9519 - val_loss: 4.9589\n","Epoch 31/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9518 - val_loss: 4.9589\n","Epoch 32/250\n","6138/6138 [==============================] - 2s 335us/step - loss: 4.9518 - val_loss: 4.9593\n","Epoch 33/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9518 - val_loss: 4.9590\n","Epoch 34/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9517 - val_loss: 4.9587\n","Epoch 35/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9516 - val_loss: 4.9586\n","Epoch 36/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9516 - val_loss: 4.9589\n","Epoch 37/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9515 - val_loss: 4.9589\n","Epoch 38/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9516 - val_loss: 4.9587\n","Epoch 39/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9515 - val_loss: 4.9588\n","Epoch 40/250\n","6138/6138 [==============================] - 2s 295us/step - loss: 4.9516 - val_loss: 4.9586\n","Epoch 41/250\n","6138/6138 [==============================] - 2s 297us/step - loss: 4.9514 - val_loss: 4.9584\n","Epoch 42/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9513 - val_loss: 4.9583\n","Epoch 43/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9513 - val_loss: 4.9582\n","Epoch 44/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9514 - val_loss: 4.9584\n","Epoch 45/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9514 - val_loss: 4.9583\n","Epoch 46/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9513 - val_loss: 4.9585\n","Epoch 47/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9513 - val_loss: 4.9585\n","Epoch 48/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9513 - val_loss: 4.9581\n","Epoch 49/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9513 - val_loss: 4.9582\n","Epoch 50/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9513 - val_loss: 4.9580\n","Epoch 51/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9512 - val_loss: 4.9579\n","Epoch 52/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9512 - val_loss: 4.9579\n","Epoch 53/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9512 - val_loss: 4.9581\n","Epoch 54/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9510 - val_loss: 4.9578\n","Epoch 55/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9510 - val_loss: 4.9579\n","Epoch 56/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9511 - val_loss: 4.9581\n","Epoch 57/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9511 - val_loss: 4.9579\n","Epoch 58/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9512 - val_loss: 4.9580\n","Epoch 59/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9511 - val_loss: 4.9579\n","Epoch 60/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9511 - val_loss: 4.9578\n","Epoch 61/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9510 - val_loss: 4.9577\n","Epoch 62/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9510 - val_loss: 4.9576\n","Epoch 63/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9511 - val_loss: 4.9575\n","Epoch 64/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9509 - val_loss: 4.9574\n","Epoch 65/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9510 - val_loss: 4.9577\n","Epoch 66/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9510 - val_loss: 4.9577\n","Epoch 67/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9509 - val_loss: 4.9571\n","Epoch 68/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 69/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9508 - val_loss: 4.9572\n","Epoch 70/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9508 - val_loss: 4.9571\n","Epoch 71/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 72/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9508 - val_loss: 4.9572\n","Epoch 73/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9508 - val_loss: 4.9572\n","Epoch 74/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9507 - val_loss: 4.9574\n","Epoch 75/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9508 - val_loss: 4.9574\n","Epoch 76/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9508 - val_loss: 4.9571\n","Epoch 77/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9508 - val_loss: 4.9572\n","Epoch 78/250\n","6138/6138 [==============================] - 2s 298us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 79/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9508 - val_loss: 4.9571\n","Epoch 80/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 81/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9508 - val_loss: 4.9571\n","Epoch 82/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 83/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 84/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9508 - val_loss: 4.9573\n","Epoch 85/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 86/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9507 - val_loss: 4.9576\n","Epoch 87/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9506 - val_loss: 4.9568\n","Epoch 88/250\n","6138/6138 [==============================] - 2s 299us/step - loss: 4.9506 - val_loss: 4.9568\n","Epoch 89/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9506 - val_loss: 4.9571\n","Epoch 90/250\n","6138/6138 [==============================] - 2s 298us/step - loss: 4.9507 - val_loss: 4.9570\n","Epoch 91/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 92/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 93/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 94/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 95/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9507 - val_loss: 4.9569\n","Epoch 96/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9505 - val_loss: 4.9567\n","Epoch 97/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9506 - val_loss: 4.9569\n","Epoch 98/250\n","6138/6138 [==============================] - 2s 346us/step - loss: 4.9506 - val_loss: 4.9568\n","Epoch 99/250\n","6138/6138 [==============================] - 2s 327us/step - loss: 4.9506 - val_loss: 4.9568\n","Epoch 100/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 101/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9506 - val_loss: 4.9566\n","Epoch 102/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9505 - val_loss: 4.9566\n","Epoch 103/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9505 - val_loss: 4.9567\n","Epoch 104/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9505 - val_loss: 4.9568\n","Epoch 105/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9506 - val_loss: 4.9567\n","Epoch 106/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9505 - val_loss: 4.9572\n","Epoch 107/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9505 - val_loss: 4.9566\n","Epoch 108/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9505 - val_loss: 4.9566\n","Epoch 109/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9504 - val_loss: 4.9567\n","Epoch 110/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9505 - val_loss: 4.9566\n","Epoch 111/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9505 - val_loss: 4.9565\n","Epoch 112/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9504 - val_loss: 4.9566\n","Epoch 113/250\n","6138/6138 [==============================] - 2s 317us/step - loss: 4.9504 - val_loss: 4.9567\n","Epoch 114/250\n","6138/6138 [==============================] - 2s 297us/step - loss: 4.9504 - val_loss: 4.9565\n","Epoch 115/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9504 - val_loss: 4.9571\n","Epoch 116/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9505 - val_loss: 4.9566\n","Epoch 117/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9505 - val_loss: 4.9566\n","Epoch 118/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9504 - val_loss: 4.9566\n","Epoch 119/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9505 - val_loss: 4.9570\n","Epoch 120/250\n","6138/6138 [==============================] - 2s 325us/step - loss: 4.9504 - val_loss: 4.9566\n","Epoch 121/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9504 - val_loss: 4.9565\n","Epoch 122/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9504 - val_loss: 4.9566\n","Epoch 123/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9504 - val_loss: 4.9563\n","Epoch 124/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9504 - val_loss: 4.9564\n","Epoch 125/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9504 - val_loss: 4.9565\n","Epoch 126/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9504 - val_loss: 4.9566\n","Epoch 127/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9504 - val_loss: 4.9563\n","Epoch 128/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9504 - val_loss: 4.9566\n","Epoch 129/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9503 - val_loss: 4.9564\n","Epoch 130/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9504 - val_loss: 4.9565\n","Epoch 131/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9503 - val_loss: 4.9564\n","Epoch 132/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9503 - val_loss: 4.9565\n","Epoch 133/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9504 - val_loss: 4.9567\n","Epoch 134/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9503 - val_loss: 4.9565\n","Epoch 135/250\n","6138/6138 [==============================] - 2s 328us/step - loss: 4.9503 - val_loss: 4.9565\n","Epoch 136/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9503 - val_loss: 4.9564\n","Epoch 137/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9503 - val_loss: 4.9563\n","Epoch 138/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9503 - val_loss: 4.9563\n","Epoch 139/250\n","6138/6138 [==============================] - 2s 328us/step - loss: 4.9503 - val_loss: 4.9563\n","Epoch 140/250\n","6138/6138 [==============================] - 2s 297us/step - loss: 4.9503 - val_loss: 4.9563\n","Epoch 141/250\n","6138/6138 [==============================] - 2s 305us/step - loss: 4.9503 - val_loss: 4.9565\n","Epoch 142/250\n","6138/6138 [==============================] - 2s 299us/step - loss: 4.9503 - val_loss: 4.9562\n","Epoch 143/250\n","6138/6138 [==============================] - 2s 327us/step - loss: 4.9503 - val_loss: 4.9565\n","Epoch 144/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9503 - val_loss: 4.9563\n","Epoch 145/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9503 - val_loss: 4.9562\n","Epoch 146/250\n","6138/6138 [==============================] - 2s 303us/step - loss: 4.9502 - val_loss: 4.9562\n","Epoch 147/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9503 - val_loss: 4.9563\n","Epoch 148/250\n","6138/6138 [==============================] - 2s 334us/step - loss: 4.9503 - val_loss: 4.9563\n","Epoch 149/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9502 - val_loss: 4.9561\n","Epoch 150/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9503 - val_loss: 4.9563\n","Epoch 151/250\n","6138/6138 [==============================] - 2s 300us/step - loss: 4.9502 - val_loss: 4.9562\n","Epoch 152/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9503 - val_loss: 4.9562\n","Epoch 153/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9503 - val_loss: 4.9563\n","Epoch 154/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9503 - val_loss: 4.9561\n","Epoch 155/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9502 - val_loss: 4.9562\n","Epoch 156/250\n","6138/6138 [==============================] - 2s 299us/step - loss: 4.9502 - val_loss: 4.9562\n","Epoch 157/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9502 - val_loss: 4.9561\n","Epoch 158/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9502 - val_loss: 4.9560\n","Epoch 159/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9502 - val_loss: 4.9562\n","Epoch 160/250\n","6138/6138 [==============================] - 2s 290us/step - loss: 4.9502 - val_loss: 4.9560\n","Epoch 161/250\n","6138/6138 [==============================] - 2s 318us/step - loss: 4.9502 - val_loss: 4.9560\n","Epoch 162/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9502 - val_loss: 4.9559\n","Epoch 163/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9502 - val_loss: 4.9561\n","Epoch 164/250\n","6138/6138 [==============================] - 2s 325us/step - loss: 4.9502 - val_loss: 4.9561\n","Epoch 165/250\n","6138/6138 [==============================] - 2s 341us/step - loss: 4.9502 - val_loss: 4.9560\n","Epoch 166/250\n","6138/6138 [==============================] - 2s 325us/step - loss: 4.9502 - val_loss: 4.9560\n","Epoch 167/250\n","6138/6138 [==============================] - 2s 334us/step - loss: 4.9502 - val_loss: 4.9560\n","Epoch 168/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9501 - val_loss: 4.9561\n","Epoch 169/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9501 - val_loss: 4.9559\n","Epoch 170/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9502 - val_loss: 4.9561\n","Epoch 171/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9502 - val_loss: 4.9559\n","Epoch 172/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9501 - val_loss: 4.9560\n","Epoch 173/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9501 - val_loss: 4.9559\n","Epoch 174/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9501 - val_loss: 4.9559\n","Epoch 175/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9501 - val_loss: 4.9559\n","Epoch 176/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9501 - val_loss: 4.9559\n","Epoch 177/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9501 - val_loss: 4.9559\n","Epoch 178/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9501 - val_loss: 4.9559\n","Epoch 179/250\n","6138/6138 [==============================] - 2s 306us/step - loss: 4.9501 - val_loss: 4.9561\n","Epoch 180/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9502 - val_loss: 4.9558\n","Epoch 181/250\n","6138/6138 [==============================] - 2s 329us/step - loss: 4.9501 - val_loss: 4.9558\n","Epoch 182/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9501 - val_loss: 4.9559\n","Epoch 183/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9502 - val_loss: 4.9558\n","Epoch 184/250\n","6138/6138 [==============================] - 2s 312us/step - loss: 4.9501 - val_loss: 4.9557\n","Epoch 185/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9501 - val_loss: 4.9559\n","Epoch 186/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9501 - val_loss: 4.9557\n","Epoch 187/250\n","6138/6138 [==============================] - 2s 319us/step - loss: 4.9501 - val_loss: 4.9559\n","Epoch 188/250\n","6138/6138 [==============================] - 2s 320us/step - loss: 4.9501 - val_loss: 4.9559\n","Epoch 189/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9501 - val_loss: 4.9558\n","Epoch 190/250\n","6138/6138 [==============================] - 2s 297us/step - loss: 4.9501 - val_loss: 4.9558\n","Epoch 191/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9501 - val_loss: 4.9559\n","Epoch 192/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9501 - val_loss: 4.9559\n","Epoch 193/250\n","6138/6138 [==============================] - 2s 311us/step - loss: 4.9501 - val_loss: 4.9556\n","Epoch 194/250\n","6138/6138 [==============================] - 2s 323us/step - loss: 4.9501 - val_loss: 4.9558\n","Epoch 195/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9501 - val_loss: 4.9559\n","Epoch 196/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9501 - val_loss: 4.9557\n","Epoch 197/250\n","6138/6138 [==============================] - 2s 301us/step - loss: 4.9501 - val_loss: 4.9557\n","Epoch 198/250\n","6138/6138 [==============================] - 2s 328us/step - loss: 4.9501 - val_loss: 4.9556\n","Epoch 199/250\n","6138/6138 [==============================] - 2s 325us/step - loss: 4.9501 - val_loss: 4.9556\n","Epoch 200/250\n","6138/6138 [==============================] - 2s 315us/step - loss: 4.9501 - val_loss: 4.9556\n","Epoch 201/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9501 - val_loss: 4.9556\n","Epoch 202/250\n","6138/6138 [==============================] - 2s 309us/step - loss: 4.9501 - val_loss: 4.9556\n","Epoch 203/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9501 - val_loss: 4.9557\n","Epoch 204/250\n","6138/6138 [==============================] - 2s 310us/step - loss: 4.9500 - val_loss: 4.9556\n","Epoch 205/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9500 - val_loss: 4.9555\n","Epoch 206/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9501 - val_loss: 4.9557\n","Epoch 207/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9501 - val_loss: 4.9555\n","Epoch 208/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9500 - val_loss: 4.9556\n","Epoch 209/250\n","6138/6138 [==============================] - 2s 324us/step - loss: 4.9500 - val_loss: 4.9554\n","Epoch 210/250\n","6138/6138 [==============================] - 2s 313us/step - loss: 4.9500 - val_loss: 4.9554\n","Epoch 211/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9501 - val_loss: 4.9557\n","Epoch 212/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9500 - val_loss: 4.9555\n","Epoch 213/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9500 - val_loss: 4.9555\n","Epoch 214/250\n","6138/6138 [==============================] - 2s 321us/step - loss: 4.9500 - val_loss: 4.9554\n","Epoch 215/250\n","6138/6138 [==============================] - 2s 329us/step - loss: 4.9500 - val_loss: 4.9556\n","Epoch 216/250\n","6138/6138 [==============================] - 2s 326us/step - loss: 4.9500 - val_loss: 4.9556\n","Epoch 217/250\n","6138/6138 [==============================] - 2s 307us/step - loss: 4.9500 - val_loss: 4.9555\n","Epoch 218/250\n","6138/6138 [==============================] - 2s 322us/step - loss: 4.9500 - val_loss: 4.9556\n","Epoch 219/250\n","6138/6138 [==============================] - 2s 296us/step - loss: 4.9500 - val_loss: 4.9556\n","Epoch 220/250\n","6138/6138 [==============================] - 2s 308us/step - loss: 4.9500 - val_loss: 4.9555\n","Epoch 221/250\n","6138/6138 [==============================] - 2s 314us/step - loss: 4.9500 - val_loss: 4.9555\n","Epoch 222/250\n"," 600/6138 [=>............................] - ETA: 1s - loss: 4.9501Buffered data was truncated after reaching the output size limit."],"name":"stdout"}]},{"metadata":{"id":"nuDPmVEd7Y2V","colab_type":"text"},"cell_type":"markdown","source":["# **RCAE-MNIST 0_Vs_all**"]},{"metadata":{"id":"3BJ3fNrr6oOu","colab_type":"code","outputId":"fb235bce-9b1a-426c-f531-2457dcacf50c","executionInfo":{"status":"ok","timestamp":1541303669449,"user_tz":-660,"elapsed":4437458,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":99475}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 5s 856us/step - loss: 5.0644 - val_loss: 5.0573\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9924 - val_loss: 4.9929\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9762 - val_loss: 4.9833\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9695 - val_loss: 4.9760\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9663 - val_loss: 4.9741\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9645 - val_loss: 4.9709\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9633 - val_loss: 4.9693\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9624 - val_loss: 4.9693\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9615 - val_loss: 4.9678\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9608 - val_loss: 4.9676\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9602 - val_loss: 4.9667\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9598 - val_loss: 4.9663\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9594 - val_loss: 4.9659\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9588 - val_loss: 4.9643\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9585 - val_loss: 4.9650\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9585 - val_loss: 4.9649\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9581 - val_loss: 4.9642\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9580 - val_loss: 4.9658\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9577 - val_loss: 4.9650\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9574 - val_loss: 4.9651\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9573 - val_loss: 4.9637\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9568 - val_loss: 4.9632\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9566 - val_loss: 4.9624\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9566 - val_loss: 4.9626\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9563 - val_loss: 4.9649\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9564 - val_loss: 4.9623\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9560 - val_loss: 4.9613\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9559 - val_loss: 4.9615\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9560 - val_loss: 4.9614\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9560 - val_loss: 4.9613\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9558 - val_loss: 4.9619\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9556 - val_loss: 4.9612\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9555 - val_loss: 4.9611\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9553 - val_loss: 4.9604\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9553 - val_loss: 4.9613\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9552 - val_loss: 4.9599\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9552 - val_loss: 4.9603\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9550 - val_loss: 4.9596\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9553 - val_loss: 4.9595\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9552 - val_loss: 4.9590\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9550 - val_loss: 4.9607\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9548 - val_loss: 4.9591\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 288us/step - loss: 4.9549 - val_loss: 4.9599\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9548 - val_loss: 4.9586\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9548 - val_loss: 4.9600\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9546 - val_loss: 4.9589\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9546 - val_loss: 4.9585\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9547 - val_loss: 4.9590\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9546 - val_loss: 4.9587\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9546 - val_loss: 4.9586\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9543 - val_loss: 4.9584\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9543 - val_loss: 4.9587\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9544 - val_loss: 4.9587\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9588\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9586\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9585\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9541 - val_loss: 4.9586\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9539 - val_loss: 4.9585\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 329us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9539 - val_loss: 4.9584\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9525 - val_loss: 4.9576\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9526 - val_loss: 4.9577\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9525 - val_loss: 4.9576\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9527 - val_loss: 4.9576\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9579\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9525 - val_loss: 4.9577\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9525 - val_loss: 4.9597\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9525 - val_loss: 4.9577\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9525 - val_loss: 4.9577\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9524 - val_loss: 4.9578\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9525 - val_loss: 4.9580\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9525 - val_loss: 4.9577\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9579\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9525 - val_loss: 4.9579\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 331us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9578\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9525 - val_loss: 4.9576\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9525 - val_loss: 4.9576\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9578\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9523 - val_loss: 4.9575\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9524 - val_loss: 4.9597\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9524 - val_loss: 4.9586\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9524 - val_loss: 4.9577\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9523 - val_loss: 4.9578\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9524 - val_loss: 4.9577\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 45813 0.5\n","The shape of N (5975, 784)\n","The minimum value of N  -0.7498049139976501\n","The max value of N 0.7495116591453552\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9995416050698479\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 4s 742us/step - loss: 5.0735 - val_loss: 5.1122\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9970 - val_loss: 4.9998\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9799 - val_loss: 4.9857\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9721 - val_loss: 4.9782\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9684 - val_loss: 4.9737\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9663 - val_loss: 4.9732\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9647 - val_loss: 4.9705\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9639 - val_loss: 4.9696\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9629 - val_loss: 4.9694\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9623 - val_loss: 4.9677\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9613 - val_loss: 4.9669\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9611 - val_loss: 4.9671\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9606 - val_loss: 4.9677\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9609 - val_loss: 4.9722\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9601 - val_loss: 4.9702\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9599 - val_loss: 4.9668\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9591 - val_loss: 4.9659\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9592 - val_loss: 4.9688\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9586 - val_loss: 4.9654\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9584 - val_loss: 4.9651\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9580 - val_loss: 4.9640\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9579 - val_loss: 4.9644\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9577 - val_loss: 4.9653\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9577 - val_loss: 4.9630\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9575 - val_loss: 4.9636\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9574 - val_loss: 4.9630\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9572 - val_loss: 4.9649\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9571 - val_loss: 4.9621\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9567 - val_loss: 4.9617\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9565 - val_loss: 4.9622\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9566 - val_loss: 4.9615\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9565 - val_loss: 4.9609\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9567 - val_loss: 4.9615\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9564 - val_loss: 4.9610\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9562 - val_loss: 4.9618\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9563 - val_loss: 4.9605\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9564 - val_loss: 4.9602\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9560 - val_loss: 4.9600\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9562 - val_loss: 4.9595\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9559 - val_loss: 4.9603\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9558 - val_loss: 4.9591\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9559 - val_loss: 4.9602\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9560 - val_loss: 4.9600\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9561 - val_loss: 4.9595\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 334us/step - loss: 4.9557 - val_loss: 4.9598\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 331us/step - loss: 4.9556 - val_loss: 4.9595\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9556 - val_loss: 4.9588\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9557 - val_loss: 4.9612\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9556 - val_loss: 4.9595\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9554 - val_loss: 4.9592\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9553 - val_loss: 4.9591\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9553 - val_loss: 4.9591\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9552 - val_loss: 4.9587\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9586\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9552 - val_loss: 4.9587\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9552 - val_loss: 4.9592\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9551 - val_loss: 4.9584\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9583\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9551 - val_loss: 4.9590\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9550 - val_loss: 4.9583\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9550 - val_loss: 4.9583\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9549 - val_loss: 4.9584\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9548 - val_loss: 4.9583\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9547 - val_loss: 4.9584\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9547 - val_loss: 4.9587\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9547 - val_loss: 4.9582\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9546 - val_loss: 4.9585\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9546 - val_loss: 4.9588\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9545 - val_loss: 4.9585\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9605\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 334us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 335us/step - loss: 4.9540 - val_loss: 4.9624\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9537 - val_loss: 4.9593\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9588\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9536 - val_loss: 4.9584\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9584\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9534 - val_loss: 4.9583\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9587\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9534 - val_loss: 4.9583\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9533 - val_loss: 4.9616\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9532 - val_loss: 4.9581\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 48414 0.5\n","The shape of N (5975, 784)\n","The minimum value of N  -0.7499992847442627\n","The max value of N 0.7499465346336365\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9993467872245333\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 5s 853us/step - loss: 5.0619 - val_loss: 5.0751\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9922 - val_loss: 4.9902\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9774 - val_loss: 4.9821\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9708 - val_loss: 4.9761\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9679 - val_loss: 4.9826\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9658 - val_loss: 4.9723\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9641 - val_loss: 4.9708\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9628 - val_loss: 4.9692\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9624 - val_loss: 4.9695\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9615 - val_loss: 4.9695\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9613 - val_loss: 4.9673\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9606 - val_loss: 4.9677\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9600 - val_loss: 4.9675\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9601 - val_loss: 4.9701\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9594 - val_loss: 4.9680\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9592 - val_loss: 4.9675\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9587 - val_loss: 4.9646\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9584 - val_loss: 4.9656\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9584 - val_loss: 4.9687\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9581 - val_loss: 4.9650\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 346us/step - loss: 4.9579 - val_loss: 4.9644\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9575 - val_loss: 4.9643\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9573 - val_loss: 4.9644\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9572 - val_loss: 4.9623\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9570 - val_loss: 4.9631\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9567 - val_loss: 4.9641\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9566 - val_loss: 4.9631\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9566 - val_loss: 4.9629\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9563 - val_loss: 4.9619\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9561 - val_loss: 4.9611\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9562 - val_loss: 4.9609\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9561 - val_loss: 4.9608\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9563 - val_loss: 4.9627\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9560 - val_loss: 4.9607\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9558 - val_loss: 4.9594\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9558 - val_loss: 4.9617\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9558 - val_loss: 4.9610\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9558 - val_loss: 4.9615\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9560 - val_loss: 4.9617\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9557 - val_loss: 4.9601\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9556 - val_loss: 4.9599\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9556 - val_loss: 4.9600\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9555 - val_loss: 4.9593\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9555 - val_loss: 4.9599\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9554 - val_loss: 4.9592\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9553 - val_loss: 4.9600\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9554 - val_loss: 4.9592\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9552 - val_loss: 4.9591\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9551 - val_loss: 4.9591\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9551 - val_loss: 4.9593\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9551 - val_loss: 4.9591\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9550 - val_loss: 4.9593\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9551 - val_loss: 4.9600\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9549 - val_loss: 4.9591\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9550 - val_loss: 4.9593\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9549 - val_loss: 4.9590\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9549 - val_loss: 4.9591\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9550 - val_loss: 4.9589\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9548 - val_loss: 4.9592\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9548 - val_loss: 4.9588\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9550 - val_loss: 4.9595\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9547 - val_loss: 4.9587\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9547 - val_loss: 4.9588\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9547 - val_loss: 4.9587\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9546 - val_loss: 4.9587\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9547 - val_loss: 4.9585\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9547 - val_loss: 4.9589\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9546 - val_loss: 4.9589\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9546 - val_loss: 4.9592\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9545 - val_loss: 4.9588\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9546 - val_loss: 4.9587\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9545 - val_loss: 4.9591\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9544 - val_loss: 4.9584\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9586\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9545 - val_loss: 4.9585\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9544 - val_loss: 4.9581\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9545 - val_loss: 4.9583\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9543 - val_loss: 4.9585\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9543 - val_loss: 4.9586\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9543 - val_loss: 4.9589\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9543 - val_loss: 4.9584\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9542 - val_loss: 4.9586\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9542 - val_loss: 4.9584\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9542 - val_loss: 4.9588\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9544 - val_loss: 4.9588\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 332us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 332us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9541 - val_loss: 4.9585\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9541 - val_loss: 4.9585\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9585\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9538 - val_loss: 4.9590\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9540 - val_loss: 4.9588\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9541 - val_loss: 4.9594\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9540 - val_loss: 4.9585\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9539 - val_loss: 4.9585\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9538 - val_loss: 4.9585\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9538 - val_loss: 4.9586\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9538 - val_loss: 4.9585\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9536 - val_loss: 4.9596\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9595\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9538 - val_loss: 4.9587\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9537 - val_loss: 4.9584\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9586\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9536 - val_loss: 4.9585\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9537 - val_loss: 4.9585\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9537 - val_loss: 4.9584\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9537 - val_loss: 4.9584\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9535 - val_loss: 4.9585\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 353us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 332us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9585\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9534 - val_loss: 4.9584\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9585\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9532 - val_loss: 4.9590\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9532 - val_loss: 4.9585\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9533 - val_loss: 4.9588\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9533 - val_loss: 4.9587\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9533 - val_loss: 4.9587\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9532 - val_loss: 4.9589\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9532 - val_loss: 4.9588\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9532 - val_loss: 4.9585\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9531 - val_loss: 4.9585\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9531 - val_loss: 4.9585\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9530 - val_loss: 4.9584\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9586\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9586\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9587\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9532 - val_loss: 4.9589\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9587\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 58258 0.5\n","The shape of N (5975, 784)\n","The minimum value of N  -0.7499998807907104\n","The max value of N 0.7497299909591675\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9990516954882479\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 5s 961us/step - loss: 5.0522 - val_loss: 5.0989\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9892 - val_loss: 4.9971\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9768 - val_loss: 4.9883\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9712 - val_loss: 4.9785\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9672 - val_loss: 4.9741\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9650 - val_loss: 4.9738\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9650 - val_loss: 4.9717\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9629 - val_loss: 4.9697\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9615 - val_loss: 4.9711\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9616 - val_loss: 4.9680\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9619 - val_loss: 4.9680\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9601 - val_loss: 4.9657\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9601 - val_loss: 4.9671\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9597 - val_loss: 4.9681\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9593 - val_loss: 4.9662\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9586 - val_loss: 4.9654\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9584 - val_loss: 4.9660\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9582 - val_loss: 4.9643\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9578 - val_loss: 4.9668\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9583 - val_loss: 5.0167\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9614 - val_loss: 5.0167\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9607 - val_loss: 4.9947\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9592 - val_loss: 4.9764\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9586 - val_loss: 4.9732\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9580 - val_loss: 4.9673\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9578 - val_loss: 4.9707\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9578 - val_loss: 4.9667\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9572 - val_loss: 4.9626\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9572 - val_loss: 4.9610\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9566 - val_loss: 4.9605\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9594 - val_loss: 4.9714\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9574 - val_loss: 4.9633\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9567 - val_loss: 4.9615\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9564 - val_loss: 4.9598\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9577 - val_loss: 4.9756\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9575 - val_loss: 4.9660\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9569 - val_loss: 4.9606\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9566 - val_loss: 4.9600\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9564 - val_loss: 4.9595\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9562 - val_loss: 4.9588\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9558 - val_loss: 4.9585\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9559 - val_loss: 4.9599\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9559 - val_loss: 4.9591\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9556 - val_loss: 4.9581\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9556 - val_loss: 4.9584\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9553 - val_loss: 4.9583\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9556 - val_loss: 4.9590\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9557 - val_loss: 4.9591\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9558 - val_loss: 4.9585\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9554 - val_loss: 4.9585\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9553 - val_loss: 4.9582\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9573 - val_loss: 5.0511\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9586 - val_loss: 4.9886\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9571 - val_loss: 4.9766\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9564 - val_loss: 4.9689\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9559 - val_loss: 4.9628\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9557 - val_loss: 4.9608\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9557 - val_loss: 4.9595\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9557 - val_loss: 4.9591\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9554 - val_loss: 4.9765\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9555 - val_loss: 4.9609\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9552 - val_loss: 4.9588\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9550 - val_loss: 4.9582\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 336us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 334us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9543 - val_loss: 4.9623\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9551 - val_loss: 4.9589\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9534 - val_loss: 4.9603\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9593\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9537 - val_loss: 4.9971\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9542 - val_loss: 4.9686\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9613\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9593\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 348us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9525 - val_loss: 4.9569\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 58769 0.5\n","The shape of N (5975, 784)\n","The minimum value of N  -0.743528425693512\n","The max value of N 0.7491300702095032\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.999077480203069\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 6s 1ms/step - loss: 5.0608 - val_loss: 5.1290\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9927 - val_loss: 5.0018\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9777 - val_loss: 4.9841\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9709 - val_loss: 4.9782\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9672 - val_loss: 4.9783\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9652 - val_loss: 4.9748\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9636 - val_loss: 4.9767\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9631 - val_loss: 4.9707\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9624 - val_loss: 4.9686\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9612 - val_loss: 4.9702\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9609 - val_loss: 4.9676\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9605 - val_loss: 4.9683\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9599 - val_loss: 4.9707\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9596 - val_loss: 4.9680\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9593 - val_loss: 4.9671\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9592 - val_loss: 4.9666\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9588 - val_loss: 4.9667\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9586 - val_loss: 4.9654\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9583 - val_loss: 4.9641\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9579 - val_loss: 4.9639\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9577 - val_loss: 4.9657\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9576 - val_loss: 4.9635\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9574 - val_loss: 4.9633\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9572 - val_loss: 4.9640\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9572 - val_loss: 4.9636\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9570 - val_loss: 4.9629\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9568 - val_loss: 4.9695\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9569 - val_loss: 4.9662\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9567 - val_loss: 4.9632\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9565 - val_loss: 4.9630\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9563 - val_loss: 4.9616\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9564 - val_loss: 4.9618\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9562 - val_loss: 4.9625\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9563 - val_loss: 4.9620\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9561 - val_loss: 4.9615\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9560 - val_loss: 4.9614\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9560 - val_loss: 4.9608\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9559 - val_loss: 4.9605\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9556 - val_loss: 4.9603\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9555 - val_loss: 4.9604\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9557 - val_loss: 4.9602\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9556 - val_loss: 4.9605\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9554 - val_loss: 4.9601\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 337us/step - loss: 4.9554 - val_loss: 4.9595\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9553 - val_loss: 4.9597\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9553 - val_loss: 4.9599\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9552 - val_loss: 4.9596\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9551 - val_loss: 4.9595\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9551 - val_loss: 4.9595\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9551 - val_loss: 4.9593\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9587\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9551 - val_loss: 4.9591\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9551 - val_loss: 4.9592\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9550 - val_loss: 4.9589\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9548 - val_loss: 4.9588\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9550 - val_loss: 4.9593\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9549 - val_loss: 4.9590\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9548 - val_loss: 4.9590\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9549 - val_loss: 4.9595\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9548 - val_loss: 4.9591\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9546 - val_loss: 4.9589\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9548 - val_loss: 4.9592\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9590\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9545 - val_loss: 4.9588\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9547 - val_loss: 4.9586\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9546 - val_loss: 4.9586\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9546 - val_loss: 4.9588\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9545 - val_loss: 4.9586\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9547 - val_loss: 4.9584\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9546 - val_loss: 4.9586\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9544 - val_loss: 4.9589\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9544 - val_loss: 4.9587\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9544 - val_loss: 4.9590\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9543 - val_loss: 4.9586\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9544 - val_loss: 4.9593\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9545 - val_loss: 4.9587\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9543 - val_loss: 4.9584\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9542 - val_loss: 4.9583\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9542 - val_loss: 4.9583\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9540 - val_loss: 4.9585\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9540 - val_loss: 4.9586\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9540 - val_loss: 4.9585\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9539 - val_loss: 4.9585\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9597\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9585\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 337us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9536 - val_loss: 4.9587\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9590\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9534 - val_loss: 4.9585\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9584\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9533 - val_loss: 4.9586\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9533 - val_loss: 4.9618\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9534 - val_loss: 4.9584\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9589\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9533 - val_loss: 4.9588\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9587\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9533 - val_loss: 4.9589\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9591\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 328us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 334us/step - loss: 4.9531 - val_loss: 4.9622\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9531 - val_loss: 4.9589\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9532 - val_loss: 4.9590\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9532 - val_loss: 4.9586\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9531 - val_loss: 4.9602\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9587\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9530 - val_loss: 4.9583\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9529 - val_loss: 4.9582\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9529 - val_loss: 4.9581\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9529 - val_loss: 4.9583\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 53330 0.5\n","The shape of N (5975, 784)\n","The minimum value of N  -0.7499997615814209\n","The max value of N 0.747214674949646\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9994069515591159\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 6s 1ms/step - loss: 5.0678 - val_loss: 5.1907\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9936 - val_loss: 5.0020\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9783 - val_loss: 4.9808\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9707 - val_loss: 4.9770\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9671 - val_loss: 4.9725\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9646 - val_loss: 4.9750\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9635 - val_loss: 4.9699\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9620 - val_loss: 4.9720\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9613 - val_loss: 4.9692\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9606 - val_loss: 4.9674\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9608 - val_loss: 4.9714\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 337us/step - loss: 4.9603 - val_loss: 4.9787\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9596 - val_loss: 4.9693\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9600 - val_loss: 4.9783\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9591 - val_loss: 4.9683\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9585 - val_loss: 4.9674\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9581 - val_loss: 4.9650\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9578 - val_loss: 4.9642\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9574 - val_loss: 4.9636\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9573 - val_loss: 4.9639\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9571 - val_loss: 4.9661\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9569 - val_loss: 4.9632\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9567 - val_loss: 4.9630\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9566 - val_loss: 4.9637\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9565 - val_loss: 4.9627\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9559 - val_loss: 4.9627\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9562 - val_loss: 4.9751\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9559 - val_loss: 4.9640\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9557 - val_loss: 4.9620\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9555 - val_loss: 4.9616\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9561 - val_loss: 4.9613\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9555 - val_loss: 4.9612\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9552 - val_loss: 4.9604\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9553 - val_loss: 4.9612\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9551 - val_loss: 4.9594\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9597\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9552 - val_loss: 4.9585\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9549 - val_loss: 4.9601\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9550 - val_loss: 4.9610\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9546 - val_loss: 4.9594\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9545 - val_loss: 4.9587\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9546 - val_loss: 4.9583\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9544 - val_loss: 4.9584\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9544 - val_loss: 4.9586\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9540 - val_loss: 4.9592\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9567 - val_loss: 5.0293\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9564 - val_loss: 4.9951\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9551 - val_loss: 4.9817\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9548 - val_loss: 4.9696\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9543 - val_loss: 4.9639\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9541 - val_loss: 4.9615\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9541 - val_loss: 4.9605\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 338us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 331us/step - loss: 4.9524 - val_loss: 4.9567\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 329us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9523 - val_loss: 4.9564\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9523 - val_loss: 4.9582\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9521 - val_loss: 4.9567\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9520 - val_loss: 4.9564\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9520 - val_loss: 4.9564\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9520 - val_loss: 4.9564\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9520 - val_loss: 4.9566\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9520 - val_loss: 4.9562\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9519 - val_loss: 4.9563\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9520 - val_loss: 4.9565\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9520 - val_loss: 4.9567\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9519 - val_loss: 4.9564\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9520 - val_loss: 4.9565\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9519 - val_loss: 4.9564\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9520 - val_loss: 4.9564\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9520 - val_loss: 4.9564\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9520 - val_loss: 4.9564\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9518 - val_loss: 4.9564\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9519 - val_loss: 4.9565\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9520 - val_loss: 4.9566\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9564\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9518 - val_loss: 4.9566\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9519 - val_loss: 4.9564\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9519 - val_loss: 4.9564\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 350us/step - loss: 4.9520 - val_loss: 4.9572\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 339us/step - loss: 4.9520 - val_loss: 4.9568\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 338us/step - loss: 4.9519 - val_loss: 4.9567\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9519 - val_loss: 4.9566\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9518 - val_loss: 4.9568\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9519 - val_loss: 4.9569\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9519 - val_loss: 4.9564\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 50831 0.5\n","The shape of N (5975, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.7494641542434692\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9987050343223205\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 7s 1ms/step - loss: 5.0568 - val_loss: 5.1068\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9908 - val_loss: 4.9997\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9787 - val_loss: 4.9847\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9721 - val_loss: 4.9816\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9682 - val_loss: 4.9757\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9671 - val_loss: 4.9783\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9662 - val_loss: 4.9731\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9640 - val_loss: 4.9713\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9633 - val_loss: 4.9704\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9623 - val_loss: 4.9695\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9652 - val_loss: 4.9712\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9626 - val_loss: 4.9705\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9614 - val_loss: 4.9689\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9613 - val_loss: 4.9681\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9603 - val_loss: 4.9672\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9601 - val_loss: 4.9657\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9595 - val_loss: 4.9649\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9594 - val_loss: 4.9661\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9596 - val_loss: 4.9675\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9606 - val_loss: 5.0033\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9605 - val_loss: 4.9742\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9591 - val_loss: 4.9760\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9588 - val_loss: 4.9674\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9585 - val_loss: 4.9655\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9581 - val_loss: 4.9648\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9580 - val_loss: 4.9639\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9590 - val_loss: 4.9644\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9581 - val_loss: 4.9642\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9578 - val_loss: 4.9632\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9578 - val_loss: 4.9633\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9575 - val_loss: 4.9622\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9575 - val_loss: 4.9630\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9584 - val_loss: 4.9624\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9577 - val_loss: 4.9629\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9574 - val_loss: 4.9605\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9570 - val_loss: 4.9615\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9572 - val_loss: 4.9604\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9571 - val_loss: 4.9608\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9568 - val_loss: 4.9603\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9567 - val_loss: 4.9595\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9567 - val_loss: 4.9595\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9566 - val_loss: 4.9602\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9565 - val_loss: 4.9593\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9564 - val_loss: 4.9592\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9563 - val_loss: 4.9593\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9563 - val_loss: 4.9595\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9563 - val_loss: 4.9604\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9563 - val_loss: 4.9608\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9564 - val_loss: 4.9593\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9563 - val_loss: 4.9590\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9560 - val_loss: 4.9586\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9560 - val_loss: 4.9592\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9562 - val_loss: 4.9595\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9559 - val_loss: 4.9587\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9559 - val_loss: 4.9585\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9557 - val_loss: 4.9589\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 339us/step - loss: 4.9558 - val_loss: 4.9586\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9566 - val_loss: 4.9817\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9573 - val_loss: 4.9672\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9570 - val_loss: 4.9634\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9572 - val_loss: 4.9617\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9565 - val_loss: 4.9599\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9583 - val_loss: 4.9694\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9570 - val_loss: 4.9659\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9565 - val_loss: 4.9605\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9562 - val_loss: 4.9590\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9560 - val_loss: 4.9586\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9559 - val_loss: 4.9583\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9558 - val_loss: 4.9583\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9558 - val_loss: 4.9581\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9564 - val_loss: 4.9636\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9565 - val_loss: 4.9606\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9561 - val_loss: 4.9586\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9557 - val_loss: 4.9582\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9557 - val_loss: 4.9579\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9557 - val_loss: 4.9579\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9556 - val_loss: 4.9579\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9554 - val_loss: 4.9578\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9555 - val_loss: 4.9578\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9555 - val_loss: 4.9579\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9556 - val_loss: 4.9577\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9554 - val_loss: 4.9578\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9552 - val_loss: 4.9575\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9554 - val_loss: 4.9576\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9556 - val_loss: 4.9608\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9554 - val_loss: 4.9584\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9555 - val_loss: 4.9590\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9553 - val_loss: 4.9580\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9576\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9552 - val_loss: 4.9575\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9552 - val_loss: 4.9580\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9551 - val_loss: 4.9575\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9555 - val_loss: 4.9577\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9553 - val_loss: 4.9576\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9551 - val_loss: 4.9576\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9550 - val_loss: 4.9575\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9550 - val_loss: 4.9575\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9550 - val_loss: 4.9575\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9550 - val_loss: 4.9573\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9551 - val_loss: 4.9574\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9575\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9549 - val_loss: 4.9575\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9549 - val_loss: 4.9588\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9554 - val_loss: 4.9585\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9549 - val_loss: 4.9577\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9548 - val_loss: 4.9591\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9547 - val_loss: 4.9573\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9547 - val_loss: 4.9573\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9573\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9547 - val_loss: 4.9575\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9546 - val_loss: 4.9573\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9546 - val_loss: 4.9573\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9593\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9546 - val_loss: 4.9573\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 337us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9546 - val_loss: 4.9667\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9557 - val_loss: 4.9664\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9549 - val_loss: 4.9606\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9547 - val_loss: 4.9585\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9546 - val_loss: 4.9573\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9584\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9585\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 339us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9544 - val_loss: 4.9648\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9551 - val_loss: 4.9633\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9544 - val_loss: 4.9592\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9542 - val_loss: 4.9586\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9540 - val_loss: 4.9586\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9544 - val_loss: 4.9614\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9537 - val_loss: 4.9574\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 49503 0.5\n","The shape of N (5975, 784)\n","The minimum value of N  -0.7492953538894653\n","The max value of N 0.7498576641082764\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9992837579216374\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 8s 1ms/step - loss: 5.0551 - val_loss: 5.1016\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9900 - val_loss: 5.0054\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9768 - val_loss: 4.9887\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9708 - val_loss: 4.9770\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9680 - val_loss: 4.9776\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9654 - val_loss: 4.9725\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9645 - val_loss: 4.9806\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9642 - val_loss: 4.9729\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9637 - val_loss: 4.9741\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9637 - val_loss: 4.9789\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9621 - val_loss: 4.9715\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9611 - val_loss: 4.9685\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9608 - val_loss: 4.9744\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9610 - val_loss: 5.0266\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9634 - val_loss: 5.0050\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9612 - val_loss: 4.9832\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9601 - val_loss: 4.9752\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9607 - val_loss: 4.9708\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9598 - val_loss: 4.9690\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9591 - val_loss: 4.9673\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9585 - val_loss: 4.9664\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 332us/step - loss: 4.9592 - val_loss: 4.9661\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 338us/step - loss: 4.9582 - val_loss: 4.9646\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 328us/step - loss: 4.9582 - val_loss: 4.9653\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9578 - val_loss: 4.9640\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9573 - val_loss: 4.9641\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9578 - val_loss: 4.9641\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9576 - val_loss: 4.9630\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9571 - val_loss: 4.9619\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9572 - val_loss: 4.9628\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9571 - val_loss: 4.9632\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9569 - val_loss: 4.9622\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9569 - val_loss: 4.9613\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9568 - val_loss: 4.9602\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9565 - val_loss: 4.9606\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9566 - val_loss: 4.9612\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9565 - val_loss: 4.9603\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9566 - val_loss: 4.9604\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9563 - val_loss: 4.9601\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9561 - val_loss: 4.9598\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9561 - val_loss: 4.9612\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9563 - val_loss: 4.9603\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9563 - val_loss: 4.9611\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9561 - val_loss: 4.9596\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9558 - val_loss: 4.9592\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9558 - val_loss: 4.9590\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9558 - val_loss: 4.9593\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9557 - val_loss: 4.9596\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9556 - val_loss: 4.9596\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9556 - val_loss: 4.9593\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9555 - val_loss: 4.9591\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9615 - val_loss: 4.9913\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9592 - val_loss: 4.9742\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9571 - val_loss: 4.9671\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9567 - val_loss: 4.9640\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9562 - val_loss: 4.9607\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9562 - val_loss: 4.9612\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9562 - val_loss: 4.9624\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9562 - val_loss: 4.9602\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9560 - val_loss: 4.9592\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9556 - val_loss: 4.9587\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9555 - val_loss: 4.9586\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9554 - val_loss: 4.9584\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9554 - val_loss: 4.9585\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9552 - val_loss: 4.9588\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9554 - val_loss: 4.9586\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9551 - val_loss: 4.9580\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9551 - val_loss: 4.9584\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9549 - val_loss: 4.9584\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9553 - val_loss: 4.9599\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9551 - val_loss: 4.9591\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9548 - val_loss: 4.9585\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9550 - val_loss: 4.9591\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9550 - val_loss: 4.9585\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9548 - val_loss: 4.9584\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9547 - val_loss: 4.9584\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9548 - val_loss: 4.9584\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9549 - val_loss: 4.9595\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9551 - val_loss: 4.9587\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9587\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9547 - val_loss: 4.9586\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9547 - val_loss: 4.9668\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9549 - val_loss: 4.9606\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9548 - val_loss: 4.9591\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9546 - val_loss: 4.9586\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9549 - val_loss: 4.9584\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9544 - val_loss: 4.9584\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9546 - val_loss: 4.9588\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9544 - val_loss: 4.9581\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9545 - val_loss: 4.9583\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 329us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 337us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9584\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9543 - val_loss: 4.9588\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9544 - val_loss: 4.9589\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9543 - val_loss: 4.9586\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9587\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9539 - val_loss: 4.9591\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9588\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9547 - val_loss: 4.9689\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9545 - val_loss: 4.9621\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9542 - val_loss: 4.9603\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9541 - val_loss: 4.9589\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9537 - val_loss: 4.9585\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9584\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9537 - val_loss: 4.9584\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9537 - val_loss: 4.9583\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9536 - val_loss: 4.9584\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9536 - val_loss: 4.9585\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9585\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9536 - val_loss: 4.9584\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9584\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9535 - val_loss: 4.9583\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9534 - val_loss: 4.9589\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 338us/step - loss: 4.9534 - val_loss: 4.9583\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9535 - val_loss: 4.9585\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9535 - val_loss: 4.9583\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9538 - val_loss: 4.9590\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9537 - val_loss: 4.9585\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9534 - val_loss: 4.9583\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9583\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9532 - val_loss: 4.9691\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9556 - val_loss: 4.9793\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9545 - val_loss: 4.9674\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9540 - val_loss: 4.9639\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9539 - val_loss: 4.9628\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9539 - val_loss: 4.9618\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9614\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9537 - val_loss: 4.9614\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9535 - val_loss: 4.9612\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9536 - val_loss: 4.9609\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9535 - val_loss: 4.9610\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9535 - val_loss: 4.9609\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9534 - val_loss: 4.9607\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9536 - val_loss: 4.9609\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9534 - val_loss: 4.9607\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9533 - val_loss: 4.9606\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9533 - val_loss: 4.9604\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9533 - val_loss: 4.9605\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9533 - val_loss: 4.9609\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9532 - val_loss: 4.9606\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9533 - val_loss: 4.9606\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9534 - val_loss: 4.9604\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9533 - val_loss: 4.9610\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9610\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9610\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9613\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9534 - val_loss: 4.9609\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9533 - val_loss: 4.9609\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9532 - val_loss: 4.9609\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9532 - val_loss: 4.9610\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9532 - val_loss: 4.9606\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9532 - val_loss: 4.9608\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9532 - val_loss: 4.9603\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9531 - val_loss: 4.9604\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9532 - val_loss: 4.9610\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9610\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9532 - val_loss: 4.9609\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9531 - val_loss: 4.9607\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9607\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9532 - val_loss: 4.9605\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9532 - val_loss: 4.9606\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9532 - val_loss: 4.9607\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9532 - val_loss: 4.9607\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9531 - val_loss: 4.9607\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9531 - val_loss: 4.9611\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9608\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9530 - val_loss: 4.9608\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9530 - val_loss: 4.9610\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9609\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9530 - val_loss: 4.9607\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9531 - val_loss: 4.9608\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9530 - val_loss: 4.9609\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9530 - val_loss: 4.9604\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 328us/step - loss: 4.9530 - val_loss: 4.9608\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 59401 0.5\n","The shape of N (5975, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9997679375666104\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 9s 2ms/step - loss: 5.0570 - val_loss: 5.1083\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9907 - val_loss: 5.0066\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9785 - val_loss: 4.9956\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9726 - val_loss: 4.9852\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9691 - val_loss: 4.9784\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9666 - val_loss: 4.9755\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9688 - val_loss: 4.9842\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9654 - val_loss: 4.9819\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9636 - val_loss: 4.9769\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9631 - val_loss: 4.9707\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9635 - val_loss: 5.0304\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9629 - val_loss: 4.9842\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9617 - val_loss: 4.9746\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9615 - val_loss: 4.9691\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9609 - val_loss: 4.9672\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9613 - val_loss: 4.9715\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9612 - val_loss: 4.9706\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9602 - val_loss: 4.9655\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9596 - val_loss: 4.9654\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9594 - val_loss: 4.9647\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9591 - val_loss: 4.9657\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9590 - val_loss: 4.9666\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9589 - val_loss: 4.9671\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9588 - val_loss: 4.9653\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9588 - val_loss: 4.9629\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9583 - val_loss: 4.9633\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9578 - val_loss: 4.9646\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9578 - val_loss: 4.9619\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9577 - val_loss: 4.9609\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9574 - val_loss: 4.9609\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9573 - val_loss: 4.9621\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9572 - val_loss: 4.9601\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9570 - val_loss: 4.9604\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9574 - val_loss: 4.9635\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9573 - val_loss: 4.9612\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9572 - val_loss: 4.9599\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9570 - val_loss: 4.9602\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9568 - val_loss: 4.9596\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9566 - val_loss: 4.9596\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9581 - val_loss: 4.9670\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9576 - val_loss: 4.9623\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9568 - val_loss: 4.9604\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9565 - val_loss: 4.9658\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9566 - val_loss: 4.9596\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9563 - val_loss: 4.9597\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9563 - val_loss: 4.9607\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9562 - val_loss: 4.9596\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9564 - val_loss: 4.9593\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9563 - val_loss: 4.9592\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9563 - val_loss: 4.9590\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9564 - val_loss: 4.9603\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9563 - val_loss: 4.9593\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9569 - val_loss: 4.9691\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9575 - val_loss: 4.9677\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9567 - val_loss: 4.9614\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9564 - val_loss: 4.9598\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9561 - val_loss: 4.9596\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9560 - val_loss: 4.9597\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9559 - val_loss: 4.9620\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9560 - val_loss: 4.9606\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9560 - val_loss: 4.9594\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9572 - val_loss: 4.9609\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 329us/step - loss: 4.9569 - val_loss: 4.9604\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9564 - val_loss: 4.9592\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9562 - val_loss: 4.9613\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 348us/step - loss: 4.9562 - val_loss: 4.9613\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9561 - val_loss: 4.9599\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9563 - val_loss: 4.9754\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9564 - val_loss: 4.9664\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9557 - val_loss: 4.9608\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9558 - val_loss: 4.9591\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9560 - val_loss: 4.9599\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9556 - val_loss: 4.9584\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9557 - val_loss: 4.9585\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9556 - val_loss: 4.9584\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9555 - val_loss: 4.9581\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9555 - val_loss: 4.9578\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9554 - val_loss: 4.9579\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9552 - val_loss: 4.9580\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9552 - val_loss: 4.9584\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9553 - val_loss: 4.9582\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9552 - val_loss: 4.9579\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9552 - val_loss: 4.9577\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9552 - val_loss: 4.9599\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9554 - val_loss: 4.9591\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9580\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9576\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9549 - val_loss: 4.9577\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9548 - val_loss: 4.9583\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 328us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 340us/step - loss: 4.9546 - val_loss: 4.9723\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9544 - val_loss: 4.9596\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 328us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9544 - val_loss: 4.9581\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 328us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 330us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 328us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 354us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 330us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9536 - val_loss: 4.9581\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 65930 0.5\n","The shape of N (5975, 784)\n","The minimum value of N  -0.7499998807907104\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9985789757165286\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 9s 2ms/step - loss: 5.0653 - val_loss: 5.1175\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9926 - val_loss: 4.9988\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9776 - val_loss: 4.9847\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9709 - val_loss: 4.9780\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9673 - val_loss: 4.9739\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9648 - val_loss: 4.9731\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9638 - val_loss: 4.9760\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9632 - val_loss: 4.9698\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9619 - val_loss: 4.9685\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9632 - val_loss: 4.9745\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9627 - val_loss: 5.0094\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9621 - val_loss: 4.9727\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9612 - val_loss: 4.9730\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9602 - val_loss: 4.9689\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9598 - val_loss: 4.9691\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9591 - val_loss: 4.9668\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9590 - val_loss: 4.9650\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9585 - val_loss: 4.9653\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9583 - val_loss: 4.9650\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9580 - val_loss: 4.9644\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9580 - val_loss: 4.9639\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9579 - val_loss: 4.9629\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9578 - val_loss: 4.9661\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9580 - val_loss: 4.9694\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9578 - val_loss: 4.9653\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9579 - val_loss: 4.9648\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9574 - val_loss: 4.9627\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9569 - val_loss: 4.9618\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9569 - val_loss: 4.9616\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9565 - val_loss: 4.9615\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9565 - val_loss: 4.9616\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9564 - val_loss: 4.9619\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9563 - val_loss: 4.9612\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9565 - val_loss: 4.9614\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9566 - val_loss: 4.9608\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9566 - val_loss: 4.9609\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9563 - val_loss: 4.9605\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9559 - val_loss: 4.9600\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9559 - val_loss: 4.9611\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9561 - val_loss: 4.9607\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9559 - val_loss: 4.9608\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9559 - val_loss: 4.9608\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9557 - val_loss: 4.9603\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9556 - val_loss: 4.9602\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9557 - val_loss: 4.9617\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9560 - val_loss: 4.9743\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9558 - val_loss: 4.9620\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9556 - val_loss: 4.9610\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9554 - val_loss: 4.9601\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9554 - val_loss: 4.9594\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9553 - val_loss: 4.9596\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9553 - val_loss: 4.9592\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9553 - val_loss: 4.9587\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9550 - val_loss: 4.9587\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9551 - val_loss: 4.9588\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9550 - val_loss: 4.9585\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9551 - val_loss: 4.9591\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9556 - val_loss: 4.9590\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9552 - val_loss: 4.9588\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9550 - val_loss: 4.9586\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9550 - val_loss: 4.9585\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9549 - val_loss: 4.9600\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9548 - val_loss: 4.9590\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9549 - val_loss: 4.9586\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9548 - val_loss: 4.9583\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9548 - val_loss: 4.9583\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9549 - val_loss: 4.9584\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9548 - val_loss: 4.9585\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9547 - val_loss: 4.9584\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9544 - val_loss: 4.9584\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9546 - val_loss: 4.9691\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9554 - val_loss: 4.9614\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9548 - val_loss: 4.9590\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9548 - val_loss: 4.9583\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 315us/step - loss: 4.9552 - val_loss: 4.9742\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9551 - val_loss: 4.9622\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9549 - val_loss: 4.9611\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9549 - val_loss: 4.9604\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9591\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9545 - val_loss: 4.9588\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9545 - val_loss: 4.9583\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9545 - val_loss: 4.9583\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9543 - val_loss: 4.9585\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9543 - val_loss: 4.9585\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 356us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 328us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9539 - val_loss: 4.9584\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9539 - val_loss: 4.9585\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9539 - val_loss: 4.9586\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9539 - val_loss: 4.9584\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9583\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 316us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 320us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 342us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 354us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 322us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 328us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 317us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 325us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 329us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 327us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 324us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9530 - val_loss: 4.9582\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 313us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 309us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 323us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 314us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 326us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 319us/step - loss: 4.9530 - val_loss: 4.9581\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 318us/step - loss: 4.9530 - val_loss: 4.9584\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 310us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 321us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 311us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9529 - val_loss: 4.9579\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 46734 0.5\n","The shape of N (5975, 784)\n","The minimum value of N  -0.75\n","The max value of N 0.75\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.5 0.9996848534855205\n","=======================\n","Saved model to disk....\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.9995416050698479, 0.9993467872245333, 0.9990516954882479, 0.999077480203069, 0.9994069515591159, 0.9987050343223205, 0.9992837579216374, 0.9997679375666104, 0.9985789757165286, 0.9996848534855205]\n","AUROC ===== 0.999244507855743 +/- 0.000373306690488877\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7fb219815278>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"NrfXlSbpGvOZ","colab_type":"text"},"cell_type":"markdown","source":["Produce Embeddings"]},{"metadata":{"id":"kFetanMC7Yi_","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"_HiGOs197YsG","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"_etAfbNH7YvN","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"tjnkvt_y7Y2B","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"IKP8mM5i7Y5r","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"noGHYp_wbdZ4","colab_type":"text"},"cell_type":"markdown","source":["### Pretrain Autoencoder"]},{"metadata":{"id":"7Zbkh68AbdZ_","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"8oQZWfYvbdaB","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"gBEmq6iHbdaC","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"xo4yNY9ZbdaF","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"7URgC_zOGs4x","colab_type":"text"},"cell_type":"markdown","source":[""]}]}